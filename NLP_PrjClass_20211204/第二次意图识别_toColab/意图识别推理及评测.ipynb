{"cells":[{"cell_type":"markdown","source":["#注意:\n","##经过colab运行验证，模型效果textRCNN < transformer < textCNN < textRNN < TextRNN_Attention,其中transformer没用预训练模型。"],"metadata":{"id":"H4ZCzgw7-I6x"}},{"cell_type":"code","source":["#colab中运行jupyter文件的步骤：\n","# 1.挂载云盘\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# 2.安装需要的软件\n","!pip3 install transformers\n","!pip3 install pytorch-crf==0.7.2\n","\n","import os\n","def get_root_dir():\n","    if os.path.exists('/content/gdrive/MyDrive/第二次意图识别_toColab/'):\n","        return '/content/gdrive/MyDrive/第二次意图识别_toColab/' #在Colab里\n","    else:\n","        return './' #在本地\n","\n","# 3.调用系统命令，切换到对应工程路径，相当于cd，但是直接!cd是不行的\n","print(\"change to path:\",get_root_dir())\n","os.chdir(get_root_dir())\n","\n","# 4.再次确认路径\n","print('current path:')\n","!pwd\n","print('ls in current path:')\n","!ls\n","\n","# 不借助print()实现多输出结果的打印\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = 'all'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQXnSIzV-FN1","executionInfo":{"status":"ok","timestamp":1641301518290,"user_tz":-480,"elapsed":33204,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"c9fdbb04-73e7-456b-ded3-5ae63b3909df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 28.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 628 kB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 59.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 65.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n","Collecting pytorch-crf==0.7.2\n","  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n","Installing collected packages: pytorch-crf\n","Successfully installed pytorch-crf-0.7.2\n","change to path: /content/gdrive/MyDrive/第二次意图识别_toColab/\n","current path:\n","/content/gdrive/MyDrive/第二次意图识别_toColab\n","ls in current path:\n"," data\t\t\t       textcnn\n"," dataSet.pkl\t\t       textrcnn\n"," model-cnn.h5\t\t       textrnn\n"," model-rcnn.h5\t\t       textrnn+Attention\n"," model-rnn+Attention.h5        TextRNN+Attention.ipynb\n"," model-rnn.h5\t\t       transformerEncoder\n"," model-transformerEncoder.h5   意图识别推理及评测.ipynb\n"," parameter.pkl\t\t      '第二次进行意图识别(文本分类)-最新版.ipynb'\n"," runs\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T16:40:40.120318Z","start_time":"2021-08-07T16:40:39.581730Z"},"id":"K5HAYRSp-ETL","executionInfo":{"status":"ok","timestamp":1641303488090,"user_tz":-480,"elapsed":1155,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import pickle as pk\n","from tqdm import tqdm\n","import torch.nn.functional as F # pytorch 激活函数的类\n","from torch import nn,optim # 构建模型和优化器\n","from operator import itemgetter\n","from collections import defaultdict\n","import pdb\n","\n","\n","# 构建分类模型\n","class TextRNN(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextRNN, self).__init__()\n","        embedding_dim = parameter['embedding_dim']\n","        hidden_size = parameter['hidden_size']\n","        output_size = parameter['output_size']\n","        num_layers = parameter['num_layers']\n","        dropout = parameter['dropout']\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size*2, output_size)\n","        \n","    def forward(self, x):\n","        out,(h, c)= self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","    \n","\n","class TextCNN(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextCNN, self).__init__()\n","        filter_size=(3,4,5)\n","        hidden_size = parameter['hidden_size']\n","        embedding_dim = parameter['embedding_dim']\n","        output_size = parameter['output_size']\n","        dropout = parameter['dropout']\n","        \n","        self.convs = nn.ModuleList([nn.Conv2d(1, hidden_size,(k, embedding_dim)) for k in filter_size])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size * len(filter_size), output_size)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1) # [batch, channel, word_num, embedding_dim] = [N,C,H,W] -> (163, 1, 20, 300)\n","        x = [F.tanh(conv(x)).squeeze(3) for conv in self.convs] # len(filter_size) * (N, filter_num, H) -> 3 * (163, 100, 18)\n","        out_new = []\n","        for output in x:\n","            try:\n","                out_new.append(F.max_pool1d(output,output.shape[2].item()).squeeze(2))\n","            except:\n","                out_new.append(F.max_pool1d(output,output.shape[2]).squeeze(2))\n","        x = out_new\n","        x = torch.cat(x, 1) # (N, filter_num * len(filter_size)) -> (163, 100 * 3)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n","    \n","\n","class TextRCNN(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextRCNN, self).__init__()\n","        embedding_dim = parameter['embedding_dim']\n","        hidden_size = parameter['hidden_size']\n","        output_size = parameter['output_size']\n","        num_layers = parameter['num_layers']\n","        dropout = parameter['dropout']\n","        self.lstm = nn.LSTM(embedding_dim,hidden_size, \\\n","                            num_layers, bidirectional=True, \\\n","                            batch_first=True, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_size*2, output_size)\n","        self.fc_for_concat = nn.Linear(hidden_size * 2 + embedding_dim, hidden_size * 2)\n","    \n","    def forward(self, x):\n","        out,(h, c)= self.lstm(x)\n","        out = self.fc_for_concat(torch.cat((x, out), 2))\n","        # 激活函数\n","        out = F.tanh(out)\n","        out = out.permute(0, 2, 1)\n","        try:\n","            out = F.max_pool1d(out, out.size(2).item())\n","        except:\n","            out = F.max_pool1d(out, out.size(2))\n","        out = out.squeeze(-1)\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n","    \n","class TextRNN_Attention(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextRNN_Attention, self).__init__()\n","        embedding_dim = parameter['embedding_dim']\n","        hidden_size = parameter['hidden_size']\n","        output_size = parameter['output_size']\n","        num_layers = parameter['num_layers']\n","        dropout = parameter['dropout']\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n","        self.fc_attention = nn.Linear(hidden_size*2, hidden_size*2)\n","        self.fc = nn.Linear(hidden_size*2, output_size)\n","        \n","        # attention\n","        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n","        \n","    def forward(self, x):\n","        out,(h, c)= self.lstm(x)\n","        \n","        alpha = F.softmax(torch.matmul(F.tanh(self.fc_attention(out)),self.w),dim = 1).unsqueeze(-1)\n","        out = F.relu(torch.sum(out * alpha,1))\n","        out = self.fc(out)\n","        return out\n","    \n","def batch_yield(chars,labels,parameter,shuffle = True):\n","    for train_epoch in range(parameter['epoch']):\n","        if shuffle:\n","            permutation = np.random.permutation(len(chars))\n","            chars = chars[permutation]\n","            labels = labels[permutation]\n","        max_len = 0\n","        batch_x,batch_y,x_len = [],[],[]\n","        for iters in tqdm(range(len(chars))):\n","            batch_ids = itemgetter(*chars[iters])(parameter['char2ind'])\n","            try:\n","                batch_ids = list(batch_ids)\n","            except:\n","                batch_ids = [batch_ids,0]\n","            if len(batch_ids) > max_len:\n","                max_len = len(batch_ids)\n","            batch_x.append(batch_ids)\n","            batch_y.append(labels[iters])\n","            x_len.append(len(batch_ids))\n","            if len(batch_x) >= parameter['batch_size']:\n","                batch_x = [np.array(list(itemgetter(*x_ids)(parameter['ind2embeding']))+[parameter['ind2embeding'][0]]*(max_len-len(x_ids))) for x_ids in batch_x]\n","                device = parameter['cuda']\n","                yield torch.from_numpy(np.array(batch_x)).to(device),np.array(batch_y),x_len,True,None\n","                max_len,batch_x,batch_y,x_len = 0,[],[],[]\n","        batch_x = [np.array(list(itemgetter(*x_ids)(parameter['ind2embeding']))+[parameter['ind2embeding'][0]]*(max_len-len(x_ids))) for x_ids in batch_x]\n","        device = parameter['cuda']\n","        yield torch.from_numpy(np.array(batch_x)).to(device),np.array(batch_y),x_len,True,train_epoch\n","        max_len,batch_x,batch_y,x_len = 0,[],[],[]\n","    yield None,None,None,False,None\n","    \n","def load_model(way = 'TextRNN'):\n","    [train_chars,test_chars,train_labels,test_labels] = pk.load(open('dataSet.pkl','rb'))\n","    parameter = pk.load(open('parameter.pkl','rb'))\n","#     parameter['cuda'] = torch.device('cpu')\n","    parameter['dropout'] = 0\n","    pdb.set_trace()\n","    model = eval(way+\"(parameter).to(parameter['cuda'])\") #eval(\"TextCNN(parameter).to(parameter['cuda'])\")\n","    if way == 'TextRNN':\n","        model.load_state_dict(torch.load('model-rnn.h5'))\n","    if way == 'TextCNN':\n","        model.load_state_dict(torch.load('model-cnn.h5'))\n","    if way == 'TextRCNN':\n","        model.load_state_dict(torch.load('model-rcnn.h5'))\n","    if way == 'transformerEncoder':\n","        model.load_state_dict(torch.load('model-transformerEncoder.h5'))\n","    if way == 'TextRNN_Attention':\n","        model.load_state_dict(torch.load('model-rnn+Attention.h5'))\n","#     model = eval(\"transformerEncoder(parameter).to(parameter['cuda'])\")\n","#     model.load_state_dict(torch.load('model-lstm.h5'))\n","    return parameter,model,[test_chars,test_labels]\n","\n","def compare(real,predict,histroy,parameter):\n","    com = real - predict\n","    for i in range(parameter['output_size']):\n","        histroy[i]['tp'] += len(np.where((com == 0) & (real == i))[0])\n","        histroy[i]['all_real'] += len(np.where((real == i))[0])\n","        histroy[i]['all_predict'] += len(np.where((predict == i))[0])\n","\n","def toEstimate(way = 'TextRNN'):\n","    global device\n","    parameter,model,[test_chars,test_labels] = load_model(way)\n","    model.eval() \n","    device = parameter['cuda']\n","    parameter['epoch'] = 1\n","    test_yield = batch_yield(test_chars,test_labels,parameter)\n","    histroy = dict(zip(range(parameter['output_size']),[{'tp':0,'all_real':0,'all_predict':0} for i in range(parameter['output_size'])]))\n","    while 1:\n","        seqs,labels,x_len,keys,epoch = next(test_yield)\n","        if not keys:\n","            break\n","        if way == 'transformerEncoder':\n","            res = model(seqs,x_len)\n","        else:\n","            res = model(seqs)\n","        predicted_prob,predicted_index = torch.max(F.softmax(res, 1), 1)\n","        res = predicted_index.cpu().numpy()\n","        compare(labels,res,histroy,parameter)\n","    print(histroy)\n","    tp,all_real,all_predict = [histroy[i]['tp'] for i in histroy],[histroy[i]['all_real'] for i in histroy],\\\n","    [histroy[i]['all_predict'] for i in histroy]\n","    tp.append(sum(tp))\n","    all_real.append(sum(all_real))\n","    all_predict.append(sum(all_predict))\n","    res = pd.DataFrame(np.array([tp,all_real,all_predict]).transpose())\n","    res.columns = ['tp','all_real','all_predict']\n","    res['recall'] = res['tp']/res['all_real']\n","    res['precision'] = res['tp']/res['all_predict']\n","    res['f1'] = 2*res['recall']*res['precision']/(res['recall']+res['precision'])\n","    res.index =['标签'+str(i) for i in range(15)]+['综合']\n","    return res\n","    \n","device = None\n"]},{"cell_type":"code","source":["import math\n","import torch\n","import numpy as np\n","import pandas as pd\n","import pickle as pk\n","from tqdm import tqdm\n","import torch.nn.functional as F # pytorch 激活函数的类\n","from torch import nn,optim # 构建模型和优化器\n","from operator import itemgetter\n","from collections import defaultdict"],"metadata":{"id":"Na2hiMT8NbYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, parameter):#d_model, dropout=0.1, max_len=200):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=parameter['dropout'])\n","        d_model = parameter['embedding_dim']\n","        max_len = parameter['max_len']\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        '''\n","        x: [seq_len, batch_size, d_model]\n","        '''\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","    \n","def get_attn_pad_mask(q_len_list, k_len_list):\n","    global device\n","    len_q = max(q_len_list)\n","    len_k = max(k_len_list)\n","    batch_size = len(q_len_list)\n","    pad_attn_mask =  torch.from_numpy(np.array([[False]*i+[True]*(len_k-i) for i in k_len_list])).unsqueeze(1)\n","    return pad_attn_mask.expand(batch_size, len_q, len_k).byte().to(device)  # [batch_size, len_q, len_k]\n","    \n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self,parameter):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.d_k = parameter['d_k']\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        '''\n","        Q: [batch_size, n_heads, len_q, d_k]\n","        K: [batch_size, n_heads, len_k, d_k]\n","        V: [batch_size, n_heads, len_v(=len_k), d_v]\n","        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n","        '''\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k) # scores : [batch_size, n_heads, len_q, len_k]\n","        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n","#         print(scores)\n","#         print('scores:',scores.shape)\n","        attn = nn.Softmax(dim=-1)(scores)\n","#         print(attn)\n","#         print('attn:',attn.shape)\n","        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n","#         print('context:',context.shape)\n","        return context, attn\n","    \n","class MultiHeadAttention(nn.Module):\n","    def __init__(self,parameter):\n","        super(MultiHeadAttention, self).__init__()\n","        device = parameter['cuda']\n","        self.d_q,self.d_k,self.d_v,self.d_model,self.n_heads = parameter['d_q'],parameter['d_k'], \\\n","        parameter['d_v'],parameter['embedding_dim'],parameter['n_heads']\n","        self.W_Q = nn.Linear(self.d_model, self.d_q * self.n_heads, bias=False)\n","        self.W_K = nn.Linear(self.d_model, self.d_k * self.n_heads, bias=False)\n","        self.W_V = nn.Linear(self.d_model, self.d_v * self.n_heads, bias=False)\n","        self.fc = nn.Linear(self.n_heads * self.d_v, self.d_model, bias=False)\n","        self.sdp = ScaledDotProductAttention(parameter).to(device)\n","        self.add_norm = nn.LayerNorm(self.d_model)\n","        \n","    def forward(self, input_Q, input_K, input_V, attn_mask):\n","        '''\n","        input_Q: [batch_size, len_q, d_model]\n","        input_K: [batch_size, len_k, d_model]\n","        input_V: [batch_size, len_v(=len_k), d_model]\n","        attn_mask: [batch_size, seq_len, seq_len]\n","        '''\n","#         print('input-shape',input_Q.shape)\n","        residual, batch_size = input_Q, input_Q.size(0)\n","        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n","        Q = self.W_Q(input_Q).view(batch_size, -1, self.n_heads, self.d_q).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_q]\n","        K = self.W_K(input_K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n","        V = self.W_V(input_V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n","#         print('QKV-shape',Q.shape,K.shape,V.shape)\n","#         print('test:',K.transpose(-1,-2).shape)\n","#         print('attn-shape0:',attn_mask.shape)\n","#         print(attn_mask.shape)\n","        attn_mask_new = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n","#         print('attn-shape1:',attn_mask.shape)\n","#         print(attn_mask.shape)\n","        context, attn = self.sdp(Q, K, V, attn_mask_new)\n","        context = context.transpose(1, 2).reshape(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size, len_q, n_heads * d_v]\n","        output = self.fc(context) # [batch_size, len_q, d_model]\n","        output = self.add_norm(output + residual)\n","        return output, attn\n","    \n","class PoswiseFeedForwardNet(nn.Module):\n","    def __init__(self,parameter):\n","        self.d_ff,self.d_model = parameter['d_ff'],parameter['embedding_dim']\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.d_model, self.d_ff, bias=False),\n","            nn.ReLU(),\n","            nn.Linear(self.d_ff, self.d_model, bias=False)\n","        )\n","        self.add_norm = nn.LayerNorm(self.d_model)\n","    def forward(self, inputs):\n","        '''\n","        inputs: [batch_size, seq_len, d_model]\n","        '''\n","        residual = inputs\n","        output = self.fc(inputs)\n","        return self.add_norm(output + residual) # [batch_size, seq_len, d_model]\n","    \n","class EncoderLayer(nn.Module):\n","    def __init__(self,parameter):\n","        super(EncoderLayer, self).__init__()\n","        device = parameter['cuda']\n","        self.enc_self_attn = MultiHeadAttention(parameter).to(device)\n","        self.pos_ffn = PoswiseFeedForwardNet(parameter).to(device)\n","\n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        '''\n","        enc_inputs: [batch_size, src_len, d_model]\n","        enc_self_attn_mask: [batch_size, src_len, src_len]\n","        '''\n","        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n","        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n","        return enc_outputs, attn\n","    \n","class Encoder(nn.Module):\n","    def __init__(self,parameter):\n","        super(Encoder, self).__init__()\n","        n_layers = parameter['n_layers']\n","        self.pos_emb = PositionalEncoding(parameter)\n","        self.layers = nn.ModuleList([EncoderLayer(parameter) for _ in range(n_layers)])\n","\n","    def forward(self, enc_inputs,len_inputs):\n","        '''\n","        enc_inputs: [batch_size, src_len, d_model]\n","        '''\n","        enc_outputs = self.pos_emb(enc_inputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n","        enc_self_attn_mask = get_attn_pad_mask(len_inputs, len_inputs) # [batch_size, src_len, src_len]\n","        enc_self_attns = []\n","        for layer in self.layers:\n","            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n","            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n","            enc_self_attns.append(enc_self_attn)\n","        return enc_outputs, enc_self_attns\n","    \n","class transformerEncoder(nn.Module):\n","    def __init__(self,parameter):\n","        super(transformerEncoder, self).__init__()\n","        output_dim = parameter['output_size']\n","        d_model = parameter['embedding_dim']\n","        device = parameter['cuda']\n","        self.encoder = Encoder(parameter).to(device)\n","        self.fc = nn.Linear(d_model, output_dim)\n","        \n","    def forward(self,enc_inputs,len_inputs):\n","        enc_outputs, enc_self_attns = self.encoder(enc_inputs,len_inputs)\n","#         print(enc_outputs.shape)\n","#         enc_outputs = enc_outputs.permute(0, 2, 1)\n","        enc_outputs,_ = torch.max(enc_outputs, 1)\n","#         print(enc_outputs.shape)\n","        outputs = self.fc(enc_outputs)\n","        return outputs"],"metadata":{"id":"1ck2bcCONPjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#各个模型评价"],"metadata":{"id":"TMkqw_EXFfdm"}},{"cell_type":"code","source":["toEstimate('TextCNN')\n","# toEstimate('TextRNN_Attention')\n","# toEstimate('TextRCNN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":903},"id":"qaPh_Rw6-fVD","executionInfo":{"status":"ok","timestamp":1641303900468,"user_tz":-480,"elapsed":14677,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"bda782a2-9c60-48c9-db7b-6fda4620d202"},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":["> <ipython-input-11-44f84693a314>(147)load_model()\n","-> model = eval(way+\"(parameter).to(parameter['cuda'])\")\n","(Pdb) c\n"]},{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.7/bdb.py\", line 343, in set_continue\n","    sys.settrace(None)\n","\n","  0%|          | 0/76529 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","100%|██████████| 76529/76529 [00:05<00:00, 13061.11it/s]"]},{"output_type":"stream","name":"stdout","text":["{0: {'tp': 873, 'all_real': 1238, 'all_predict': 1101}, 1: {'tp': 4556, 'all_real': 5577, 'all_predict': 5531}, 2: {'tp': 6597, 'all_real': 7920, 'all_predict': 8252}, 3: {'tp': 6623, 'all_real': 7524, 'all_predict': 7411}, 4: {'tp': 4029, 'all_real': 5439, 'all_predict': 5231}, 5: {'tp': 3032, 'all_real': 3543, 'all_predict': 3375}, 6: {'tp': 6350, 'all_real': 7141, 'all_predict': 7188}, 7: {'tp': 4718, 'all_real': 5366, 'all_predict': 5590}, 8: {'tp': 6952, 'all_real': 8291, 'all_predict': 8904}, 9: {'tp': 4043, 'all_real': 4970, 'all_predict': 4772}, 10: {'tp': 3377, 'all_real': 4284, 'all_predict': 4423}, 11: {'tp': 4098, 'all_real': 5293, 'all_predict': 5393}, 12: {'tp': 0, 'all_real': 61, 'all_predict': 0}, 13: {'tp': 3222, 'all_real': 4003, 'all_predict': 3884}, 14: {'tp': 4783, 'all_real': 5879, 'all_predict': 5474}}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-d841f5d1-930a-4cc2-9a5b-60786ba7eca3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tp</th>\n","      <th>all_real</th>\n","      <th>all_predict</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>标签0</th>\n","      <td>873</td>\n","      <td>1238</td>\n","      <td>1101</td>\n","      <td>0.705170</td>\n","      <td>0.792916</td>\n","      <td>0.746473</td>\n","    </tr>\n","    <tr>\n","      <th>标签1</th>\n","      <td>4556</td>\n","      <td>5577</td>\n","      <td>5531</td>\n","      <td>0.816927</td>\n","      <td>0.823721</td>\n","      <td>0.820310</td>\n","    </tr>\n","    <tr>\n","      <th>标签2</th>\n","      <td>6597</td>\n","      <td>7920</td>\n","      <td>8252</td>\n","      <td>0.832955</td>\n","      <td>0.799443</td>\n","      <td>0.815855</td>\n","    </tr>\n","    <tr>\n","      <th>标签3</th>\n","      <td>6623</td>\n","      <td>7524</td>\n","      <td>7411</td>\n","      <td>0.880250</td>\n","      <td>0.893672</td>\n","      <td>0.886910</td>\n","    </tr>\n","    <tr>\n","      <th>标签4</th>\n","      <td>4029</td>\n","      <td>5439</td>\n","      <td>5231</td>\n","      <td>0.740761</td>\n","      <td>0.770216</td>\n","      <td>0.755201</td>\n","    </tr>\n","    <tr>\n","      <th>标签5</th>\n","      <td>3032</td>\n","      <td>3543</td>\n","      <td>3375</td>\n","      <td>0.855772</td>\n","      <td>0.898370</td>\n","      <td>0.876554</td>\n","    </tr>\n","    <tr>\n","      <th>标签6</th>\n","      <td>6350</td>\n","      <td>7141</td>\n","      <td>7188</td>\n","      <td>0.889231</td>\n","      <td>0.883417</td>\n","      <td>0.886314</td>\n","    </tr>\n","    <tr>\n","      <th>标签7</th>\n","      <td>4718</td>\n","      <td>5366</td>\n","      <td>5590</td>\n","      <td>0.879240</td>\n","      <td>0.844007</td>\n","      <td>0.861263</td>\n","    </tr>\n","    <tr>\n","      <th>标签8</th>\n","      <td>6952</td>\n","      <td>8291</td>\n","      <td>8904</td>\n","      <td>0.838500</td>\n","      <td>0.780773</td>\n","      <td>0.808607</td>\n","    </tr>\n","    <tr>\n","      <th>标签9</th>\n","      <td>4043</td>\n","      <td>4970</td>\n","      <td>4772</td>\n","      <td>0.813481</td>\n","      <td>0.847234</td>\n","      <td>0.830014</td>\n","    </tr>\n","    <tr>\n","      <th>标签10</th>\n","      <td>3377</td>\n","      <td>4284</td>\n","      <td>4423</td>\n","      <td>0.788282</td>\n","      <td>0.763509</td>\n","      <td>0.775698</td>\n","    </tr>\n","    <tr>\n","      <th>标签11</th>\n","      <td>4098</td>\n","      <td>5293</td>\n","      <td>5393</td>\n","      <td>0.774230</td>\n","      <td>0.759874</td>\n","      <td>0.766985</td>\n","    </tr>\n","    <tr>\n","      <th>标签12</th>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>标签13</th>\n","      <td>3222</td>\n","      <td>4003</td>\n","      <td>3884</td>\n","      <td>0.804896</td>\n","      <td>0.829557</td>\n","      <td>0.817041</td>\n","    </tr>\n","    <tr>\n","      <th>标签14</th>\n","      <td>4783</td>\n","      <td>5879</td>\n","      <td>5474</td>\n","      <td>0.813574</td>\n","      <td>0.873767</td>\n","      <td>0.842597</td>\n","    </tr>\n","    <tr>\n","      <th>综合</th>\n","      <td>63253</td>\n","      <td>76529</td>\n","      <td>76529</td>\n","      <td>0.826523</td>\n","      <td>0.826523</td>\n","      <td>0.826523</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d841f5d1-930a-4cc2-9a5b-60786ba7eca3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d841f5d1-930a-4cc2-9a5b-60786ba7eca3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d841f5d1-930a-4cc2-9a5b-60786ba7eca3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         tp  all_real  all_predict    recall  precision        f1\n","标签0     873      1238         1101  0.705170   0.792916  0.746473\n","标签1    4556      5577         5531  0.816927   0.823721  0.820310\n","标签2    6597      7920         8252  0.832955   0.799443  0.815855\n","标签3    6623      7524         7411  0.880250   0.893672  0.886910\n","标签4    4029      5439         5231  0.740761   0.770216  0.755201\n","标签5    3032      3543         3375  0.855772   0.898370  0.876554\n","标签6    6350      7141         7188  0.889231   0.883417  0.886314\n","标签7    4718      5366         5590  0.879240   0.844007  0.861263\n","标签8    6952      8291         8904  0.838500   0.780773  0.808607\n","标签9    4043      4970         4772  0.813481   0.847234  0.830014\n","标签10   3377      4284         4423  0.788282   0.763509  0.775698\n","标签11   4098      5293         5393  0.774230   0.759874  0.766985\n","标签12      0        61            0  0.000000        NaN       NaN\n","标签13   3222      4003         3884  0.804896   0.829557  0.817041\n","标签14   4783      5879         5474  0.813574   0.873767  0.842597\n","综合    63253     76529        76529  0.826523   0.826523  0.826523"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["toEstimate('TextRNN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594},"id":"5aehOlrJAfpW","executionInfo":{"status":"ok","timestamp":1641214735579,"user_tz":-480,"elapsed":12845,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"66329e62-ac9f-4d01-916a-7ca74cd550e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 76529/76529 [00:06<00:00, 11218.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{0: {'tp': 941, 'all_real': 1238, 'all_predict': 1225}, 1: {'tp': 4638, 'all_real': 5577, 'all_predict': 5709}, 2: {'tp': 6586, 'all_real': 7920, 'all_predict': 7961}, 3: {'tp': 6539, 'all_real': 7524, 'all_predict': 7077}, 4: {'tp': 4013, 'all_real': 5439, 'all_predict': 5172}, 5: {'tp': 3025, 'all_real': 3543, 'all_predict': 3338}, 6: {'tp': 6470, 'all_real': 7141, 'all_predict': 7376}, 7: {'tp': 4705, 'all_real': 5366, 'all_predict': 5566}, 8: {'tp': 6796, 'all_real': 8291, 'all_predict': 8197}, 9: {'tp': 4096, 'all_real': 4970, 'all_predict': 4894}, 10: {'tp': 3480, 'all_real': 4284, 'all_predict': 4713}, 11: {'tp': 4024, 'all_real': 5293, 'all_predict': 5193}, 12: {'tp': 0, 'all_real': 61, 'all_predict': 1}, 13: {'tp': 3435, 'all_real': 4003, 'all_predict': 4463}, 14: {'tp': 4916, 'all_real': 5879, 'all_predict': 5644}}\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-58a133c5-0342-41d8-a9a5-399890805759\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tp</th>\n","      <th>all_real</th>\n","      <th>all_predict</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>标签0</th>\n","      <td>941</td>\n","      <td>1238</td>\n","      <td>1225</td>\n","      <td>0.760097</td>\n","      <td>0.768163</td>\n","      <td>0.764109</td>\n","    </tr>\n","    <tr>\n","      <th>标签1</th>\n","      <td>4638</td>\n","      <td>5577</td>\n","      <td>5709</td>\n","      <td>0.831630</td>\n","      <td>0.812401</td>\n","      <td>0.821903</td>\n","    </tr>\n","    <tr>\n","      <th>标签2</th>\n","      <td>6586</td>\n","      <td>7920</td>\n","      <td>7961</td>\n","      <td>0.831566</td>\n","      <td>0.827283</td>\n","      <td>0.829419</td>\n","    </tr>\n","    <tr>\n","      <th>标签3</th>\n","      <td>6539</td>\n","      <td>7524</td>\n","      <td>7077</td>\n","      <td>0.869086</td>\n","      <td>0.923979</td>\n","      <td>0.895692</td>\n","    </tr>\n","    <tr>\n","      <th>标签4</th>\n","      <td>4013</td>\n","      <td>5439</td>\n","      <td>5172</td>\n","      <td>0.737819</td>\n","      <td>0.775909</td>\n","      <td>0.756385</td>\n","    </tr>\n","    <tr>\n","      <th>标签5</th>\n","      <td>3025</td>\n","      <td>3543</td>\n","      <td>3338</td>\n","      <td>0.853796</td>\n","      <td>0.906231</td>\n","      <td>0.879233</td>\n","    </tr>\n","    <tr>\n","      <th>标签6</th>\n","      <td>6470</td>\n","      <td>7141</td>\n","      <td>7376</td>\n","      <td>0.906036</td>\n","      <td>0.877169</td>\n","      <td>0.891369</td>\n","    </tr>\n","    <tr>\n","      <th>标签7</th>\n","      <td>4705</td>\n","      <td>5366</td>\n","      <td>5566</td>\n","      <td>0.876817</td>\n","      <td>0.845311</td>\n","      <td>0.860776</td>\n","    </tr>\n","    <tr>\n","      <th>标签8</th>\n","      <td>6796</td>\n","      <td>8291</td>\n","      <td>8197</td>\n","      <td>0.819684</td>\n","      <td>0.829084</td>\n","      <td>0.824357</td>\n","    </tr>\n","    <tr>\n","      <th>标签9</th>\n","      <td>4096</td>\n","      <td>4970</td>\n","      <td>4894</td>\n","      <td>0.824145</td>\n","      <td>0.836943</td>\n","      <td>0.830495</td>\n","    </tr>\n","    <tr>\n","      <th>标签10</th>\n","      <td>3480</td>\n","      <td>4284</td>\n","      <td>4713</td>\n","      <td>0.812325</td>\n","      <td>0.738383</td>\n","      <td>0.773591</td>\n","    </tr>\n","    <tr>\n","      <th>标签11</th>\n","      <td>4024</td>\n","      <td>5293</td>\n","      <td>5193</td>\n","      <td>0.760249</td>\n","      <td>0.774889</td>\n","      <td>0.767500</td>\n","    </tr>\n","    <tr>\n","      <th>标签12</th>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>标签13</th>\n","      <td>3435</td>\n","      <td>4003</td>\n","      <td>4463</td>\n","      <td>0.858106</td>\n","      <td>0.769662</td>\n","      <td>0.811481</td>\n","    </tr>\n","    <tr>\n","      <th>标签14</th>\n","      <td>4916</td>\n","      <td>5879</td>\n","      <td>5644</td>\n","      <td>0.836197</td>\n","      <td>0.871013</td>\n","      <td>0.853250</td>\n","    </tr>\n","    <tr>\n","      <th>综合</th>\n","      <td>63664</td>\n","      <td>76529</td>\n","      <td>76529</td>\n","      <td>0.831894</td>\n","      <td>0.831894</td>\n","      <td>0.831894</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58a133c5-0342-41d8-a9a5-399890805759')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-58a133c5-0342-41d8-a9a5-399890805759 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-58a133c5-0342-41d8-a9a5-399890805759');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         tp  all_real  all_predict    recall  precision        f1\n","标签0     941      1238         1225  0.760097   0.768163  0.764109\n","标签1    4638      5577         5709  0.831630   0.812401  0.821903\n","标签2    6586      7920         7961  0.831566   0.827283  0.829419\n","标签3    6539      7524         7077  0.869086   0.923979  0.895692\n","标签4    4013      5439         5172  0.737819   0.775909  0.756385\n","标签5    3025      3543         3338  0.853796   0.906231  0.879233\n","标签6    6470      7141         7376  0.906036   0.877169  0.891369\n","标签7    4705      5366         5566  0.876817   0.845311  0.860776\n","标签8    6796      8291         8197  0.819684   0.829084  0.824357\n","标签9    4096      4970         4894  0.824145   0.836943  0.830495\n","标签10   3480      4284         4713  0.812325   0.738383  0.773591\n","标签11   4024      5293         5193  0.760249   0.774889  0.767500\n","标签12      0        61            1  0.000000   0.000000       NaN\n","标签13   3435      4003         4463  0.858106   0.769662  0.811481\n","标签14   4916      5879         5644  0.836197   0.871013  0.853250\n","综合    63664     76529        76529  0.831894   0.831894  0.831894"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["toEstimate('TextRCNN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"id":"9BO6gVCkAwla","executionInfo":{"status":"ok","timestamp":1641216938115,"user_tz":-480,"elapsed":11098,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"f5bc7317-e5a0-4036-f43e-3140854fc2b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/76529 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","100%|██████████| 76529/76529 [00:06<00:00, 11402.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{0: {'tp': 922, 'all_real': 1238, 'all_predict': 1128}, 1: {'tp': 4376, 'all_real': 5577, 'all_predict': 5396}, 2: {'tp': 6537, 'all_real': 7920, 'all_predict': 8806}, 3: {'tp': 6482, 'all_real': 7524, 'all_predict': 7394}, 4: {'tp': 3936, 'all_real': 5439, 'all_predict': 5324}, 5: {'tp': 3006, 'all_real': 3543, 'all_predict': 3433}, 6: {'tp': 6336, 'all_real': 7141, 'all_predict': 7599}, 7: {'tp': 4601, 'all_real': 5366, 'all_predict': 5581}, 8: {'tp': 6436, 'all_real': 8291, 'all_predict': 8151}, 9: {'tp': 3911, 'all_real': 4970, 'all_predict': 4839}, 10: {'tp': 3278, 'all_real': 4284, 'all_predict': 4788}, 11: {'tp': 3729, 'all_real': 5293, 'all_predict': 4865}, 12: {'tp': 0, 'all_real': 61, 'all_predict': 0}, 13: {'tp': 3259, 'all_real': 4003, 'all_predict': 4076}, 14: {'tp': 4463, 'all_real': 5879, 'all_predict': 5149}}\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-6cf005ac-c9f4-4914-8f5c-b4bf2965882e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tp</th>\n","      <th>all_real</th>\n","      <th>all_predict</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>标签0</th>\n","      <td>922</td>\n","      <td>1238</td>\n","      <td>1128</td>\n","      <td>0.744750</td>\n","      <td>0.817376</td>\n","      <td>0.779374</td>\n","    </tr>\n","    <tr>\n","      <th>标签1</th>\n","      <td>4376</td>\n","      <td>5577</td>\n","      <td>5396</td>\n","      <td>0.784651</td>\n","      <td>0.810971</td>\n","      <td>0.797594</td>\n","    </tr>\n","    <tr>\n","      <th>标签2</th>\n","      <td>6537</td>\n","      <td>7920</td>\n","      <td>8806</td>\n","      <td>0.825379</td>\n","      <td>0.742335</td>\n","      <td>0.781657</td>\n","    </tr>\n","    <tr>\n","      <th>标签3</th>\n","      <td>6482</td>\n","      <td>7524</td>\n","      <td>7394</td>\n","      <td>0.861510</td>\n","      <td>0.876657</td>\n","      <td>0.869017</td>\n","    </tr>\n","    <tr>\n","      <th>标签4</th>\n","      <td>3936</td>\n","      <td>5439</td>\n","      <td>5324</td>\n","      <td>0.723662</td>\n","      <td>0.739294</td>\n","      <td>0.731395</td>\n","    </tr>\n","    <tr>\n","      <th>标签5</th>\n","      <td>3006</td>\n","      <td>3543</td>\n","      <td>3433</td>\n","      <td>0.848434</td>\n","      <td>0.875619</td>\n","      <td>0.861812</td>\n","    </tr>\n","    <tr>\n","      <th>标签6</th>\n","      <td>6336</td>\n","      <td>7141</td>\n","      <td>7599</td>\n","      <td>0.887271</td>\n","      <td>0.833794</td>\n","      <td>0.859701</td>\n","    </tr>\n","    <tr>\n","      <th>标签7</th>\n","      <td>4601</td>\n","      <td>5366</td>\n","      <td>5581</td>\n","      <td>0.857436</td>\n","      <td>0.824404</td>\n","      <td>0.840596</td>\n","    </tr>\n","    <tr>\n","      <th>标签8</th>\n","      <td>6436</td>\n","      <td>8291</td>\n","      <td>8151</td>\n","      <td>0.776263</td>\n","      <td>0.789596</td>\n","      <td>0.782873</td>\n","    </tr>\n","    <tr>\n","      <th>标签9</th>\n","      <td>3911</td>\n","      <td>4970</td>\n","      <td>4839</td>\n","      <td>0.786922</td>\n","      <td>0.808225</td>\n","      <td>0.797431</td>\n","    </tr>\n","    <tr>\n","      <th>标签10</th>\n","      <td>3278</td>\n","      <td>4284</td>\n","      <td>4788</td>\n","      <td>0.765173</td>\n","      <td>0.684628</td>\n","      <td>0.722663</td>\n","    </tr>\n","    <tr>\n","      <th>标签11</th>\n","      <td>3729</td>\n","      <td>5293</td>\n","      <td>4865</td>\n","      <td>0.704515</td>\n","      <td>0.766495</td>\n","      <td>0.734200</td>\n","    </tr>\n","    <tr>\n","      <th>标签12</th>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>标签13</th>\n","      <td>3259</td>\n","      <td>4003</td>\n","      <td>4076</td>\n","      <td>0.814139</td>\n","      <td>0.799558</td>\n","      <td>0.806783</td>\n","    </tr>\n","    <tr>\n","      <th>标签14</th>\n","      <td>4463</td>\n","      <td>5879</td>\n","      <td>5149</td>\n","      <td>0.759143</td>\n","      <td>0.866770</td>\n","      <td>0.809394</td>\n","    </tr>\n","    <tr>\n","      <th>综合</th>\n","      <td>61272</td>\n","      <td>76529</td>\n","      <td>76529</td>\n","      <td>0.800638</td>\n","      <td>0.800638</td>\n","      <td>0.800638</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6cf005ac-c9f4-4914-8f5c-b4bf2965882e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6cf005ac-c9f4-4914-8f5c-b4bf2965882e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6cf005ac-c9f4-4914-8f5c-b4bf2965882e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         tp  all_real  all_predict    recall  precision        f1\n","标签0     922      1238         1128  0.744750   0.817376  0.779374\n","标签1    4376      5577         5396  0.784651   0.810971  0.797594\n","标签2    6537      7920         8806  0.825379   0.742335  0.781657\n","标签3    6482      7524         7394  0.861510   0.876657  0.869017\n","标签4    3936      5439         5324  0.723662   0.739294  0.731395\n","标签5    3006      3543         3433  0.848434   0.875619  0.861812\n","标签6    6336      7141         7599  0.887271   0.833794  0.859701\n","标签7    4601      5366         5581  0.857436   0.824404  0.840596\n","标签8    6436      8291         8151  0.776263   0.789596  0.782873\n","标签9    3911      4970         4839  0.786922   0.808225  0.797431\n","标签10   3278      4284         4788  0.765173   0.684628  0.722663\n","标签11   3729      5293         4865  0.704515   0.766495  0.734200\n","标签12      0        61            0  0.000000        NaN       NaN\n","标签13   3259      4003         4076  0.814139   0.799558  0.806783\n","标签14   4463      5879         5149  0.759143   0.866770  0.809394\n","综合    61272     76529        76529  0.800638   0.800638  0.800638"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["toEstimate('TextRNN_Attention')  #model-rnn+Attention.h5 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"id":"Vgj2h452A1xs","executionInfo":{"status":"ok","timestamp":1641217063084,"user_tz":-480,"elapsed":11167,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"93fa605d-bc25-46ae-b6de-52baec119236"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/76529 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","100%|██████████| 76529/76529 [00:06<00:00, 11282.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{0: {'tp': 955, 'all_real': 1238, 'all_predict': 1202}, 1: {'tp': 4648, 'all_real': 5577, 'all_predict': 5532}, 2: {'tp': 6756, 'all_real': 7920, 'all_predict': 8219}, 3: {'tp': 6776, 'all_real': 7524, 'all_predict': 7493}, 4: {'tp': 4115, 'all_real': 5439, 'all_predict': 5188}, 5: {'tp': 3129, 'all_real': 3543, 'all_predict': 3522}, 6: {'tp': 6462, 'all_real': 7141, 'all_predict': 7189}, 7: {'tp': 4712, 'all_real': 5366, 'all_predict': 5422}, 8: {'tp': 6945, 'all_real': 8291, 'all_predict': 8470}, 9: {'tp': 4159, 'all_real': 4970, 'all_predict': 4956}, 10: {'tp': 3463, 'all_real': 4284, 'all_predict': 4450}, 11: {'tp': 4133, 'all_real': 5293, 'all_predict': 5215}, 12: {'tp': 0, 'all_real': 61, 'all_predict': 0}, 13: {'tp': 3321, 'all_real': 4003, 'all_predict': 3963}, 14: {'tp': 5003, 'all_real': 5879, 'all_predict': 5708}}\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-c3700c83-f005-47f9-afab-e5f80e8a7864\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tp</th>\n","      <th>all_real</th>\n","      <th>all_predict</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>标签0</th>\n","      <td>955</td>\n","      <td>1238</td>\n","      <td>1202</td>\n","      <td>0.771405</td>\n","      <td>0.794509</td>\n","      <td>0.782787</td>\n","    </tr>\n","    <tr>\n","      <th>标签1</th>\n","      <td>4648</td>\n","      <td>5577</td>\n","      <td>5532</td>\n","      <td>0.833423</td>\n","      <td>0.840202</td>\n","      <td>0.836799</td>\n","    </tr>\n","    <tr>\n","      <th>标签2</th>\n","      <td>6756</td>\n","      <td>7920</td>\n","      <td>8219</td>\n","      <td>0.853030</td>\n","      <td>0.821998</td>\n","      <td>0.837227</td>\n","    </tr>\n","    <tr>\n","      <th>标签3</th>\n","      <td>6776</td>\n","      <td>7524</td>\n","      <td>7493</td>\n","      <td>0.900585</td>\n","      <td>0.904311</td>\n","      <td>0.902444</td>\n","    </tr>\n","    <tr>\n","      <th>标签4</th>\n","      <td>4115</td>\n","      <td>5439</td>\n","      <td>5188</td>\n","      <td>0.756573</td>\n","      <td>0.793177</td>\n","      <td>0.774442</td>\n","    </tr>\n","    <tr>\n","      <th>标签5</th>\n","      <td>3129</td>\n","      <td>3543</td>\n","      <td>3522</td>\n","      <td>0.883150</td>\n","      <td>0.888416</td>\n","      <td>0.885775</td>\n","    </tr>\n","    <tr>\n","      <th>标签6</th>\n","      <td>6462</td>\n","      <td>7141</td>\n","      <td>7189</td>\n","      <td>0.904915</td>\n","      <td>0.898873</td>\n","      <td>0.901884</td>\n","    </tr>\n","    <tr>\n","      <th>标签7</th>\n","      <td>4712</td>\n","      <td>5366</td>\n","      <td>5422</td>\n","      <td>0.878122</td>\n","      <td>0.869052</td>\n","      <td>0.873563</td>\n","    </tr>\n","    <tr>\n","      <th>标签8</th>\n","      <td>6945</td>\n","      <td>8291</td>\n","      <td>8470</td>\n","      <td>0.837655</td>\n","      <td>0.819953</td>\n","      <td>0.828710</td>\n","    </tr>\n","    <tr>\n","      <th>标签9</th>\n","      <td>4159</td>\n","      <td>4970</td>\n","      <td>4956</td>\n","      <td>0.836821</td>\n","      <td>0.839185</td>\n","      <td>0.838001</td>\n","    </tr>\n","    <tr>\n","      <th>标签10</th>\n","      <td>3463</td>\n","      <td>4284</td>\n","      <td>4450</td>\n","      <td>0.808357</td>\n","      <td>0.778202</td>\n","      <td>0.792993</td>\n","    </tr>\n","    <tr>\n","      <th>标签11</th>\n","      <td>4133</td>\n","      <td>5293</td>\n","      <td>5215</td>\n","      <td>0.780843</td>\n","      <td>0.792522</td>\n","      <td>0.786639</td>\n","    </tr>\n","    <tr>\n","      <th>标签12</th>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>标签13</th>\n","      <td>3321</td>\n","      <td>4003</td>\n","      <td>3963</td>\n","      <td>0.829628</td>\n","      <td>0.838002</td>\n","      <td>0.833794</td>\n","    </tr>\n","    <tr>\n","      <th>标签14</th>\n","      <td>5003</td>\n","      <td>5879</td>\n","      <td>5708</td>\n","      <td>0.850995</td>\n","      <td>0.876489</td>\n","      <td>0.863554</td>\n","    </tr>\n","    <tr>\n","      <th>综合</th>\n","      <td>64577</td>\n","      <td>76529</td>\n","      <td>76529</td>\n","      <td>0.843824</td>\n","      <td>0.843824</td>\n","      <td>0.843824</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3700c83-f005-47f9-afab-e5f80e8a7864')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c3700c83-f005-47f9-afab-e5f80e8a7864 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c3700c83-f005-47f9-afab-e5f80e8a7864');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         tp  all_real  all_predict    recall  precision        f1\n","标签0     955      1238         1202  0.771405   0.794509  0.782787\n","标签1    4648      5577         5532  0.833423   0.840202  0.836799\n","标签2    6756      7920         8219  0.853030   0.821998  0.837227\n","标签3    6776      7524         7493  0.900585   0.904311  0.902444\n","标签4    4115      5439         5188  0.756573   0.793177  0.774442\n","标签5    3129      3543         3522  0.883150   0.888416  0.885775\n","标签6    6462      7141         7189  0.904915   0.898873  0.901884\n","标签7    4712      5366         5422  0.878122   0.869052  0.873563\n","标签8    6945      8291         8470  0.837655   0.819953  0.828710\n","标签9    4159      4970         4956  0.836821   0.839185  0.838001\n","标签10   3463      4284         4450  0.808357   0.778202  0.792993\n","标签11   4133      5293         5215  0.780843   0.792522  0.786639\n","标签12      0        61            0  0.000000        NaN       NaN\n","标签13   3321      4003         3963  0.829628   0.838002  0.833794\n","标签14   5003      5879         5708  0.850995   0.876489  0.863554\n","综合    64577     76529        76529  0.843824   0.843824  0.843824"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["toEstimate('transformerEncoder')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"DwUwFA_HKYG2","executionInfo":{"status":"ok","timestamp":1641301807983,"user_tz":-480,"elapsed":11598,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"11d4f454-9265-4770-efd3-10390432c300"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/76529 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","100%|██████████| 76529/76529 [00:08<00:00, 8731.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{0: {'tp': 987, 'all_real': 1238, 'all_predict': 1284}, 1: {'tp': 4606, 'all_real': 5577, 'all_predict': 5838}, 2: {'tp': 6553, 'all_real': 7920, 'all_predict': 8397}, 3: {'tp': 6612, 'all_real': 7524, 'all_predict': 7398}, 4: {'tp': 3895, 'all_real': 5439, 'all_predict': 5004}, 5: {'tp': 3065, 'all_real': 3543, 'all_predict': 3521}, 6: {'tp': 6389, 'all_real': 7141, 'all_predict': 7520}, 7: {'tp': 4562, 'all_real': 5366, 'all_predict': 5369}, 8: {'tp': 6802, 'all_real': 8291, 'all_predict': 8739}, 9: {'tp': 4148, 'all_real': 4970, 'all_predict': 5271}, 10: {'tp': 3330, 'all_real': 4284, 'all_predict': 4554}, 11: {'tp': 3566, 'all_real': 5293, 'all_predict': 4302}, 12: {'tp': 1, 'all_real': 61, 'all_predict': 1}, 13: {'tp': 3146, 'all_real': 4003, 'all_predict': 3735}, 14: {'tp': 4728, 'all_real': 5879, 'all_predict': 5596}}\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7e8a6dd3-5457-4062-a62c-2e6e0ccc6cd8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tp</th>\n","      <th>all_real</th>\n","      <th>all_predict</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>标签0</th>\n","      <td>987</td>\n","      <td>1238</td>\n","      <td>1284</td>\n","      <td>0.797254</td>\n","      <td>0.768692</td>\n","      <td>0.782712</td>\n","    </tr>\n","    <tr>\n","      <th>标签1</th>\n","      <td>4606</td>\n","      <td>5577</td>\n","      <td>5838</td>\n","      <td>0.825892</td>\n","      <td>0.788969</td>\n","      <td>0.807008</td>\n","    </tr>\n","    <tr>\n","      <th>标签2</th>\n","      <td>6553</td>\n","      <td>7920</td>\n","      <td>8397</td>\n","      <td>0.827399</td>\n","      <td>0.780398</td>\n","      <td>0.803211</td>\n","    </tr>\n","    <tr>\n","      <th>标签3</th>\n","      <td>6612</td>\n","      <td>7524</td>\n","      <td>7398</td>\n","      <td>0.878788</td>\n","      <td>0.893755</td>\n","      <td>0.886208</td>\n","    </tr>\n","    <tr>\n","      <th>标签4</th>\n","      <td>3895</td>\n","      <td>5439</td>\n","      <td>5004</td>\n","      <td>0.716124</td>\n","      <td>0.778377</td>\n","      <td>0.745954</td>\n","    </tr>\n","    <tr>\n","      <th>标签5</th>\n","      <td>3065</td>\n","      <td>3543</td>\n","      <td>3521</td>\n","      <td>0.865086</td>\n","      <td>0.870491</td>\n","      <td>0.867780</td>\n","    </tr>\n","    <tr>\n","      <th>标签6</th>\n","      <td>6389</td>\n","      <td>7141</td>\n","      <td>7520</td>\n","      <td>0.894693</td>\n","      <td>0.849601</td>\n","      <td>0.871564</td>\n","    </tr>\n","    <tr>\n","      <th>标签7</th>\n","      <td>4562</td>\n","      <td>5366</td>\n","      <td>5369</td>\n","      <td>0.850168</td>\n","      <td>0.849693</td>\n","      <td>0.849930</td>\n","    </tr>\n","    <tr>\n","      <th>标签8</th>\n","      <td>6802</td>\n","      <td>8291</td>\n","      <td>8739</td>\n","      <td>0.820408</td>\n","      <td>0.778350</td>\n","      <td>0.798826</td>\n","    </tr>\n","    <tr>\n","      <th>标签9</th>\n","      <td>4148</td>\n","      <td>4970</td>\n","      <td>5271</td>\n","      <td>0.834608</td>\n","      <td>0.786947</td>\n","      <td>0.810077</td>\n","    </tr>\n","    <tr>\n","      <th>标签10</th>\n","      <td>3330</td>\n","      <td>4284</td>\n","      <td>4554</td>\n","      <td>0.777311</td>\n","      <td>0.731225</td>\n","      <td>0.753564</td>\n","    </tr>\n","    <tr>\n","      <th>标签11</th>\n","      <td>3566</td>\n","      <td>5293</td>\n","      <td>4302</td>\n","      <td>0.673720</td>\n","      <td>0.828917</td>\n","      <td>0.743304</td>\n","    </tr>\n","    <tr>\n","      <th>标签12</th>\n","      <td>1</td>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>0.016393</td>\n","      <td>1.000000</td>\n","      <td>0.032258</td>\n","    </tr>\n","    <tr>\n","      <th>标签13</th>\n","      <td>3146</td>\n","      <td>4003</td>\n","      <td>3735</td>\n","      <td>0.785911</td>\n","      <td>0.842303</td>\n","      <td>0.813130</td>\n","    </tr>\n","    <tr>\n","      <th>标签14</th>\n","      <td>4728</td>\n","      <td>5879</td>\n","      <td>5596</td>\n","      <td>0.804218</td>\n","      <td>0.844889</td>\n","      <td>0.824052</td>\n","    </tr>\n","    <tr>\n","      <th>综合</th>\n","      <td>62390</td>\n","      <td>76529</td>\n","      <td>76529</td>\n","      <td>0.815247</td>\n","      <td>0.815247</td>\n","      <td>0.815247</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e8a6dd3-5457-4062-a62c-2e6e0ccc6cd8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7e8a6dd3-5457-4062-a62c-2e6e0ccc6cd8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7e8a6dd3-5457-4062-a62c-2e6e0ccc6cd8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         tp  all_real  all_predict    recall  precision        f1\n","标签0     987      1238         1284  0.797254   0.768692  0.782712\n","标签1    4606      5577         5838  0.825892   0.788969  0.807008\n","标签2    6553      7920         8397  0.827399   0.780398  0.803211\n","标签3    6612      7524         7398  0.878788   0.893755  0.886208\n","标签4    3895      5439         5004  0.716124   0.778377  0.745954\n","标签5    3065      3543         3521  0.865086   0.870491  0.867780\n","标签6    6389      7141         7520  0.894693   0.849601  0.871564\n","标签7    4562      5366         5369  0.850168   0.849693  0.849930\n","标签8    6802      8291         8739  0.820408   0.778350  0.798826\n","标签9    4148      4970         5271  0.834608   0.786947  0.810077\n","标签10   3330      4284         4554  0.777311   0.731225  0.753564\n","标签11   3566      5293         4302  0.673720   0.828917  0.743304\n","标签12      1        61            1  0.016393   1.000000  0.032258\n","标签13   3146      4003         3735  0.785911   0.842303  0.813130\n","标签14   4728      5879         5596  0.804218   0.844889  0.824052\n","综合    62390     76529        76529  0.815247   0.815247  0.815247"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# 各个模型推测"],"metadata":{"id":"weaRcwyWKkw-"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"HqaDO3pB-ETY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641305679258,"user_tz":-480,"elapsed":616,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"4a769b49-8565-4770-dc91-7b23709373e9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"]},{"output_type":"execute_result","data":{"text/plain":["(2, 0.8751038312911987)"]},"metadata":{},"execution_count":30}],"source":["import torch.nn.functional as F # pytorch 激活函数的类\n","from torch import nn,optim # 构建模型和优化器\n","import pickle as pk\n","import numpy as np\n","import torch\n","\n","def batch_yield_predict(chars,parameter):\n","    batch_x,batch_y = [],[]\n","    for iters in range(len(chars)):\n","        if chars[iters] in parameter['char2ind']:\n","            batch_x.append(parameter['ind2embeding'][parameter['char2ind'][chars[iters]]])\n","        else:\n","            batch_x.append(parameter['ind2embeding'][parameter['char2ind']['<unk>']])\n","    batch_x = [batch_x]\n","#     batch_y = [0]\n","    device = parameter['cuda']\n","    return torch.from_numpy(np.array(batch_x)).to(device)#,torch.from_numpy(np.array(batch_y)).to(device).long()\n","\n","class TextRCNN(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextRCNN, self).__init__()\n","        embedding_dim = parameter['embedding_dim']\n","        hidden_size = parameter['hidden_size']\n","        output_size = parameter['output_size']\n","        num_layers = parameter['num_layers']\n","        dropout = parameter['dropout']\n","        self.lstm = nn.LSTM(embedding_dim,hidden_size, \\\n","                            num_layers, bidirectional=True, \\\n","                            batch_first=True, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        #self.fc = nn.Linear(hidden_size*2+embedding_dim, output_size)\n","        self.fc = nn.Linear(hidden_size*2, output_size)#张宁修改\n","        self.fc_for_concat = nn.Linear(hidden_size * 2 + embedding_dim, hidden_size * 2) #张宁新增\n","\n","    def forward(self, x):\n","        out,(h, c)= self.lstm(x)\n","        #out = torch.cat((x, out), 2)\n","        out = self.fc_for_concat(torch.cat((x, out), 2))\n","        # 激活函数\n","        out = F.tanh(out)\n","        out = out.permute(0, 2, 1)\n","        try:\n","            out = F.max_pool1d(out, out.size(2).item())\n","        except:\n","            out = F.max_pool1d(out, out.size(2))\n","        out = out.squeeze(-1)\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n","    \n","def load_model():\n","    parameter = pk.load(open('parameter.pkl','rb'))\n","    model = TextRCNN(parameter).to(parameter['cuda'])\n","    model.load_state_dict(torch.load('model-rcnn.h5'))\n","    return parameter,model\n","\n","def predict(model,parameter,strs):\n","    strs = strs.split()\n","    strs = batch_yield_predict(strs,parameter)\n","    outputs = model(strs)\n","    predicted_prob,predicted_index = torch.max(F.softmax(outputs, 1), 1)\n","    return predicted_index.item(),predicted_prob.item()\n","    \n","\n","parameter,model = load_model()\n","\n","test = 'nnt 演 过 那 些 戏'\n","predict(model,parameter,test)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcikiTA0-ETZ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T16:40:47.651733Z","start_time":"2021-08-07T16:40:47.635776Z"},"id":"mr52lDjb-ETa","executionInfo":{"status":"ok","timestamp":1641304553044,"user_tz":-480,"elapsed":404,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[],"source":["class TextRNN_Attention(nn.Module):\n","    def __init__(self, parameter):\n","        super(TextRNN_Attention, self).__init__()\n","        embedding_dim = parameter['embedding_dim']\n","        hidden_size = parameter['hidden_size']\n","        output_size = parameter['output_size']\n","        num_layers = parameter['num_layers']\n","        dropout = parameter['dropout']\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n","        self.fc_attention = nn.Linear(hidden_size*2, hidden_size*2)\n","        self.fc = nn.Linear(hidden_size*2, output_size)\n","        \n","        # attention\n","        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n","        \n","    def forward(self, x):\n","        out,(h, c)= self.lstm(x)\n","        \n","        alpha = F.softmax(torch.matmul(F.tanh(self.fc_attention(out)),self.w),dim = 1).unsqueeze(-1)\n","        out = F.relu(torch.sum(out * alpha,1))\n","        out = self.fc(out)\n","        return out,alpha"]},{"cell_type":"code","execution_count":18,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T16:45:08.629842Z","start_time":"2021-08-07T16:45:06.878526Z"},"id":"HC5uL4fP-ETb","outputId":"32639b4b-86e1-4c5f-94fa-358c19969d6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641304560238,"user_tz":-480,"elapsed":3507,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":18}],"source":["way = 'TextRNN_Attention'\n","[train_chars,test_chars,train_labels,test_labels] = pk.load(open('dataSet.pkl','rb'))\n","parameter = pk.load(open('parameter.pkl','rb'))\n","parameter['dropout'] = 0\n","model = eval(way+\"(parameter).to(parameter['cuda'])\")\n","model.load_state_dict(torch.load('model-rnn+Attention.h5'))"]},{"cell_type":"code","execution_count":23,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T16:48:14.341174Z","start_time":"2021-08-07T16:48:14.319232Z"},"id":"IwqzVrM--ETd","outputId":"cc95d332-25ed-46f7-e3e8-474a29240fdb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641304749362,"user_tz":-480,"elapsed":419,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TextRNN_Attention(\n","  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, bidirectional=True)\n","  (fc_attention): Linear(in_features=256, out_features=256, bias=True)\n","  (fc): Linear(in_features=256, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":23},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  after removing the cwd from sys.path.\n","100%|██████████| 4/4 [00:00<00:00, 28197.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," **************************************************\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.return_types.max(values=tensor([0.8002, 0.9973, 0.9942, 0.9937], device='cuda:0',\n","       grad_fn=<MaxBackward0>), indices=tensor([ 1,  1,  6, 11], device='cuda:0'))"]},"metadata":{},"execution_count":23},{"output_type":"stream","name":"stdout","text":["\n"," --------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.   , 0.8  , 0.001, 0.001, 0.129, 0.001, 0.001, 0.001, 0.025,\n","        0.003, 0.002, 0.015, 0.001, 0.01 , 0.011],\n","       [0.   , 0.997, 0.   , 0.002, 0.   , 0.   , 0.   , 0.   , 0.   ,\n","        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n","       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.994, 0.   , 0.005,\n","        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n","       [0.   , 0.   , 0.001, 0.   , 0.001, 0.   , 0.   , 0.   , 0.   ,\n","        0.004, 0.   , 0.994, 0.   , 0.   , 0.   ]], dtype=float32)"]},"metadata":{},"execution_count":23}],"source":["model.eval() \n","device = parameter['cuda']\n","parameter['epoch'] = 1\n","test_ins = np.array(['一 元 硬 币 是 这 种 现 在 价 值 num 元 找 找 看'.split(),\\\n","                         '下 联 夕 陽 西 下 已 黄 昏 上 联 是 什 麽'.split(),\\\n","                    '十 万 元 能 上 路 的 汽 车 买 什 么 比 较 好'.split(),\\\n","                    '特 朗 普 为 支 持 拥 枪 拿 伦 敦 当 反 面 教 材'.split()]),np.array([1,1,6,11])\n","test_yield = batch_yield(*test_ins,parameter,shuffle = False)\n","seqs,labels,x_len,keys,epoch = next(test_yield)\n","out,alpha = model(seqs)\n","print('\\n','*'*50)\n","torch.max(F.softmax(out, 1), 1)\n","print('\\n','-'*50)\n","np.around(F.softmax(out, 1).cpu().detach().numpy(),3)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T17:26:45.768543Z","start_time":"2021-08-07T17:26:45.721668Z"},"id":"6JhiZnlf-ETd","outputId":"0a8cd787-3cbb-467f-d867-8621a6c74423"},"outputs":[{"data":{"text/plain":["tensor([[[-3.2381e+00,  1.2783e-01, -1.4556e+00,  ...,  2.5821e+00,\n","           4.7758e+00, -3.7567e-01],\n","         [ 4.6198e-01, -7.2332e-01,  3.2733e+00,  ..., -1.8295e+00,\n","           1.4050e+00, -5.9407e+00],\n","         [ 3.0078e+00,  4.3929e-01, -8.5224e-01,  ...,  4.5261e-01,\n","          -2.8140e+00, -1.2985e+00],\n","         ...,\n","         [-1.4830e+00, -7.9927e-01, -1.0464e+00,  ...,  1.4740e+00,\n","           1.0086e+00, -6.1044e-02],\n","         [-1.4830e+00, -7.9927e-01, -1.0464e+00,  ...,  1.4740e+00,\n","           1.0086e+00, -6.1044e-02],\n","         [-1.5632e-01,  8.3224e-01, -2.1072e+00,  ...,  8.6913e-01,\n","           2.6341e+00,  3.1374e+00]],\n","\n","        [[ 1.7158e-01, -4.2617e-01, -4.8433e-01,  ..., -4.3868e-01,\n","           2.1605e+00,  5.1938e-01],\n","         [ 1.1475e+00, -3.1508e+00,  6.4085e+00,  ...,  6.7769e-01,\n","           2.4792e+00,  3.3659e+00],\n","         [ 1.0284e+00,  5.8255e+00, -1.5697e+00,  ..., -1.1450e-01,\n","           8.7394e-02, -3.1986e+00],\n","         ...,\n","         [ 7.0657e-03,  2.3605e-02, -5.0100e-03,  ..., -5.9170e-03,\n","          -1.1207e-02,  7.7851e-03],\n","         [-2.2930e-02,  2.5490e-02, -6.2729e-03,  ...,  1.4266e-02,\n","          -2.7251e-02,  9.7082e-03],\n","         [-2.2930e-02,  2.5490e-02, -6.2729e-03,  ...,  1.4266e-02,\n","          -2.7251e-02,  9.7082e-03]],\n","\n","        [[-3.4959e+00,  7.9662e-01,  1.0738e-01,  ..., -2.1564e+00,\n","          -1.7871e+00,  1.2063e+00],\n","         [ 1.8323e+00,  1.2064e+00,  7.0225e-01,  ..., -2.9383e+00,\n","           2.2151e+00, -4.8074e+00],\n","         [ 4.6198e-01, -7.2332e-01,  3.2733e+00,  ..., -1.8295e+00,\n","           1.4050e+00, -5.9407e+00],\n","         ...,\n","         [-1.3529e+00,  1.8998e+00, -7.0678e-01,  ...,  3.2658e+00,\n","           4.0264e-01,  1.7029e+00],\n","         [ 3.3567e-01, -1.4175e+00,  3.8146e+00,  ..., -2.3963e+00,\n","           3.3276e+00,  3.4062e+00],\n","         [-2.2930e-02,  2.5490e-02, -6.2729e-03,  ...,  1.4266e-02,\n","          -2.7251e-02,  9.7082e-03]],\n","\n","        [[ 7.3000e-01,  8.0273e-01,  2.7689e+00,  ...,  3.0663e-01,\n","          -2.7907e+00, -2.1870e+00],\n","         [ 2.9885e+00,  3.1719e+00, -7.2723e-01,  ...,  3.2050e+00,\n","           4.7315e-01, -2.7227e+00],\n","         [-2.0014e+00, -1.6207e+00,  2.3460e+00,  ..., -1.1435e+00,\n","          -4.7551e+00,  2.8217e+00],\n","         ...,\n","         [-6.8665e+00, -1.8022e+00, -2.4155e+00,  ...,  8.4056e-01,\n","          -1.2794e+00, -3.3074e-01],\n","         [-1.8251e+00, -3.6359e-01,  1.6676e+00,  ..., -3.2242e+00,\n","          -1.3988e+00,  4.4412e+00],\n","         [-1.4996e+00, -1.8066e+00,  9.2648e-02,  ..., -1.4956e-01,\n","           9.0076e-01, -3.2040e-01]]], device='cuda:0')"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["seqs"]},{"cell_type":"code","execution_count":24,"metadata":{"ExecuteTime":{"end_time":"2021-08-07T16:46:49.631538Z","start_time":"2021-08-07T16:46:49.527816Z"},"id":"8fANJWNV-ETe","outputId":"38b92fc7-03e3-4103-eabd-8afcccc130b2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1641304858721,"user_tz":-480,"elapsed":1545,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['特', '朗', '普', '为', '支', '持', '拥', '枪', '拿', '伦', '敦', '当', '反', '面', '教', '材']\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PolyCollection at 0x7fdb876189d0>"]},"metadata":{},"execution_count":24},{"output_type":"stream","name":"stderr","text":["findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 29305 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 26391 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 26222 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 20026 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25903 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25345 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25317 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 26538 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25343 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 20262 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25958 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 24403 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 21453 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 38754 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25945 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 26448 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 29305 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 26391 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 26222 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 20026 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25903 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25345 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25317 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 26538 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25343 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 20262 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25958 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 24403 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 21453 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 38754 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25945 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 26448 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","plt.rcParams['font.sans-serif'] = 'Microsoft YaHei'\n","plt.rcParams['axes.unicode_minus'] = False\n","\n","ins_num = 3\n","y_label = test_ins[0][ins_num]\n","attention = alpha[ins_num,:,0].cpu().detach().numpy()\n","target = [attention]\n","y_label += ['pad']*(len(attention) - len(y_label))\n","print(y_label)\n","plt.pcolor(target, cmap=plt.cm.Reds, edgecolors = 'white')\n","# 添加x轴和y轴刻度标签(加0.5是为了让刻度标签居中显示)\n","a = plt.xticks(np.arange(len(attention))+0.5,y_label)"]},{"cell_type":"code","source":[""],"metadata":{"id":"jlocCtFnZJbq"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"意图识别推理及评测.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}