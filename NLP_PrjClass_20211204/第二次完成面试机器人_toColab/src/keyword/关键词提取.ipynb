{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T15:24:26.478310Z",
     "start_time": "2021-08-26T15:24:26.123143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import torch \n",
    "import jieba\n",
    "import json\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定模型训练方式，GPU训练或CPU训练\n",
    "parameter_copy = {\n",
    "    # 此处embedding维度为768\n",
    "    'd_model':768, \n",
    "    # rnn的隐层维度为300\n",
    "    'hid_dim':300,\n",
    "    # 训练的批次为100轮\n",
    "    'epoch':20,\n",
    "    # 单次训练的batch_size为100条数据\n",
    "    'batch_size':50,\n",
    "    # 设置序列的最大长度为100\n",
    "    'n_layers':2,\n",
    "    # 设置dropout，为防止过拟合\n",
    "    'dropout':0.1,\n",
    "    # 配置cpu、gpu\n",
    "    'device':device,\n",
    "    # 设置训练学习率\n",
    "    'lr':0.001,\n",
    "    # 优化器的参数，动量主要用于随机梯度下降\n",
    "    'momentum':0.99,\n",
    "    'max_len':50,\n",
    "}\n",
    "\n",
    "def build_dataSet(parameter,data_path = '../../dataSet/tagging.txt'):\n",
    "    data = open(data_path,'r',encoding = 'utf-8').readlines()\n",
    "    data_set = {'input':[],'label':[]}\n",
    "    key_table = defaultdict(int)\n",
    "    vocab_table = defaultdict(int)\n",
    "    vocab_table['<PAD>'] = 0\n",
    "    vocab_table['<UNK>'] = 0\n",
    "    for i in data:\n",
    "        i = i.strip().split()\n",
    "        data_set['input'].append(i[0])\n",
    "        data_set['label'].append(i[1])\n",
    "        vocab_table[i[0]] += 1\n",
    "        key_table[i[1]] += 1\n",
    "    key2ind = dict(zip(key_table.keys(),range(len(key_table))))\n",
    "    ind2key = dict(zip(range(len(key_table)),key_table.keys()))\n",
    "    word2ind = dict(zip(vocab_table.keys(),range(len(vocab_table))))\n",
    "    ind2word = dict(zip(range(len(vocab_table)),vocab_table.keys()))\n",
    "    parameter['key2ind'] = key2ind\n",
    "    parameter['ind2key'] = ind2key\n",
    "    parameter['word2ind'] = word2ind\n",
    "    parameter['ind2word'] = ind2word\n",
    "    parameter['data_set'] = data_set\n",
    "    parameter['output_size'] = len(key2ind)\n",
    "    parameter['word_size'] = len(word2ind)\n",
    "    return parameter\n",
    "\n",
    "def sample(parameter):\n",
    "    while 1:\n",
    "        data_set = parameter['data_set']\n",
    "        select_id = random.randint(0,len(data_set['label'])-parameter['max_len'])\n",
    "        select_id = [select_id,select_id+parameter['max_len']-1]\n",
    "        while data_set['label'][select_id[0]][0] not in ['O','B','S'] and select_id[0] < len(data_set['label']):\n",
    "            select_id[0] += 1\n",
    "        while data_set['label'][select_id[1]][0] not in ['O','E','S'] and select_id[1] > 0:\n",
    "            select_id[1] -= 1\n",
    "        if select_id[1] > select_id[0] and \\\n",
    "            data_set['label'][select_id[0]][0] in ['O','B','S'] and \\\n",
    "            data_set['label'][select_id[1]][0] in ['O','E','S']:\n",
    "            select_label = data_set['label'][select_id[0]:select_id[1]+1]\n",
    "            select_input = data_set['input'][select_id[0]:select_id[1]+1]\n",
    "            return select_input,select_label\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def batch_yield(parameter):\n",
    "    Epoch = parameter['epoch'] \n",
    "    for epoch in range(Epoch):\n",
    "        inputs,targets = [],[]\n",
    "        max_len = 0\n",
    "        for items in tqdm(range(10000)):\n",
    "            input,label = sample(parameter)\n",
    "            input = tokenizer.convert_tokens_to_ids(input)\n",
    "            label = itemgetter(*label)(parameter['key2ind'])\n",
    "            label = label if type(label) == type(()) else (label,0)\n",
    "            if len(input) > max_len:\n",
    "                max_len = len(input)\n",
    "            inputs.append(list(input))\n",
    "            targets.append(list(label))\n",
    "            if len(inputs) >= parameter['batch_size']:\n",
    "                inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n",
    "                targets = [i+[0]*(max_len-len(i)) for i in targets]\n",
    "                if items < 10000-1:\n",
    "                    yield list2torch(inputs),list2torch(targets),None,False\n",
    "                else:\n",
    "                    yield list2torch(inputs),list2torch(targets),epoch,False\n",
    "                inputs,targets = [],[]\n",
    "                max_len = 0\n",
    "        inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n",
    "        targets = [i+[0]*(max_len-len(i)) for i in targets]\n",
    "    yield None,None,None,True\n",
    "            \n",
    "\n",
    "def list2torch(ins):\n",
    "    return torch.from_numpy(np.array(ins)).long().to(parameter['device'])\n",
    "\n",
    "parameter = build_dataSet(parameter_copy)\n",
    "pk.dump(parameter,open('parameter.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T15:24:28.577492Z",
     "start_time": "2021-08-26T15:24:28.381489Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, BertConfig,get_linear_schedule_with_warmup,AdamW, BertTokenizer\n",
    "from transformers import BertModel,BertPreTrainedModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F # pytorch 激活函数的类\n",
    "from torch import nn,optim # 构建模型和优化器\n",
    "from torchcrf import CRF\n",
    "\n",
    "class bert_crf(BertPreTrainedModel):\n",
    "    def __init__(self, config,parameter):\n",
    "        super(bert_crf, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        embedding_dim = parameter['d_model']\n",
    "        output_size = parameter['output_size']\n",
    "        self.fc = nn.Linear(embedding_dim, output_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.crf = CRF(output_size,batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,labels=None):\n",
    "        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.fc(sequence_output)\n",
    "        return logits\n",
    "    \n",
    "config_class, bert_crf, tokenizer_class = BertConfig, bert_crf, BertTokenizer\n",
    "config = config_class.from_pretrained(\"prev_trained_model\")\n",
    "tokenizer = tokenizer_class.from_pretrained(\"prev_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T15:59:05.553933Z",
     "start_time": "2021-08-26T15:24:50.716373Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_crf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d6eff4fac6fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# 构建模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_crf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prev_trained_model\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 决定训练权重\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_crf' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle as pk\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "# 构建模型\n",
    "model = bert_crf.from_pretrained(\"prev_trained_model\",config=config,parameter = parameter).to(parameter['device'])\n",
    "\n",
    "# 决定训练权重\n",
    "full_finetuning = True\n",
    "if full_finetuning:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "else: \n",
    "        param_optimizer = list(model.fc.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "# 确定优化器和策略\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, correct_bias=False)\n",
    "train_steps_per_epoch = 10000 // parameter['batch_size']\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=train_steps_per_epoch, num_training_steps=parameter['epoch'] * train_steps_per_epoch)\n",
    "\n",
    "# 确定训练模式\n",
    "model.train()\n",
    "\n",
    "# 准备迭代器\n",
    "train_yield = batch_yield(parameter)\n",
    "\n",
    "# 开始训练\n",
    "loss_cal = []\n",
    "min_loss = float('inf')\n",
    "logging_steps = 0\n",
    "while 1:\n",
    "        inputs,targets,epoch,keys = next(train_yield)\n",
    "        if keys:\n",
    "            break\n",
    "        out = model(inputs)\n",
    "        loss = -model.crf(out,targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_cal.append(loss.item())\n",
    "        logging_steps += 1\n",
    "        if logging_steps%20 == 0:\n",
    "            print(sum(loss_cal)/len(loss_cal))\n",
    "        if epoch is not None:\n",
    "            if (epoch+1)%1 == 0:\n",
    "                loss_cal = sum(loss_cal)/len(loss_cal)\n",
    "                if loss_cal < min_loss:\n",
    "                    min_loss = loss_cal\n",
    "                    torch.save(model.state_dict(), 'bert_crf.h5')\n",
    "                print('epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \\\n",
    "                                                       parameter['epoch'],loss_cal))\n",
    "            loss_cal = [loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
