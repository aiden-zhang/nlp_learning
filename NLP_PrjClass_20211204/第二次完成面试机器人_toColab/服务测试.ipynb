{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T17:34:36.152112Z",
     "start_time": "2021-08-26T17:34:30.873954Z"
    }
   },
   "source": [
    "# 关键词提取及打分算法功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T05:40:19.906726Z",
     "start_time": "2021-08-27T05:40:15.436490Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.grade_model import grade_predict\n",
    "from src.keyword_model import keyword_predict_long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T17:34:38.249585Z",
     "start_time": "2021-08-26T17:34:38.224503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6618735194206238, 0.17569005489349365)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'AutoML参数选择所使用的方法？'\n",
    "a_p = 'random_search'\n",
    "a_n = '增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差'\n",
    "grade_predict(q,a_p),grade_predict(q,a_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T05:40:30.463435Z",
     "start_time": "2021-08-27T05:40:30.088087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['训练']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '模型训练为什么要引入偏差和方差？请理论论证。'\n",
    "keyword_predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T06:13:35.006645Z",
     "start_time": "2021-08-27T06:13:33.607152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SparkSQL',\n",
       " '代数',\n",
       " '  kafka  ',\n",
       " '召回',\n",
       " ' Redis  ',\n",
       " ' A ',\n",
       " '   7',\n",
       " '挑战性',\n",
       " '数据挖掘',\n",
       " '         8',\n",
       " ' IT ',\n",
       " 'Flume  ',\n",
       " ' Java ',\n",
       " ' SparkMLlib ',\n",
       " '预处理',\n",
       " ' H  R  ',\n",
       " ' HQL ',\n",
       " '好奇心',\n",
       " ' Hadoop ',\n",
       " '负载',\n",
       " ' JVM ',\n",
       " '调度',\n",
       " ' Hive ',\n",
       " '  Sqoop  ',\n",
       " ' HDFS ',\n",
       " 'RF',\n",
       " '统计',\n",
       " ' HBase ',\n",
       " 'BD ',\n",
       " '人工智能',\n",
       " 'Mace ',\n",
       " '维度',\n",
       " '指标分析',\n",
       " '准确性',\n",
       " 'DW ',\n",
       " '       ',\n",
       " '  HDFS  ',\n",
       " '预测',\n",
       " '     ',\n",
       " 'Sparktng ',\n",
       " '优化',\n",
       " ' Hie ',\n",
       " ' Flink ',\n",
       " '推荐',\n",
       " '评级',\n",
       " ' Spark ',\n",
       " ' L',\n",
       " '目标',\n",
       " '训练',\n",
       " '抽取',\n",
       " '回归',\n",
       " ' Java',\n",
       " ' ID ',\n",
       " 'Yarn  ',\n",
       " '机器学习',\n",
       " '快速',\n",
       " '    ',\n",
       " '加工',\n",
       " '  ',\n",
       " '  Redis  ',\n",
       " ' ➢ ',\n",
       " ' ODS ',\n",
       " '特征',\n",
       " '均衡',\n",
       " 'B ',\n",
       " ' Sqoop ',\n",
       " '  Hase  ',\n",
       " '   ',\n",
       " '挖掘',\n",
       " '逻辑']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.keyword_model import keyword_predict_long_text\n",
    "\n",
    "#假设已经完成pdf转txt接口开发得到txt:\n",
    "long_text = '大数据开发工程师       个人信息     名：                           性        别：  男   xxxx x 岁x x x xx 年  姓 年        龄： 籍        贯： 工作经验：                                                    学        历：    本科，工科学士                                             联系电话：                                                                 电子邮件：      求职意向           工作性质：全职                目标职能：大数据数仓开发工程师        目标地点：上海     目标薪资：面议  教育经历 教育经历  学校名称：清华大学             层次： 毕业时间：2017 年 6 月 专业： 生物工程      工作经历          单位名称：xxxxxxxxxxx          职位名称：大数据数仓开发工程师            工作时间：2017.6-2019 年 2 月          工作描述：1、对接各部门，根据需求进行脚本开发，对数据进行逻辑加工，源数据运维  2、需求调研，需求分析，功能模块分析以及编码实现等。                       3、ERwin  ，Cognos 等工具数据仓库模型的设计优，输出逻辑数据模型 并完成物理化工作 4、采用 Kettle，DataStage 等工具进行 ETL、ELT 开发 5、Tableau, PowerBI ,FineBI  进行数据看板开发 6、降范式建模，源数据 ID 调研，BD 需求调研，数据集成 7、核心系统，个人信贷系统，对公信贷系统等 19 个数据源系统数据调研，输出调研文档，根据调研情况进行模型设计，输出    Mapping 设计文档，中科天玑大数据平台进行HQL，SQL 开发和优化         \f",
    "单位名称： x x x x x x              职位名称：机器学习/数据挖掘工程师（初级） 工作时间：2019.2-至今            工作描述：1、项目的需求分析                        2、项目的架构设计                         3、项目的扩展性设计                                       4、Atlas 元数据管理                         5、采用即席查询工具 Presto,Druid,Kylin，进行指标分析                         6、Griffin  质量监控，分析数据异常，并处理异常                         7.  评估模型的精读，召回率，  准确率                         8.  构建并改善识别付款欺诈行为的线性模型                         9.  处理时间数据调整数据格式，使数据适合训练机器学习模型                         10.使用 SageMaker 的 DeepAr 算法进行时间序列预测    专业技能 1、  熟练 Spark 底层原理和运行流程，能使用 SparkCore、SparkSQL、SparkStreaming 进行开发，了解 SparkMLlib 并能使用其实现数据模型的加载，训练，系数调优以及模型的评估； 2、  熟悉 Hadoop 工作流程以及原理，包括  HDFS  的读写流程，MapReduce 原理流程，Yarn  任务调度以及元数据的管理 3、  熟练使用 Hive 构建数据仓库并能做一定的 Hive 调优 4、  熟练 HBase  架构，熟悉  RowKey  设计原则，了解  HBase  优化 5、  熟悉 Redis  内存数据库，了解  Redis  的持久化 6、  熟悉  kafka  集群的架构，消息不丢失、负载均衡等机制，了解集群的搭建与调优   7、  熟悉 Flink 底层原理，能够使用 Flink 开发常见的指标分析 8、  熟练使用  Sqoop  数据迁移框架、Flume  采集工具。 9、  精通 Java，Scala 语言，了解 JVM 调优，有前后端开发能力。 10、 精通 Linux，了解 Shell 编程； 11、 精通 Mysql，Oracle,有 SQL 调优能力. 12、 了解机器学习的常见算法，如决策树，随机森林。      项目经历 ⚫ 金融产品个性化推荐平台 ➢  开发环境：Windows + JDK1.8 +IDEA+Linux+Scala ➢  系统架构  ：Hive+Zookeeper+SparkMLlib+Flume+HDFS+Sqoop+Redis+Mysql +Kafka +Hbase ➢  开发时间/开发周期：➢   项目目标：  公司重点开发数据挖掘、数据分析、人工智能等相关领域产品。为相关理财产品推广并获取更多用户，让用户选择更适合自己的产品。对前期用户消费行为、额度使用情况、 \f",
    "业务订购等不同历史行为进行维度分析；通过聚类分析建立模型，对用户业务推广提升效率。通过分析用户大量历史数据获取用户潜在 规律和价值，智能为用户推荐喜欢的业务和产品。再和基本特征权重加和排序取出  ➢  责任描述：   1.  参与项目初期设计，需求调研、技术选型、文档编写； 2.  将 MySQL 数据导入到大数据 HDFS 文件系统，用 HQL 分析处理数据；将相关数据导入 Hive数据仓库，进行后期数据处理，将 Hive 中数据转化成建模所需格式； 3.  通过 SparkStreaming 对数据清洗并进行分析，为算法同事提供模型调优数据，将数据模型转为推广到用户群的产品； 4.    用将清洗后的数据用来模型训练，构建特征索引，以便通过逻辑回归算法构建模型放在 redis中； 5.    对产品特征数据进行抽取以及关联的历史购买记录维度数据进行优化，保证推荐结果准确性； 6.    使用 ALS 协同过滤算法，将推荐算法与业务系统紧密结合； 7.    对用户历史数据和负例样本作去噪处理，保证模型准确度； 8.    从 Hbase 获取产品基本特征和用户历史购买数据，从 Redis 读出模型，预测排名                        ⚫ 用户画像平台 ➢   开发环境：Windows + JDK1.8 + Hadoop-2.6.0+IDEA+linux+Mysql+Scala2.11 ➢  系统架构：Saprk+hadoop+Zookeeper+Flume+Hive+Hbase+sqoop ➢  开发时间/开发周期： ➢   项目目标：           通过采集网站用户访问日志数据和已有业务系统的静态数据，进行数仓建模，提炼用户画像，为银行实现精准营销、推荐系统、完善产品运营等做数据支持。将业务系统的业务数据从 mysql 中的 user 表、order 表和 log 日志表导入到 Hbase 中，加载 Hbase 中的原始数据以及 MySQL 用户标签规则数据，用 spark 对匹配型、统计型和挖掘型标签进行计算,最终得到标签结果,将结果存入 HBase 中.  ➢  主要职责：                     1.负责前期用户画像需求分析与架构设计； 2.使用 Flume 采集用户信息数据、用户交易数据、用户访问日志数据到 HDFS 中； 3.在 Hive 中建立用户画像模型表，并将之映射到 Hbase 中 4.Spark 与  Hive  的整合，利用  Spark  引擎计算用户的标签属性。 5..负责用户画像中匹配型标签和统计型标签的计算 6..使用机器学习算法完成 RFE 用户活跃度模型、RFM 户价值模型、PSM 价格敏感度模型等算法挖掘型标签的开发和算法参数对的调优 7.建立 solr 与 Hbase 之间的映射并最终将数据展示在 web 页面上           \f",
    "⚫ 信用评级风控系统 ➢  开发环境：Windows + JDK1.8 + Hadoop-2.6.0+IDEA+linux+Mysql+Maven3.0.5 ➢  软件架构：Hadoop+ Flume+Hbase+Hive+Sqoop+Mysql ➢  开发时间/开发周期： ➢  项目目标： 该项目通过电商部门数据结合互联网数据对商户信用评级，根据评级给商户划定贷款额度。主要分析的数据来源有公司的业务数据，通过和其他数据公司的整合数据等，通过分析商户的资产状况，生活习惯，个人信息以及交际圈得出信用等级，给公司的金融部门做决策。 ➢ 责任描述：  1.使用 Flume 将业务系统产生的日志数据采集到 HDFS 中，为使用 Hive 做数据分析工作做准备。 2.使用 Sqoop 将业务数据库的数据导入到 Hive 表中，用来分析查询。 3.对来自第三方的数据使用 Spark 程序进行数据的预处理清洗工作，将不合法数据以及缺失字段的数据做标记逻辑删除。 4.将存在于 HDFS 上的数据映射到 Hive 表中，整合 Spark SQL 做数据 ETL 工作，构建数据仓库。 5.使用 Hive 构建数据仓库中 ODS 层数据表，DW 层维度表，事实表，使用 HQL 抽取数据转换数据到相应的数据层。 6.将 Hive 分析后的数据导入到 HBase 中，为业务部分查询分析做准备。 7.使用 Phoenix+Hbase 整合做查询优化分析，构建用户画像，分析用户风险指标。 8.使用 Sqoop 将数据导出到 MySQL 数据库中，为其他部门做深度数据挖掘，算法分析工作做数据支撑。 9.使用逻辑回归构建信用评分卡 A 卡、B 卡、C 卡   自我评价   1、参与过多个项目的开发，熟悉常见业务处理，能快速融入团队； 2、逻辑思维能力强，思路清楚，学习能力强，对新技术有着强烈的好奇心； 3、对工作尽职尽责，乐于从事有挑战性的工作； 4、乐于与用户以及同事和领导沟通，以便快速解决项目遇到的问题； 5、具有良好的英语阅读能力，能阅读英文资料、技术文档等； 6、高考物理满分，大学高数一 98 分，线性代数，概率论，统计学高分通过，对数字数据较为敏感； 7、初中开始接触计算机编程，热爱游戏和编程开发，热爱 IT 行业新技术；         8、扎实的 Java 基础，良好的编码风格。   '\n",
    "\n",
    "#根据简历内容得到关键词\n",
    "keyword = keyword_predict_long_text(long_text)\n",
    "keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neo4j搜索功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T04:47:43.825631Z",
     "start_time": "2021-08-27T04:47:43.258032Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.neo4j_model import neo4j_model,neo4j_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T04:47:49.728385Z",
     "start_time": "2021-08-27T04:47:45.631626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归',\n",
       " '遇到过的机器学习中的偏差与方差问题？',\n",
       " '长文本预测如何构造Tokens？',\n",
       " '不定长文本数据如何输入deepFM？']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#根据问题的关键词搜索问题\n",
    "question = neo4j_model.feature2question(['机器学习','长文本'])\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T04:47:49.933747Z",
     "start_time": "2021-08-27T04:47:49.903861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['树模型', '缺失值多：', '核svm', '数据量少特征少：', '决策树算法', '逻辑回归', '数据量大特征多：', '数据问题：', '贝叶斯，决策树，核svm，DNN', '非线性：', '逻辑回归，线性svm', '线性：', '线性问题：']\n",
      "['从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。', '从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。']\n",
      "['head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）', 'tailonly：保存最后510个token', 'headonly：保存前510个token（留两个位置给\\\\[CLS]和\\\\[SEP]）']\n",
      "['结合文本id+文本长度，在做文本处理之前，先做不等长的sum_pooled的操作', '截断补齐']\n"
     ]
    }
   ],
   "source": [
    "#根据问题搜索出对应答案\n",
    "answer = neo4j_model.question2answer(question)\n",
    "for i in answer:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T04:47:50.136557Z",
     "start_time": "2021-08-27T04:47:50.108603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['特征', 'DNN', '非线性', '树', '逻辑', '核', '回归', 'svm']\n",
      "['Boosting', '分解', '剪枝']\n",
      "['tokens', 'token']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#根据答案搜索出关键词，注意这里因为已经存在neo4j数据库里所以不用用模型预测\n",
    "feature = neo4j_model.answer2feature(answer)\n",
    "for i in feature:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-27T04:47:50.370296Z",
     "start_time": "2021-08-27T04:47:50.310410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 长文本预测如何构造Tokens？\n",
      "['head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）', 'tailonly：保存最后510个token', 'headonly：保存前510个token（留两个位置给\\\\[CLS]和\\\\[SEP]）']\n",
      "['tokens', 'token'] \n",
      "\n",
      "1 各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归\n",
      "['树模型', '缺失值多：', '核svm', '数据量少特征少：', '决策树算法', '逻辑回归', '数据量大特征多：', '数据问题：', '贝叶斯，决策树，核svm，DNN', '非线性：', '逻辑回归，线性svm', '线性：', '线性问题：']\n",
      "['特征', 'DNN', '非线性', '树', '逻辑', '核', '回归', 'svm'] \n",
      "\n",
      "2 遇到过的机器学习中的偏差与方差问题？\n",
      "['从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。', '从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。']\n",
      "['Boosting', '分解', '剪枝'] \n",
      "\n",
      "3 残差网络残差作用\n",
      "['H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化', 'X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2', 'X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1', '对输出的变化更敏感', '恒等映射使得网络突破层数限制，避免网络退化', '防止梯度消失']\n",
      "['输出', '梯度'] \n",
      "\n",
      "4 lr加l1还是l2好？\n",
      "['刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2', '这个问题还可以换一个说法，l1和l2的各自作用。']\n",
      "['拉普拉斯', '参数', 'l1', '特征', 'l2'] \n",
      "\n",
      "5 你觉得bn过程是什么样的？\n",
      "['变换系数需要学习', '对标准化的数据进行线性变换', '对整体数据进行标准化', '按batch进行期望和标准差计算']\n",
      "['batch', '标准化'] \n",
      "\n",
      "6 负采样流程？\n",
      "['负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布', '每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）', '统计每个词出现对概率，丢弃词频过低对词']\n",
      "['输出', '统计', 'softmax', '词频'] \n",
      "\n",
      "7 RF的参数有哪些，如何调参？\n",
      "['max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树', 'class_weight也可以调整正负样本的权重', '其他参数中', '分类：max_features=sqrt(n_features)', '回归：max_features=n_features', 'max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多', 'n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好', '要调整的参数主要是n_estimators和max_features']\n",
      "['树', '分类', '回归', '参数', '临界值', '修剪', '特征'] \n",
      "\n",
      "8 你有用过sklearn中的lr么？你用的是哪个包？\n",
      "['sklearn.linear_model.LogisticRegression']\n",
      "['LogisticRegression', 'sklearn'] \n",
      "\n",
      "9 最小二乘回归树的切分过程是怎么样的？\n",
      "['递归重复以上步骤，直到满足叶子结点上值的要求', '属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值', '分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性', '回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值']\n",
      "['递归', '分枝', '误差', '回归'] \n",
      "\n",
      "['监督学习', '预测', '训练', '优化', '回归', '稳定性', '误差', '准确性'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "q,a,f,f_l = neo4j_predict(['机器学习','长文本'])\n",
    "for i in range(len(q)):\n",
    "    print(i,q[i])\n",
    "    print(a[i])\n",
    "    print(f[i],'\\n')\n",
    "print(f_l,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
