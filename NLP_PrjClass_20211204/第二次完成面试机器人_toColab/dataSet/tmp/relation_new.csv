实体1,实体2,关系
基础概念,AutoML,主科目到一级科目
AutoML,AutoML问题构成?,知识点->提问
AutoML问题构成?,"- 特征选择
- 模型选择
- 算法选择
",提问->答案
AutoML,特征工程选择思路？,知识点->提问
特征工程选择思路？,"- 有监督的特征选择
    - 基于模型，lr的系数，树模型的importance等等
    - 基于选择，前项后项选择
- 无监督的特征选择
    - 基于统计信息的，熵、相关性、KL系数
    - 基于方差，因子分解，PCA主成分分享，方差系数
",提问->答案
AutoML,模型相关的选择思路?,知识点->提问
模型相关的选择思路?,"- 模型选择
    - 各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择
- 参数选择
    - grid_search
    - random_search
    - ...
",提问->答案
AutoML,常见优化算法思路？,知识点->提问
常见优化算法思路？,"- SGD
- GD
- L-BFGS
- FTRL
",提问->答案
AutoML,AutoML参数选择所使用的方法？,知识点->提问
AutoML参数选择所使用的方法？,"- 暴力搜索
    - grid_search
    - random_search
- 拟合搜索
    - 贝叶斯优化
- 其他方法
    - Meta学习
    - 转移学习
",提问->答案
AutoML,讲讲贝叶斯优化如何在automl上应用？,知识点->提问
讲讲贝叶斯优化如何在automl上应用？,"- 目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数
- 步骤：
    - 随机选取几个超参数进行f拟合，得到先验数据集合D
    - 根据先验数据D得到模型M
    - 根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D
    - 循环2-3两步直至达到条件
- 问题：
    - 稳定性：同一组超参数的预测结果在不同轮次不一致
    - f函数需要多次计算，资源耗费时间损失
    - 难以确定比较通用的拟合模型f
- 手记：
    - ![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9n45r6f3nj30q40tkjz8.jpg)
",提问->答案
AutoML,以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,知识点->提问
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,"- 随机生成若干超参点，更新gp模型
- 根据gp模型选取最优推荐值
- 推荐值附近随机生成点，根据cquisition function选取附近点极值点，acquisition function通常：
    - 基于GP-UCB的最大置信上界
        - 基于均值和方差的平衡结果
    - Thompson Sampling
    - EI(期望提升)
- 重复以上步骤
",提问->答案
基础概念,先验概率和后验概率,主科目到一级科目
先验概率和后验概率,写出全概率公式&贝叶斯公式,知识点->提问
写出全概率公式&贝叶斯公式,"全概率公式：设事件![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wed60nzaj305i01cmx2.jpg)构成一个完备事件组，即它们两两不相容，和为全集且![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wedhjqtej304w01cjra.jpg) ，则对任一事件A有：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8weetxaqxj30dk01e74b.jpg)
贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg)
",提问->答案
先验概率和后验概率,说说你怎么理解为什么有全概率公式&贝叶斯公式,知识点->提问
说说你怎么理解为什么有全概率公式&贝叶斯公式,"全概率公式为全概率就是表示达到某个目的，有多种方式，算到达目的的概率。**key：算概率**
贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**
",提问->答案
先验概率和后验概率,什么是先验概率,知识点->提问
什么是先验概率,"先验概率（prior probability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计
",提问->答案
先验概率和后验概率,什么是后验概率,知识点->提问
什么是后验概率,"后验概率（posterior probability）：指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。key：条件概率
",提问->答案
先验概率和后验概率,经典概率题,知识点->提问
经典概率题,"有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？
P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。
P[i]= (i/M) * P[i] + (1-i/M)* P[i+1] + 1
解释一下，每一次抽取，(i/M)概率不变，(1-i/M)进入下一轮，额外加一次本次操作",提问->答案
基础概念,方差与偏差,主科目到一级科目
方差与偏差,解释方差：,知识点->提问
解释方差：,"期望值与真实值之间的波动程度，衡量的是**稳定性**
",提问->答案
方差与偏差,解释偏差：,知识点->提问
解释偏差：,"期望值与真实值之间的一致差距，衡量的是**准确性**
",提问->答案
方差与偏差,模型训练为什么要引入偏差和方差？请理论论证。,知识点->提问
模型训练为什么要引入偏差和方差？请理论论证。,"优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和
**Err = bias + var + irreducible error** 
以回归任务为例,其实更准确的公式为：**Err = bias^2 + var + irreducible error^2** 
符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分
- 训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。
- 方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg)
- 偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg)
- Err(x) = Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))+Err(f,Y)
    - Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差
    - Err(f,Y) 为噪声e部分，即为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxc64l5yj300g00b0pn.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxyj2g67j301k00mmwx.jpg) 可推导如下：
        ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxkvx39gj30gh01b3yj.jpg)
    - f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f-![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)中![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxm8gzmrj301z00mq2p.jpg)为常数。所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxo2v9ozj30bw00m0sn.jpg) = 0
    - Err(x) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxvudtfgj30cm00mjra.jpg)
",提问->答案
方差与偏差,什么情况下引发高方差？,知识点->提问
什么情况下引发高方差？,"- 过高复杂度的模型，对训练集进行过拟合
    - 带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差
    - 更加形象的理解就是用一条高次方程去拟合线性数据
",提问->答案
方差与偏差,如何解决高方差问题？,知识点->提问
如何解决高方差问题？,"- 在模型复杂程度不变的情况下，增加更多数据
- 在数据量不变的情况下，减少特征维度
- 在数据和模型都不变的情况下，加入正则化
",提问->答案
方差与偏差,以上方法是否一定有效？,知识点->提问
以上方法是否一定有效？,"- 增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差
    - smote对样本进行扩充是否必定可以避免高方差？
    - 过采样是否解决高方差问题？
- 减少的特征维度如果是共线性的维度，对原模型没有任何影响
    - 罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？
- 正则化通常都是有效的
",提问->答案
方差与偏差,如何解决高偏差问题？,知识点->提问
如何解决高偏差问题？,"- 尝试获得更多的特征
    - 从数据入手，进行特征交叉，或者特征的embedding化
- 尝试增加多项式特征
    - 从模型入手，增加更多线性及非线性变化，提高模型的复杂度
- 尝试减少正则化程度λ
",提问->答案
方差与偏差,以上方法是否一定有效？,知识点->提问
以上方法是否一定有效？,"- 特征越稀疏，高方差的风险越高
- 多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换
- 正则化通常都是有效的
",提问->答案
方差与偏差,遇到过的机器学习中的偏差与方差问题？,知识点->提问
遇到过的机器学习中的偏差与方差问题？,"- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。
",提问->答案
方差与偏差,就理论角度论证Bagging、Boosting的方差偏差问题,知识点->提问
就理论角度论证Bagging、Boosting的方差偏差问题,"- 基础：
    - bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等
    - Var(x,y) = Var(x) + Var(y) + 2Cov(x,y)
- Bagging
    - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg)
            = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyp35ynkj308l00qglh.jpg)
        - 其中
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lypqw1tjj300d00c0q9.jpg)可以直接提取出来
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyuaep07j308v00ya9x.jpg)
        - 所以，化简以上的式子可得：Var(F) = m * ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg) * ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg) + ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg)
        - 以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可
        - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg)
        - E(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz97enuuj3059011743.jpg)
    - 结论：
        - 整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似
        - 整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高
        - bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的
- Boosting 同理
    - boosting的前提是弱模型之间高度相关，我们不妨设相关度为1
    - Var(F) = ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzj06hv1j304300qwea.jpg)
    - 结论：
        - 整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值
        - 整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。
        - Gradient Boosting Decision Tree为典型例子
",提问->答案
方差与偏差,遇到过的深度学习中的偏差与方差问题？,知识点->提问
遇到过的深度学习中的偏差与方差问题？,"神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。
- dropout
- dense中的normalization
- 数据的shuffle
",提问->答案
方差与偏差,方差、偏差与模型的复杂度之间的关系？,知识点->提问
方差、偏差与模型的复杂度之间的关系？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ly6pdyouj30dn07eq38.jpg),提问->答案
基础概念,生成与判别模型,主科目到一级科目
生成与判别模型,什么叫生成模型？,知识点->提问
什么叫生成模型？,"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x) = p(x,y)/p(x)。
说白了就是玩的一手x在不同类别下出现的概率+不同类别的概率+x出现的概率进行复合计算，复合的时候考虑独立性的问题。
",提问->答案
生成与判别模型,什么叫判别模型？,知识点->提问
什么叫判别模型？,"求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y
",提问->答案
生成与判别模型,什么时候会选择生成/判别模型？,知识点->提问
什么时候会选择生成/判别模型？,"明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。
所以这个问题更想问的是什么时候要去用生成模型：
- 但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响
- 如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性
    - 一般会追问，如何构造？
        - FM/FFM
        - Neural Network
            - 线性Dense
            - 非线性激活
",提问->答案
生成与判别模型,CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,知识点->提问
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,"因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。
- 生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM
    - 仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率
- 判别式模型：最大熵模型，CRF
",提问->答案
生成与判别模型,我的理解：,知识点->提问
我的理解：,"- 无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类
- 算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型
    - 生成模型得分布，判别模型得最优划分
- 生成模型可以得到判别模型，反之不成立
- 生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断",提问->答案
基础概念,频率概率,主科目到一级科目
频率概率,极大似然估计 - MLE,知识点->提问
极大似然估计 - MLE,"原理：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值
",提问->答案
频率概率,最大后验估计 - MAP,知识点->提问
最大后验估计 - MAP,"MAP的基础使贝叶斯公式：P(θ/X) = P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可
",提问->答案
频率概率,极大似然估计与最大后验概率的区别？,知识点->提问
极大似然估计与最大后验概率的区别？,"- 最大似然估计中的采样满足所有采样都是独立同分布的假设
- 最大后验概率在考虑了p(X/θ)的同时，还考虑了p(θ)
",提问->答案
频率概率,到底什么是似然什么是概率估计？,知识点->提问
到底什么是似然什么是概率估计？,"- 似然：给定了x求θ真实的可能性
- 概率估计：给定了θ，X=x的可能性
",提问->答案
推荐,DeepFM,主科目到一级科目
DeepFM,DNN与DeepFM之间的区别?,知识点->提问
DNN与DeepFM之间的区别?,"DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征
",提问->答案
DeepFM,Wide&Deep与DeepFM之间的区别?,知识点->提问
Wide&Deep与DeepFM之间的区别?,"DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征
",提问->答案
DeepFM,你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,知识点->提问
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,"- 欠拟合：增加deep部分的层数，增加epoch的轮数，增加learning rate，减少正则化力度
- 过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据
",提问->答案
DeepFM,DeepFM怎么优化的？,知识点->提问
DeepFM怎么优化的？,"- embedding向量可以通过FM初始化 
- Deep层可以做优化
    - NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致
- FM层可以变得交叉更多阶
    - XDeepFM
",提问->答案
DeepFM,不定长文本数据如何输入deepFM？,知识点->提问
不定长文本数据如何输入deepFM？,"- 截断补齐
- 结合文本id+文本长度，在做文本处理之前，先做不等长的sum_pooled的操作
",提问->答案
DeepFM,deepfm的embedding初始化有什么值得注意的地方吗？,知识点->提问
deepfm的embedding初始化有什么值得注意的地方吗？,"- 常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n-1],layer\[n])*np.sqrt(1/layer\[n-1])
    - relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n-1],layer\[n])*np.sqrt(2/layer\[n-1])
- 文本项目上也可以用预训练好的特征
",提问->答案
推荐,DIN,主科目到一级科目
DIN,主要使用了什么机制?,知识点->提问
主要使用了什么机制?,"Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。
",提问->答案
DIN,activation unit的作用,知识点->提问
activation unit的作用,"基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。
- ![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15ugjlkmj308901imx1.jpg)
- 通常用户兴趣可以由历史行为(点击/浏览/收藏)等合并得到，及![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15vxtcu8j302w01imwy.jpg)
- activation unit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性
",提问->答案
DIN,DICE怎么设计的,知识点->提问
DICE怎么设计的,"- 先对input数据进行bn，在进行sigmoid归一化到0-1，再进行一个加权平衡alpha*(1-x_p)`*`x+x_p`*`x
    - x_p=tf.sigmoid(tf.layers.batch_normalization(x, center=False, scale=False,training=True))
    - aplha*(1-x_p)*x+x_p*x
",提问->答案
DIN,DICE使用的过程中，有什么需要注意的地方,知识点->提问
DICE使用的过程中，有什么需要注意的地方,"- 在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望
- test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m-1)",提问->答案
推荐,XDeepFM,主科目到一级科目
XDeepFM,选用的原因？,知识点->提问
选用的原因？,"- 类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的
- 向量级别的特征交互而不是元素级交互
    - 经验上，vector-wise的方式构建的特征交叉关系比bit-wise的方式更容易学习
        - 我也不知道具体好在哪，如果有大佬会可以指导一下，感恩
- 之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理
    - 思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定
    - 思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM
",提问->答案
XDeepFM,什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,知识点->提问
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,"- 显示是可以写出feature交互的公式，隐式相反
- 元素级是以feature值交互，向量级是feature向量级点乘处理
- 高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式
",提问->答案
XDeepFM,简单介绍一下XDeepFm的思想？,知识点->提问
简单介绍一下XDeepFm的思想？,"- 借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**
    - 高阶特征交互：DNN
    - 低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg))
        - 这样的网络结构保证来来自X0的1，2，3...N阶的特征组合
- 借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg)
- 流程概述：
    - feature embedding
    - 构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层
    - CIN中：
        - 先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份
        - 设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：
            - 获取上一次的layer out：X0，并进行切分：embedding`*`\[batch,field,1]
            - 进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]
            - 对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]
            - 加偏置项，并进行激活函数处理，完成一轮处理
        - 将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果
        - 实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等
",提问->答案
XDeepFM,和DCN比，有哪些核心的变化？,知识点->提问
和DCN比，有哪些核心的变化？,"- DCN是bit-wise的，而CIN 是vector-wise的
- DCN每层是1～l+1阶特征，而CIN每层只包含 l+1 阶的组合特征
    - ![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg) 和 ![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg)差异的![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9in06u4m6j300j00f0sh.jpg)导致的
",提问->答案
XDeepFM,时间复杂度多少？,知识点->提问
时间复杂度多少？,"假设CIN和DNN每层神经元/向量个数都为 H ，网络深度为 L
- CIN:O(m`*`L`*`H`*`H)
- DNN:O(m`*`D`*`H+L`*`H`*`H)",提问->答案
推荐,YouTubeNet,主科目到一级科目
YouTubeNet,变长数据如何处理的？,知识点->提问
变长数据如何处理的？,"- input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的history length
    - 20次点击过去一周内的行为，曾经尝试扩大历史点击次数到40，60没有很明显的效果提升
    - 点击行为是处理过的，停留时间过短的click不要
    - 点击行为是处理过的，连续多次的重复点击会去重
    - 点击行为是处理过的，session内的点击次数需要在约定范围内
",提问->答案
YouTubeNet,input是怎么构造的,知识点->提问
input是怎么构造的,"- 最近历史20次点击商品id/文章id，如果不足不需要补充
- 最近历史20次点击商品id对应的品牌/文章id对应的类目，如果不足不需要补充
- 最近历史20次点击商品id对应的类别/文章id对应的栏目，如果不足不需要补充
- 最后一次点击商品id/文章id
- 历史上最高频的商品id/文章id
- example age
- user_info:age/gender/地理位置/注册时长
- cross_info:最后一次点击距click时间，最后一次点击商品浏览次数
- phone_info:设备信息，登录状态
",提问->答案
YouTubeNet,最后一次点击实际如何处理的？,知识点->提问
最后一次点击实际如何处理的？,"我们会以日进行切分，每日首次点击的lastclick会以\[unknow]进行替代，隔日的点击不会进行计算
",提问->答案
YouTubeNet,output的是时候train和predict如何处理的,知识点->提问
output的是时候train和predict如何处理的,"- train的时候是进行负采样的
- predict的时候是进行的all_embedding dot
",提问->答案
YouTubeNet,如何进行负采样的？,知识点->提问
如何进行负采样的？,"- 该次点击时间之前所以的item或者article作为候选集
- 负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样
- 均衡采样，不会根据其他样本show time进行加权
    - 为了尽可能多的修正全量样本，尽快达到收敛
    - 为了避免其他推荐产生的交叉影响
",提问->答案
YouTubeNet,item向量在softmax的时候你们怎么选择的？,知识点->提问
item向量在softmax的时候你们怎么选择的？,"是用初始化我们在进行history click embedding过程中使用的初始化的向量，没有在最后层重新构造一个item embedding的结果，实测效果翻倍的要好
",提问->答案
YouTubeNet,Example Age的理解？,知识点->提问
Example Age的理解？,"- 官方：upload_time-click_time
    - 希望更倾向于新上视频
- 民间：click_time-now
    - 希望平衡样本构造时间对当前的影响
",提问->答案
YouTubeNet,什么叫做不对称的共同浏览（asymmetric co-watch）问题？,知识点->提问
什么叫做不对称的共同浏览（asymmetric co-watch）问题？,"item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测next one这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的
",提问->答案
YouTubeNet,为什么不采取类似RNN的Sequence model？,知识点->提问
为什么不采取类似RNN的Sequence model？,"在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性
",提问->答案
YouTubeNet,YouTube如何避免百万量级的softmax问题的？,知识点->提问
YouTube如何避免百万量级的softmax问题的？,"负采样
",提问->答案
YouTubeNet,serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,知识点->提问
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,"工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度
",提问->答案
YouTubeNet,Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,知识点->提问
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,"example age
",提问->答案
YouTubeNet,在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,知识点->提问
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,"不对称的共同浏览问题，避免引入future information，产生与事实不符的数据穿越
",提问->答案
YouTubeNet,整个过程中有什么亮点？有哪些决定性的提升？,知识点->提问
整个过程中有什么亮点？有哪些决定性的提升？,"- embedding
    - 引入了doc2vec做init
    - 权重共享，没有在softmax处重新构造
- 负采样
    - 限定采样集合在click时间发生之前已经有的item
    - 剔除该次点击click时同时展现的其他item
    - 均衡采样
        - 加快收敛
        - 避免热门商品item的过度影响
- click数据的预处理
    - history click 
        - 停留时间过短的click不要
        - 连续多次的重复点击会去重
        - session内的点击次数需要在约定范围内
    - last click
        - 每日初次点击的lastclick以\[unknown]替代，不做隔日的数据连接
- 在history引入multihead-attention
    - ctr提高了，但是有效点击没有变",提问->答案
数学,gcd,主科目到一级科目
gcd,辗转相除法,知识点->提问
辗转相除法,"```python
def solve(a,b):
    return a if b==0 else solve(b,a%b)
```
",提问->答案
gcd,其他方法,知识点->提问
其他方法,"- 穷举法
- 辗转相减法",提问->答案
数学,导数,主科目到一级科目
导数,四则运算,知识点->提问
四则运算,"- (u+v)'=u'+v'
- (u-v)'=u'-v'
- (uv)'=u'v+uv'
- (u/v)'=(u'v-uv')/v^2
",提问->答案
导数,常见导数,知识点->提问
常见导数,"- y=c(常数),y'=0
- y=pow(x,a),y'=a·pow(x,a-1)
- y=pow(a,x),y'=pow(a,x)·ln(a)
- y=log(a,x),y'=1/(xlna);特别的ln(x)=1/x
- y=sin(x),y'=cos(x)
- y=cos(x),y'=-sin(x)
- y=tan(x),y'=1/(cos(x)^2)
",提问->答案
导数,复合函数的运算法则,知识点->提问
复合函数的运算法则,"若y=f(g(x)),y'=f'(g(x))·g'(x),前提是g在x处可导，f在g(x)处可导
",提问->答案
导数,莱布尼兹公式,知识点->提问
莱布尼兹公式,"若u(x),v(x)均n阶可导，则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xl2xwg4ij307000udfv.jpg)",提问->答案
数学,平面曲线的切线和法线,主科目到一级科目
平面曲线的切线和法线,切线方程,知识点->提问
切线方程,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xko8hysoj306c0123yi.jpg)
",提问->答案
平面曲线的切线和法线,法线方程,知识点->提问
法线方程,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xkomfj6lj309c01amx9.jpg),提问->答案
数学,微分中值定理,主科目到一级科目
微分中值定理,费马定理,知识点->提问
费马定理,"- f在x0的邻域内有定义，且恒满足：f(x)<=f(x0)｜f(x)>=f(x0)
- f在x0处可导，则满足f'(x0)=0
",提问->答案
微分中值定理,拉格朗日中值定理,知识点->提问
拉格朗日中值定理,"设函数f(x)满足条件：
- \[a,b]上连续
- \(a,b)内可导，则\(a,b)存在ζ，使得f(b)-f(a)=f'(ζ)(b-a)
",提问->答案
微分中值定理,柯西中值定理,知识点->提问
柯西中值定理,"设函数f(x),g(x)满足条件：
- \[a,b]上连续
- \(a,b)内可导，且f'(x)和g'(x)存在，且g'(x)!=0
- 则\(a,b)存在ζ，使得(f(b)-f(a))g'(ζ)=f'(ζ)(g(b)-g(a))
",提问->答案
数学,期望、方差、标准差和协方差,主科目到一级科目
期望、方差、标准差和协方差,期望,知识点->提问
期望,"离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为 E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。
- E(ax+by+c) = aE(x)+bE(y)+c
- 如果x和y独立，E(xy)=E(x)E(y) 
",提问->答案
期望、方差、标准差和协方差,方差,知识点->提问
方差,"方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。
方差刻画了随机变量的取值对于其数学期望的离散程度。
方差深入：
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n-1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zkdv6ukj308700kweb.jpg)
- 如果x和y独立，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zl5np3yj308600kt8j.jpg)
",提问->答案
期望、方差、标准差和协方差,标准差,知识点->提问
标准差,"标准差（Standard Deviation） ，也称均方差（mean square error），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。
",提问->答案
期望、方差、标准差和协方差,协方差,知识点->提问
协方差,"协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。 方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。 回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。
在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。
- Cov(x,y) = E((x-E(x))(y-E(y)))
- Cov(c+ax,d+by) = abCov(x,y)
",提问->答案
期望、方差、标准差和协方差,相关系数,知识点->提问
相关系数,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zq1gbe7j306601ct8k.jpg)
\[-1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性
",提问->答案
数学,概率密度分布,主科目到一级科目
概率密度分布,均匀分布,知识点->提问
均匀分布,"- 离散随机变量的均匀分布：假设 X 有 k 个取值：x1, x2, ..., xk 则均匀分布的概率密度函数为:
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91ysz3sxsj30aw023glh.jpg)
",提问->答案
概率密度分布,伯努利分布,知识点->提问
伯努利分布,"伯努利分布：参数为 p∈[0,1]，设随机变量 X ∈ {0,1}，则概率分布函数为：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91yyoulv0j306m00lmwz.jpg)
期望为p，方差为p(1-p)
",提问->答案
概率密度分布,二项分布,知识点->提问
二项分布,"独立重复地进行 n 次试验中，成功 x 次的概率:
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z2sy9m5j308800ja9w.jpg)
期望为np，方差为np(1-p)
",提问->答案
概率密度分布,高斯分布,知识点->提问
高斯分布,"我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：
- 随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布
- 在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多
典型的一维正态分布的概率密度函数为 :
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z5lcnmcj30dk02fa9z.jpg)
",提问->答案
概率密度分布,拉普拉斯分布,知识点->提问
拉普拉斯分布,"概率密度函数：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z66211aj309c01hjr9.jpg)
期望为u，方差为2γ^2
拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质
",提问->答案
概率密度分布,泊松分布,知识点->提问
泊松分布,"假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为 k 的概率。
概率密度函数：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z8oplp1j306f01g0sk.jpg)
期望：λ，方差为：λ",提问->答案
数学,概率论,主科目到一级科目
概率论,条件概率,知识点->提问
条件概率,"P(A/B) = P(AB)/P(B) 
",提问->答案
概率论,独立,知识点->提问
独立,"P(AB) = P(A)P(B)
",提问->答案
概率论,概率基础公式,知识点->提问
概率基础公式,"加法：P(A+B) = P(A)+P(B)-P(AB)
减法：P(A-B) = P(A) - P(AB)
乘法：P(AB) = P(A)P(B/A)
",提问->答案
概率论,全概率：,知识点->提问
全概率：,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920h3l62yj305u00qweb.jpg)
",提问->答案
概率论,贝叶斯,知识点->提问
贝叶斯,"P(B/A)=P(B)*P(A/B)/P(A)
",提问->答案
概率论,切比雪夫不等式,知识点->提问
切比雪夫不等式,"p(|x-u|>k∂)<=1/(k^2),满足k>0,u为期望,∂为标准差
绝大多数数据都应该在均值附近
",提问->答案
概率论,抽球,知识点->提问
抽球,"- 有放回的抽取，抽取 m 个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg)
- 无放回的抽取，抽取 m 个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg)
",提问->答案
概率论,纸牌问题,知识点->提问
纸牌问题,"问题：54 张牌，分成 6 份， 每份 9 张牌， 大小王在一起的概率？
54张牌分成6等份，共有M=(C54取9)*(C45取9)*...种分法。
其中大小王在同一份的分法有N=(C6取1)*(C52取7)*(C45取9)*...种。
因此所求概率为P=N / M
",提问->答案
概率论,棍子/绳子问题,知识点->提问
棍子/绳子问题,"问题：一根棍子折三段能组成三角形的概率？
假设：棍子长度为1，第一段长度为x， 第二段长度为y， 第三段长度1-x-y 
分母：总样本空间为： 1 * 1 = 1
分子：两边之和大于第三边，得1/8
",提问->答案
概率论,贝叶斯,知识点->提问
贝叶斯,"问题：某城市发生一起汽车撞人逃跑事件，该城市只有两种颜色的车，蓝20%绿80%， 事发时现场只有一个目击者，他指正是蓝车，但根据专家分析，当时那种条件下能看正确的可能性是80%，那么肇事的车是蓝车的概率是多少？ 
假设事件 A 为目击者指正蓝车， 事件B为肇事车为蓝车，事件C为肇事车为绿车，那么有：
0.2`*`0.8/(0.2`*`0.8+0.8`*`0.2)=0.5
",提问->答案
概率论,选择时间问题,知识点->提问
选择时间问题,"问题：一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略及其概率?
1/e
",提问->答案
概率论,0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,知识点->提问
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,"均匀分布：
E(x) = (a+b)/2
标准差：D(x) = (b-a)^2/12
所以只需要对x做变换：sqrt(12(x-1/2))即可
",提问->答案
概率论,抽红蓝球球,知识点->提问
抽红蓝球球,"问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数 
假设设抽到 蓝球 的概率为 p ， 设抽到红球的概率为 q， 那么抽取到的次数为：1·p+2p·q+...+np·q^(n-1)
可得E = p\[1+2q+...+nq^(n-1)],令1+2q+...+nq^(n-1)=s，再由s为等比公式和s-sq得，E=1/p",提问->答案
数学,欧拉公式,主科目到一级科目
数学,泰勒公式,主科目到一级科目
泰勒公式,泰勒公式,知识点->提问
泰勒公式,"定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得
f(x)=f(x0)+f'(x0)(x-x0)+1/2!f''(x0)(x-x0)^2+...+Rn(x)
其中，Rn(x) = f<n+1>(ζ)/(n+1)!(x-x0)^(n+1)，为泰勒余项
",提问->答案
泰勒公式,常见泰勒公式,知识点->提问
常见泰勒公式,![](https://i.bmp.ovh/imgs/2019/11/0a10d9591cc0c4ac.png),提问->答案
数学,牛顿法,主科目到一级科目
牛顿法,迭代公式推导,知识点->提问
迭代公式推导,"- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfple4x4j303x00i742.jpg)
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfr2t8flj306l00mmwz.jpg)
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfruh1klj3043015wea.jpg)
    - 这边是2次，所以直接以y=x^2化简了
",提问->答案
牛顿法,实现它,知识点->提问
实现它,"```
def get_ans(nums,count=10000):
    ans = nums
    if not ans:
        return ans
    Times = 0
    while Times<count:
        ans = 0.5*(ans+nums/ans)
        Times+=1
    return ans
```",提问->答案
数学,矩阵,主科目到一级科目
矩阵,范数,知识点->提问
范数,"1范数：各列绝对值和的最大值
2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵 $A^TA$ 的最大特征值开平方根
",提问->答案
矩阵,特征值分解，特征向量,知识点->提问
特征值分解，特征向量,"特征值分解可以得到特征值与特征向量 
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么
矩阵A 的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg), 特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：
也可写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9201wcuefj303600qa9u.jpg)
其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵
",提问->答案
矩阵,正定性,知识点->提问
正定性,"- 如何判断矩阵的正定性？
    - 矩阵的特征值大于等于0，半正定
    - 矩阵的特征值大于0，正定
- 正定性的用途？
    - Hessian矩阵正定性在梯度下降的应用
        - 若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解
    - 在svm中核函数构造的基本假设",提问->答案
数据预处理,异常点识别,主科目到一级科目
异常点识别,统计方法,知识点->提问
统计方法,"- 3∂原则
    - 数据需要服从正态分布
    - 只能解决一维问题：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m954lh5tj301h00gdfl.jpg)
- 基于正态分布的离群点检测方法
    - 一元高斯分布校验：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m91basuwj306201h0sl.jpg)，如果概率值大小离群则代表为异常点
    - 多元高斯分布检测：
        - 假设 n 维的数据集合 ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算 n 维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg)
        - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fdr3xfj30140080sl.jpg)的协方差矩阵：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fly7rpj306g00ja9z.jpg)
        - 得到![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9gzmd1cj30bz01874b.jpg) 
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m92kshc2j30d6073dft.jpg)
- 马氏距离
    - 假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kguzdgj307e00lmx3.jpg)
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ryphxxj304u00kmx2.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第 i 维的均值，n是维度
- 箱型图
    - IQR，\[Q1-3/2(Q3-Q1),Q3+3/2(Q3-Q1)]
",提问->答案
异常点识别,矩阵分解方法,知识点->提问
矩阵分解方法,"- PCA
    - 去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间
    - 核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力
    - 问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用
- SVD
    - 假设 dataMat 是一个 p 维的数据集合，有 N 个样本，它的协方差矩阵是 X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg)
    - 其中 P 是一个 (p,p) 维的正交矩阵，它的每一列都是 X 的特征向量。D 是一个 (p,p) 维的对角矩阵，包含了特征值 ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mar6eb43j304300h746.jpg)可以认为是dataMat在主成分topj上的映射
    - 最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg)
    - 异常值分数（outlier score）：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8maynk5wkj30a300mjrc.jpg) + ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mayu64x7j305500mt8m.jpg)
",提问->答案
异常点识别,特征值和特征向量的本质是什么？,知识点->提问
特征值和特征向量的本质是什么？,"- 一个特征向量可以看成 2 维平面上面的一条线，或者高维空间里面的一个超平面
- 特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度 
",提问->答案
异常点识别,矩阵乘法的实际意义？,知识点->提问
矩阵乘法的实际意义？,"- 两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
- 矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
",提问->答案
异常点识别,密度的离群点检测,知识点->提问
密度的离群点检测,"- 定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。
    - 我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。
- Local Outlier Factor算法
- 孤立森林:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mcoswixrj30iy0em0tb.jpg)
    - 经验1：每棵树的最大深度limit length=ceiling(log2(样本大小))
    - 经验2：树的个数在256棵以下
缺点：
    - 计算量大：o(n^2)
    - 需要人为选择阈值
    
",提问->答案
异常点识别,聚类的离群点检测,知识点->提问
聚类的离群点检测,"- 一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。
- 缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择
",提问->答案
异常点识别,如何处理异常点？,知识点->提问
如何处理异常点？,"- 删除含有异常值的记录：直接将含有异常值的记录删除；
- 视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；
- 平均值修正：可用前后两个观测值的平均值修正该异常值；
- 生成列新特征：category异常
- 不处理：直接在具有异常值的数据集上进行数据挖掘；",提问->答案
数据预处理,数据平衡,主科目到一级科目
数据平衡,为什么要对数据进行采样平衡,知识点->提问
为什么要对数据进行采样平衡,"- 下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果
- 上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练
    - 比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况
",提问->答案
数据平衡,是否一定需要对原始数据进行采样平衡,知识点->提问
是否一定需要对原始数据进行采样平衡,"否。
- 采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降
- 采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合
",提问->答案
数据平衡,有哪些常见的采样方法？,知识点->提问
有哪些常见的采样方法？,"- 随机采样
    - 无放回的简单抽样：每条样本被采到的概率相等且都为1/N
    - 有放回的简单抽样：每条样本可能多次被选中
    - 上采样：即合理地增加少数类的样本
    - 下采样：欠抽样技术是将数据从原始数据集中移除
    - 平衡采样：考虑正负样本比
    - 分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样
    - 整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集
- 合成采样
相对于采样随机的方法进行上采样, 还有两种比较流行的上采样的改进方式： 
    - SMOTE
        - x_new = x + rand(0,1) * (x′−x)
        - **带来新样本的同时有可能造成不同类别样本之间的重合**
        - Borderline-SMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点
    - ADASYN
        - 同上，也是在构造样本点的过程中考虑了正负样本比
- 平衡欠采样        
    - EasyEnsemble，利用模型融合的方法（Ensemble）
        - 少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合
    - BalanceCascade，利用模型融合的方法（Boost）
        - 每次剔除预测正确的多数样本，加入新的未预测的多数样本
    - NearMiss
        - 选择离各种情况下的少数样本位置最远的多数样本进行训练                
",提问->答案
数据平衡,能否避免采样？,知识点->提问
能否避免采样？,"可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。
",提问->答案
数据平衡,你平时怎么用采样方法？,知识点->提问
你平时怎么用采样方法？,"尽量避免使用合成采样的方式去做数据填充，总结如下：
- 由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器
- 通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀
- 合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合",提问->答案
数据预处理,特征提取,主科目到一级科目
特征提取,为什么需要对数据进行变换？,知识点->提问
为什么需要对数据进行变换？,"- 避免异常点：比如对连续变量进行份桶离散化
- 可解释性或者需要连续输出：比如评分卡模型中的iv+woe
- 使得原始数据的信息量更大：比如log/sqrt变换
",提问->答案
特征提取,归一化和标准化之间的关系？,知识点->提问
归一化和标准化之间的关系？,"- 归一化(max-min)
    - 缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程
- 标准化(z-score)
    - 缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中
- 作用
    - 解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型
    - 更快的收敛
    - 去量纲化
    - 避免数值计算溢出
- 总结
    - 异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降
    - 分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下
    - 上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动
    - 值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定
    - 模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换
- 常用模型    
    - knn：计算距离，不去量冈则结果受值域范围影响大
    - neural network：梯度异常问题+激活函数问题
        
",提问->答案
特征提取,连续特征常用方法,知识点->提问
连续特征常用方法,"- 截断
    - 连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)
    - 参考异常点里面的outlier识别，以最大值填充或者以None
- 二值化
    - 数据分布过于不平衡
    - 空值/异常值过多
- 分桶
    - 小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg)
- 离散化    
    - 数值无意义，比如学历、祖籍等等
- 缩放
    - z-score标准化
    - min-max归一化
    - 范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg)
        - L1范数
        - L2范数
    - 平方根缩放
    - 对数缩放
        - 对数缩放适用于处理长尾分且取值为正数的数值变量
            - 它将大端长尾压缩为短尾，并将小端进行延伸
        - 可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)-log(y)
        - 可以把有偏分布修正为近似正太分布
    - Box-Cox转换
        - ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg)
        - 通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，Box-Cox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件
- 特征交叉
    - 人为分段交叉
        - 提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释
        - 离散变量的交并补
        - 连续变量的点积，attention类似
        - 交叉中需要并行特征筛选的步骤
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg)
    - 自动组合
        - FM/FFM中的矩阵点积
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg)
        - Neural Network里面的dense
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg)
    - 条件选择
        - 通过树或者类似的特征组合模型去做最低熵的特征选择
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg)
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg)
- 非线性编码
    - 核向量进行升维
    - 树模型的叶子结点的stack
    - 谱聚类/pca/svd等信息抽取编码
    - lda/EM等分布拟合表示
",提问->答案
特征提取,离散特征常用方法,知识点->提问
离散特征常用方法,"- one-hot-encoder
- 分层编码
    - 有一定规律的类别数据，邮政编码，手机号等等
- 计数编码
    - 将类别特征用其对应的计数来代替,这对线性和非线性模型都有效
    - 对异常值比较敏感,特征取值有可能冲突
- 计数排名编码
    - 解决上述问题，以排名代替值
- Embedding
    - 对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题
- 类别特征之间交叉组合
    - 笛卡尔交叉
- 类别特征和数值特征之间交叉组合
    - 均值、中位数、标准差、最大值和最小值
    - 分位数、方差、vif值、分段冲量
",提问->答案
特征提取,文本特征,知识点->提问
文本特征,"- 预处理手段有哪些？
    - 将字符转化为小写
    - 分词
    - 去除无用字符
    - 繁体转中文
    - 去除停用词
    - 去除稀有词
    - 半角全角切换
    - 错词纠正
    - 关键词标记
        - TF-IDF
        - LDA
        - LSA
    - 提取词根
    - 词干提取
    - 标点符号编码
    - 文档特征
    - 实体插入和提取
    - 文本向量化
        - word2vec
        - glove
        - bert
    - 文本相似性
- 如何做样本构造？
    - 按标点切分
    - 按句切分
    - 对话session切分
    - 按文章切分
    - 按场景切分
- 分词过程中会考虑哪些方面？
    - 词性标注
    - 词形还原和词干提取
        - 词形还原为了通用性特征的提取
        - 词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义
- 文本中的统计信息一般有哪些？
    - 直接统计值：
        - 文本的长度
        - 单词个数
        - 数字个数
        - 字母个数
        - 大小写单词个数
        - 大小写字母个数
        - 标点符号个数
        - 特殊字符个数
        - 数字占比
        - 字母占比
        - 特殊字符占比
        - 不同词性个数    
    - 直接统计值的统计信息：
        - 最小最大均值方差标准差
        - 分位数，最早/最晚出现位置
- 直接对文本特征进行整理手段有哪些？   
    - N-Gram模型
        - 将文本转换为连续序列，扩充样本特征
        - 连续语意的提取
    - TF-IDF
        - 权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""
    - LDA
        - 主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系
    - 相似度
        - 余弦相似度
        - Jaccard相似度
            - 共现性
        - Levenshtein(编辑距离)
            - 文本近似程度
        - 海林格距离
            - 用来衡量概率分布之间的相似性
        - JSD
            - 衡量prob1 和 prob2两个分布的相似程度
    - 向量化
        - word2vec
        - glove
        - bert
- 文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开
",提问->答案
特征提取,画一个最简单的最快速能实现的框架,知识点->提问
画一个最简单的最快速能实现的框架,"- 建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的
- 建议从简单的开始，然后面试官说还有其他方法么？再做延展：
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg)",提问->答案
数据预处理,特征选择,主科目到一级科目
特征选择,为什么要做特征选择？,知识点->提问
为什么要做特征选择？,"- 耗时：特征个数越多，分析特征、训练模型所需的时间就越长。
- 过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。    
- 共线性：单因子对目标的作用被稀释，解释力下降
",提问->答案
特征选择,从哪些方面可以做特征选择？,知识点->提问
从哪些方面可以做特征选择？,"- 方差，是的feature内的方向更大，对目标区分度提高更高贡献
- 相关性，与区分目标有高相关的特征才有意义
",提问->答案
特征选择,既然说了两个方向，分别介绍一些吧,知识点->提问
既然说了两个方向，分别介绍一些吧,"- 方差
    - 移除低方差特征
        - 移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除
    - 考虑有值数据中的占比，异常数据的占比，正常范围数据过少的数据也可以移除
- 相关性
    - 单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况
        - 皮尔森相关系数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n46pedyoj303e019t8i.jpg)
        - Fisher得分:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n4jajze8j31hc0u0dt2.jpg)
        - 假设检验
            - 卡方检验
            - ANOVA
        - 熵检验
            - 互信息熵
                - 度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立
            - KL散度
            - 相对熵
        ",提问->答案
数据预处理,缺失值处理,主科目到一级科目
缺失值处理,是不是一定需要对缺失值处理？,知识点->提问
是不是一定需要对缺失值处理？,"当缺失值占比在可接受的范围以内的时候才需要进行填充，如果缺失值大于50%以上的时候，可以选择进行二分化，如果缺失值大于80%可以完整删除该列而不是强行去填充
",提问->答案
缺失值处理,直接填充方法有哪些？,知识点->提问
直接填充方法有哪些？,"- 均值
- 中位数
- 众数
- 分位数
",提问->答案
缺失值处理,模型插值方法有哪些？及方法的问题,知识点->提问
模型插值方法有哪些？及方法的问题,"- 有效性存疑，取决于特征列数
    - 生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关
",提问->答案
缺失值处理,如何直接离散化？,知识点->提问
如何直接离散化？,"- 离散特征新增缺失的category
",提问->答案
缺失值处理,hold位填充方法有哪些？,知识点->提问
hold位填充方法有哪些？,"把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果
    - 可以参考YouTube中的新商品向量生成逻辑
    - bert中的\[UNK]向量，\[unused]向量
    
",提问->答案
缺失值处理,怎么理解分布补全？,知识点->提问
怎么理解分布补全？,"如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值
",提问->答案
缺失值处理,random方法,知识点->提问
random方法,"在缺失量特别少(通常认为小于1%)的时候，可以随机生成
",提问->答案
缺失值处理,总结,知识点->提问
总结,"- 实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多
    - 快速
    - 对原始数据的前提假设最少，也不会影响到非缺失列
- 在深度学习中，hold位填充方法用的最多
    - 在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛
    - 而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性",提问->答案
机器学习,决策树,主科目到一级科目
决策树,常见决策树,知识点->提问
常见决策树,"| 模型      | ID3      |     C4.5 |   CART   |
| :--------| :-------- | --------:| :------: |
| 结构   | 多叉树    |   多叉树 |  二叉树  |
| 特征选择   | 信息增益    |   信息增益率 |  Gini系数/均方差  |
| 连续值处理   | 不支持    |   支持 |  支持  |
| 缺失值处理   | 不支持    |   支持 |  支持  |
| 枝剪   | 不支持    |   支持 |  支持  |
",提问->答案
决策树,简述决策树构建过程,知识点->提问
简述决策树构建过程,"1. 构建根节点，将所有训练数据都放在根节点
2. 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类
3. 如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止
",提问->答案
决策树,详述信息熵计算方法及存在问题,知识点->提问
详述信息熵计算方法及存在问题,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg)
其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类)
",提问->答案
决策树,详述信息增益计算方法,知识点->提问
详述信息增益计算方法,"条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg)
信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：
I(D,A) = H(D)-H(D/A)
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设
",提问->答案
决策树,详述信息增益率计算方法,知识点->提问
详述信息增益率计算方法,"在信息增益计算的基础不变的情况下得到的：I(D,A) = H(D)-H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。
信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg)
",提问->答案
决策树,解释Gini系数,知识点->提问
解释Gini系数,"Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg)
对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg)
对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg)
",提问->答案
决策树,ID3存在的问题,知识点->提问
ID3存在的问题,"缺点：
- 存在偏向于选择取值较多的特征问题
- 连续值不支持
- 缺失值不支持
- 无法枝剪
",提问->答案
决策树,C4.5相对于ID3的改进点,知识点->提问
C4.5相对于ID3的改进点,"- 主动进行的连续的特征离散化
    - 比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点
    - **连续特征可以再后序特征划分中仍可继续参与计算**
- 缺失问题优化
    - 训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）
    - 预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布
- 采用预枝剪
",提问->答案
决策树,CART的连续特征改进点,知识点->提问
CART的连续特征改进点,"- 分类情况下的变量特征选择
    - 离散变量：二分划分
    - 连续变量：和C4.5一致，如果当前节点为连续属性，则该属性后面依旧可以参与子节点的产生选择过程
- 回归情况下，连续变量不再采取中间值划分，采用最小方差法
",提问->答案
决策树,CART分类树建立算法的具体流程,知识点->提问
CART分类树建立算法的具体流程,"我们的算法从根节点开始，用训练集递归的建立CART树。
- 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归
- 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归
- 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数
- 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2
- 递归1～4
",提问->答案
决策树,CART回归树建立算法的具体流程,知识点->提问
CART回归树建立算法的具体流程,"其他部分都一样，在构建过程中遇到连续值的话，并不是利用C4.5中的中间值基尼系数的方式，而是采取了最小方差方法：
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg)
",提问->答案
决策树,CART输出结果的逻辑？,知识点->提问
CART输出结果的逻辑？,"- 回归树：利用最终叶子的均值或者中位数来作为输出结果
- 分类树：利用最终叶子的大概率的分类类别来作为输出结果
",提问->答案
决策树,CART树算法的剪枝过程是怎么样的？,知识点->提问
CART树算法的剪枝过程是怎么样的？,"目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。
当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。
由枝剪到根结点及不枝剪两种情况可得：𝛼=(𝐶(𝑇)−𝐶(𝑇𝑡))/(|𝑇𝑡|−1) , C(T)为根结点误差
- 计算出每个子树是否剪枝的阈值𝛼
- 选择阈值𝛼集合中的最小值
- 分别针对不同的最小值𝛼所对应的剪枝后的最优子树做交叉验证
",提问->答案
决策树,树形结构为何不需要归一化？,知识点->提问
树形结构为何不需要归一化？,"无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。
",提问->答案
决策树,决策树的优缺点,知识点->提问
决策树的优缺点,"优点：
- 缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散
- 可解释性强
- 算法对数据没有强假设
- 可以解决线性及非线性问题
- 有特征选择等辅助功能
缺点：
- 处理关联性数据比较薄弱
- 正负量级有偏样本的样本效果较差
- 单棵树的拟合效果欠佳，容易过拟合",提问->答案
机器学习,支持向量机,主科目到一级科目
支持向量机,简单介绍SVM?,知识点->提问
简单介绍SVM?,"- 从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)  subject to：y(wx+b)>=1，其中||·||为2范数
- 然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题
- 最后再利用SMO（序列最小优化）来解决这个对偶问题
",提问->答案
支持向量机,什么叫最优超平面？,知识点->提问
什么叫最优超平面？,"- 两类样本分别分割在该超平面的两侧
- 超平面两侧的点离超平面尽可能的远
",提问->答案
支持向量机,什么是支持向量？,知识点->提问
什么是支持向量？,"在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点
",提问->答案
支持向量机,SVM 和全部数据有关还是和局部数据有关?,知识点->提问
SVM 和全部数据有关还是和局部数据有关?,"局部
",提问->答案
支持向量机,加大训练数据量一定能提高SVM准确率吗？,知识点->提问
加大训练数据量一定能提高SVM准确率吗？,"支持向量的添加才会提高，否则无效
",提问->答案
支持向量机,如何解决多分类问题？,知识点->提问
如何解决多分类问题？,"对训练器进行组合。其中比较典型的有一对一，和一对多
",提问->答案
支持向量机,可以做回归吗，怎么做？,知识点->提问
可以做回归吗，怎么做？,"可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg)
",提问->答案
支持向量机,SVM 能解决哪些问题？,知识点->提问
SVM 能解决哪些问题？,"- 线性问题
    - 对于n为数据，找到n-1维的超平面将数据分成2份。通过增加一个约束条件： 要求这个超平面到每边最近数据点的距离是最大的
- 非线性问题
    -  SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器
",提问->答案
支持向量机,介绍一下你知道的不同的SVM分类器？,知识点->提问
介绍一下你知道的不同的SVM分类器？,"- 硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器
- 软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器
- kernel SVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器
",提问->答案
支持向量机,什么叫软间隔？,知识点->提问
什么叫软间隔？,"软间隔允许部分样本点不满足约束条件： 1<y(wx+b)
",提问->答案
支持向量机,SVM 软间隔与硬间隔表达式,知识点->提问
SVM 软间隔与硬间隔表达式,"- 硬间隔： ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93ha4y4glj3094011a9w.jpg)
- 软间隔： ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93hae5o8lj30d701f0sn.jpg)
",提问->答案
支持向量机,SVM原问题和对偶问题的关系/解释原问题和对偶问题？,知识点->提问
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,"- svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg)
- svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)
    - 拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss 
    - 引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件
    - 在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数
",提问->答案
支持向量机,为什么要把原问题转换为对偶问题？,知识点->提问
为什么要把原问题转换为对偶问题？,"- 因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效
- 引入了核函数
",提问->答案
支持向量机,为什么求解对偶问题更加高效？,知识点->提问
为什么求解对偶问题更加高效？,"- 原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题
- 因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0
",提问->答案
支持向量机,alpha系数有多少个？,知识点->提问
alpha系数有多少个？,"样本点的个数
",提问->答案
支持向量机,KKT限制条件，KKT条件有哪些，完整描述,知识点->提问
KKT限制条件，KKT条件有哪些，完整描述,"- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的 KKT(Karush-Kuhn-Tucker) 条件
- KKT乘子λ>=0
",提问->答案
支持向量机,引入拉格朗日的优化方法后的损失函数解释,知识点->提问
引入拉格朗日的优化方法后的损失函数解释,"- 原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg)
- 优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg)
    - 要求KKT乘子λ>=0
",提问->答案
支持向量机,核函数的作用是啥,知识点->提问
核函数的作用是啥,"核函数能够将特征从低维空间映射到高维空间， 这个映射可以把低维空间中不可分的两类点变成高维线性可分的
",提问->答案
支持向量机,核函数的种类和应用场景,知识点->提问
核函数的种类和应用场景,"- 线性核函数：主要用于线性可分的情形。参数少，速度快。
- 多项式核函数：
- 高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。
- sigmoid 核函数：
- 拉普拉斯核函数：
",提问->答案
支持向量机,如何选择核函数,知识点->提问
如何选择核函数,"我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候
",提问->答案
支持向量机,常用核函数的定义？,知识点->提问
常用核函数的定义？,"在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：
1) 线性：K(v1,v2) = <v1,v2>
2) 多项式：K(v1,v2) = (r<v1,v2>+c)^n
3) Radial basis function：K(v1,v2) = exp(-r||v1-v2||^2)
4) Sigmoid：tanh(r<v1,v2>+c)
",提问->答案
支持向量机,核函数需要满足什么条件？,知识点->提问
核函数需要满足什么条件？,"Mercer定理：核函数矩阵是对称半正定的
",提问->答案
支持向量机,为什么在数据量大的情况下常常用lr代替核SVM？,知识点->提问
为什么在数据量大的情况下常常用lr代替核SVM？,"- 计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2)
- 在使用核函数的时候参数假设全靠试，时间成本过高
",提问->答案
支持向量机,高斯核可以升到多少维？为什么,知识点->提问
高斯核可以升到多少维？为什么,"无穷维
e的n次方的泰勒展开得到了一个无穷维度的映射
",提问->答案
支持向量机,SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,知识点->提问
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,"如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整
",提问->答案
支持向量机,"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",知识点->提问
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归","- 线性问题：
    - 线性：
        - 逻辑回归，线性svm
    - 非线性：
        - 贝叶斯，决策树，核svm，DNN
- 数据问题：
    - 数据量大特征多：
        - 逻辑回归
        - 决策树算法
    - 数据量少特征少：
        - 核svm
- 缺失值多：
    - 树模型
",提问->答案
支持向量机,Linear SVM 和 LR 有什么异同？,知识点->提问
Linear SVM 和 LR 有什么异同？,"- LR是参数模型，SVM为非参数模型。
- LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。
- 在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。
- LR的模型相对简单，在进行大规模线性分类时比较方便。",提问->答案
机器学习,线性回归,主科目到一级科目
线性回归,损失函数是啥,知识点->提问
损失函数是啥,"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)
",提问->答案
线性回归,最小二乘/梯度下降手推,知识点->提问
最小二乘/梯度下降手推,"- 最小二乘
    - 损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg)
    - 求导可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93489pnxxj3052014jr7.jpg)
        - 使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg) 
        - 如果X点乘X的转置可逆则有唯一解，否则无法如此求解
- 梯度下降
    - 损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)
    - 求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg)
    
",提问->答案
线性回归,介绍一下岭回归,知识点->提问
介绍一下岭回归,"加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg)
在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg)
",提问->答案
线性回归,什么时候使用岭回归？,知识点->提问
什么时候使用岭回归？,"样本数少，或者样本重复程度高
",提问->答案
线性回归,什么时候用Lasso回归？,知识点->提问
什么时候用Lasso回归？,特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,提问->答案
机器学习,聚类,主科目到一级科目
聚类,请问从EM角度理解kmeans?,知识点->提问
请问从EM角度理解kmeans?,"- k-means是两个步骤交替进行，可以分别看成E步和M步
- M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；
- E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似
",提问->答案
聚类,为什么kmeans一定会收敛?,知识点->提问
为什么kmeans一定会收敛?,"M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛
",提问->答案
聚类,kmeans初始点除了随机选取之外的方法？,知识点->提问
kmeans初始点除了随机选取之外的方法？,先层次聚类，再在不同层次上选取初始点进行kmeans聚类,提问->答案
机器学习,贝叶斯,主科目到一级科目
贝叶斯,解释一下朴素贝叶斯中考虑到的条件独立假设,知识点->提问
解释一下朴素贝叶斯中考虑到的条件独立假设,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92b15s6daj308x00rq2s.jpg)
",提问->答案
贝叶斯,讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,知识点->提问
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,"贝叶斯公式是完整的数学公式P(A/B) = P(A)P(B/A)/P(B)
朴素贝叶斯 = 贝叶斯公式 + 条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可
",提问->答案
贝叶斯,朴素贝叶斯中出现的常见模型有哪些,知识点->提问
朴素贝叶斯中出现的常见模型有哪些,"- 多项式：多项式模型适用于离散特征情况，在文本领域应用广泛， 其基本思想是：我们将重复的词语视为其出现多次
    - 因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作
        - 先验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92bmactdlj303o0133yb.jpg)
        - 后验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92borlh0nj3043018q2r.jpg)
        - 两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数
- 高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1)
    - 高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）
- 伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次
",提问->答案
贝叶斯,出现估计概率值为 0 怎么处理,知识点->提问
出现估计概率值为 0 怎么处理,"拉普拉斯平滑
",提问->答案
贝叶斯,朴素贝叶斯的优缺点？,知识点->提问
朴素贝叶斯的优缺点？,"- 优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练
- 缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）
",提问->答案
贝叶斯,朴素贝叶斯与 LR 区别？,知识点->提问
朴素贝叶斯与 LR 区别？,"- 生成模型和判别模型
- 条件独立要求
- 小数据集和大数据集
",提问->答案
机器学习,逻辑回归,主科目到一级科目
逻辑回归,logistic分布函数和密度函数，手绘大概的图像,知识点->提问
logistic分布函数和密度函数，手绘大概的图像,"- 分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg)
- 密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg)
- 其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**
",提问->答案
逻辑回归,LR推导，基础5连问,知识点->提问
LR推导，基础5连问,"- 基础公式
    - f(x) = wx + b
    - y = sigmoid(f(x))
    - 可以看作是一次线性拟合+一次sigmoid的非线性变化
- 伯努利过程
    - 对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：
        - p(y=1/x,θ) = h(θ,x)
        - p(y=0/x,θ) = 1-h(θ,x)
        - p(y/x,θ) = h(θ,x)^y · (1-h(θ,x))^(1-y) 
            - 第i个样本正确预测的概率如上可得
    - 几率odds
        - 数据特征下属于正例及反例的比值
        - ln(y/(1-y))
- 极大似然
    - 第i个样本正确预测的概率如上可得每条样本的情况下
    - 综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：
        - ∏(h(θ,x)^y · (1-h(θ,x))^(1-y))
- 损失函数
    - 通常会对极大似然取对数，得到损失函数，方便计算
        - ∑ylogh(θ,x)+(1-y)log(1-h(θ,x))最大
        - 及-1/m · ∑ylogh(θ,x)+(1-y)log(1-h(θ,x))最小    
- 梯度下降
    - 损失函数求偏导，更新θ
    - θj+1 = θj - ∆·∂Loss/∂θ =θj - ∆·1/m·∑x·(h-y)
        - ∆为学习率
",提问->答案
逻辑回归,梯度下降如何并行化？,知识点->提问
梯度下降如何并行化？,"- 首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg)
    - ∑处的并行，不同样本在不同机器上进行计算，计算完再进行合并
    - 同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算
",提问->答案
逻辑回归,LR明明是分类模型为什么叫回归？,知识点->提问
LR明明是分类模型为什么叫回归？,"观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归
",提问->答案
逻辑回归,为什么LR可以用来做CTR预估？,知识点->提问
为什么LR可以用来做CTR预估？,"1. 点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match
2. 实现简单，方便并行，计算迭代速度很快
3. 可解释性强，可结合正则化等优化方法
",提问->答案
逻辑回归,满足什么样条件的数据用LR最好？,知识点->提问
满足什么样条件的数据用LR最好？,"- 特征之间尽可能独立
    - 不独立所以我们把不独立的特征交叉了
        - 还记得FM的思路？
- 离散特征
    - 连续特征通常没有特别含义，31岁和32岁差在哪？
    - 离散特征方便交叉考虑
    - 在异常值处理上也更加方便
    - 使的lr满足分布假设
        - 什么分布假设？
- 在某种确定分类上的特征分布满足高斯分布
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wh7dd6bkj310w034gmb.jpg)
    - C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布
        - 实际中不满足的很多，不满足我们通常就离散化，oneHotEncode
此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中
",提问->答案
逻辑回归,LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,知识点->提问
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,"- 思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开
- 思路二：Exponential model 的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。
    - 二分类上：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmsc1vfkj30jm036wet.jpg)
    - 化简即为sigmoid
    - 以上思路源自：PRML（Pattern Recognition and Machine Learning）
- 思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数
",提问->答案
逻辑回归,利用几率odds的意义在哪？,知识点->提问
利用几率odds的意义在哪？,"- 直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯
- 由预测0/1的类别扩展到了预测0-1的概率值
- 任意阶可导的优秀性质    
",提问->答案
逻辑回归,Sigmoid函数到底起了什么作用？,知识点->提问
Sigmoid函数到底起了什么作用？,"- 数据规约：\[0,1]
- 线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感    
- sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题
",提问->答案
逻辑回归,LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,知识点->提问
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,"- 更新速度只与真实的x和y相关，与激活函数无关，更新平稳
    - 比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新
    - mse下的lr损失函数非凸，难以得到解析解
",提问->答案
逻辑回归,LR中若标签为+1和-1，损失函数如何推导？,知识点->提问
LR中若标签为+1和-1，损失函数如何推导？,"- way1:把0-1的sigmoid的lr结果Y映射为2y-1，推导不变
- way2:把激活函数换成tanh，因为tanh的值域范围为\[-1,1],满足结果，推导不变    
- way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可    
",提问->答案
逻辑回归,如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,知识点->提问
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,"- 如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果
- 每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了
- 增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠
- 泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差
",提问->答案
逻辑回归,LR可以用核么？可以怎么用？,知识点->提问
LR可以用核么？可以怎么用？,"结论：可以，加l2正则项后可用
原因：
- 核逻辑回归，需要把拟合参数w表示成z的线性组合及representer theorem理论。这边比较复杂，待更新，需要了解：
    - w拆解的z的线性组合中的系数α来源
    - representer theorem 的证明
        - 凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明
    - 如何将将W*表示成β的形式带到我们最佳化的问题
",提问->答案
逻辑回归,LR中的L1/L2正则项是啥？,知识点->提问
LR中的L1/L2正则项是啥？,"- L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg) ,u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项
- L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项
",提问->答案
逻辑回归,lr加l1还是l2好？,知识点->提问
lr加l1还是l2好？,"这个问题还可以换一个说法，l1和l2的各自作用。
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2    
",提问->答案
逻辑回归,正则化是依据什么理论实现模型优化？,知识点->提问
正则化是依据什么理论实现模型优化？,"结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。
",提问->答案
逻辑回归,LR可以用来处理非线性问题么？,知识点->提问
LR可以用来处理非线性问题么？,"- 特征交叉，类似fm
- 核逻辑回归，类似svm
- 线性变换+非线性激活，类似neural network
",提问->答案
逻辑回归,为什么LR需要归一化或者取对数?,知识点->提问
为什么LR需要归一化或者取对数?,"**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**
- 工程角度：
    - 加速收敛
    - 提高计算效率
- 理论角度:
    - 梯度下降过程稳定
    - 使得数据在某类上更服从高斯分布，满足前提假设，这个是必须要答出来的
    - [归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6)
",提问->答案
逻辑回归,为什么LR把特征离散化后效果更好？离散化的好处有哪些？,知识点->提问
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,"- 原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合
- 离散后结合正则化可以进行特征筛选，更好防止过拟合
- 数据的鲁棒性更好，不会因为无意义的连续值变动导致异常因素的影响，（31岁和32岁的差异在哪呢？）
- 离散变量的计算相对于连续变量更快    
",提问->答案
逻辑回归,逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,知识点->提问
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,"lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能    
",提问->答案
逻辑回归,LR对比万物？,知识点->提问
LR对比万物？,"- lr和线性回归
    - lr解用的极大似然，线性回归用的最小二乘
    - lr用于分类，线性回归用于回归
    - 但两者都是广义线性回归GLM问题
    - 两者对非线性问题的处理能力都是欠佳的
- lr和最大熵
    - 在解决二分类问题是等同的
- lr和svm
    - 都可分类，都是判别式模型思路
    - 通常都是用正则化进行规约
    - 模型上
        - lr是交叉熵，svm是HingeLoss
        - lr是全量数据拟合，svm是支持向量拟合
        - lr是参数估计有参数的前提假设，svm没有
        - lr依赖的是极大似然，svm依赖的是距离
- lr和朴素贝叶斯
    - 如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致
    - lr是判别模型，朴素贝叶斯是生成模型
    - lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立
- lr和最大熵模型
    - 本质没有区别
    - 最大熵模型在解决二分类问题就是逻辑回归
    - 最大熵模型在解决多分类问题的时候就是多项逻辑回归回归
",提问->答案
逻辑回归,LR梯度下降方法？,知识点->提问
LR梯度下降方法？,"- 随机梯度下降
    - 局部最优解，可跳出鞍点
    - 计算快
- 批梯度下降
    - 全局最优解
    - 计算量大
- mini批梯度下降
    - 综合以上两种方法
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中
",提问->答案
逻辑回归,LR的优缺点？,知识点->提问
LR的优缺点？,"- 优点
    - 简单，易部署，训练速度快
    - 模型下限较高
    - 可解释性强
- 缺点
    - 只能线性可分
    - 数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11)
    - 模型上限较低
",提问->答案
逻辑回归,除了做分类，你还会用LR做什么？,知识点->提问
除了做分类，你还会用LR做什么？,"特征筛选，特征的系数决定该特征的重要性
",提问->答案
逻辑回归,你有用过sklearn中的lr么？你用的是哪个包？,知识点->提问
你有用过sklearn中的lr么？你用的是哪个包？,"sklearn.linear_model.LogisticRegression
",提问->答案
逻辑回归,看过源码么？为什么去看？,知识点->提问
看过源码么？为什么去看？,"- 看部分参数的解释
    - 比如dual、weight_class中的1:0还是0:1比
    - 比如输出值的形式，输出的格式
",提问->答案
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,知识点->提问
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,"- penalty是正则化，solver是函数优化方法
- penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等
- 牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。
- l1和l2选择参考上面讲的正则化部分
- 随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法
",提问->答案
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,知识点->提问
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,"- 首先，决定是否为多分类的参数是multi_class
- 在二分类的时候，multi和ovr和auto都是一样的
- 在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况    
",提问->答案
逻辑回归,我的总结,知识点->提问
我的总结,"- 逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的
- 逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等
- 逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验
- 逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响",提问->答案
机器学习,随机森林,主科目到一级科目
随机森林,解释下随机森林?,知识点->提问
解释下随机森林?,"- 随机森林=bagging+决策树
- 随机：特征选择随机+数据采样随机
    - 特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机
    - 每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**
    - 数据采样，是有放回的采样
        - 1个样本**未被选到**的概率为p = (1 - 1/N)^N = 1/e，即为OOB
- 森林：多决策树组合
    - 可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票
",提问->答案
随机森林,随机森林用的是什么树？,知识点->提问
随机森林用的是什么树？,"CART树
",提问->答案
随机森林,随机森林的生成过程？,知识点->提问
随机森林的生成过程？,"- 生成单棵决策树
    - 随机选取样本
    - 从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂
    - 不需要剪枝，直到该节点的所有训练样例都属于同一类
- 生成若干个决策树
",提问->答案
随机森林,解释下随机森林节点的分裂策略？,知识点->提问
解释下随机森林节点的分裂策略？,"Gini系数
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)
",提问->答案
随机森林,随机森林的损失函数是什么？,知识点->提问
随机森林的损失函数是什么？,"- 分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益
- 回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae
- 参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)
",提问->答案
随机森林,为了防止随机森林过拟合可以怎么做?,知识点->提问
为了防止随机森林过拟合可以怎么做?,"- 增加树的数量
- 增加叶子结点的数据数量
- bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7)
- 随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高
",提问->答案
随机森林,随机森林特征选择的过程？,知识点->提问
随机森林特征选择的过程？,"特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无
    - 通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）
    - 是使用uniform或者gaussian抽取随机值替换原特征
    
",提问->答案
随机森林,是否用过随机森林，有什么技巧?,知识点->提问
是否用过随机森林，有什么技巧?,"- 除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForest-subspace变成randomForest-combination
",提问->答案
随机森林,RF的参数有哪些，如何调参？,知识点->提问
RF的参数有哪些，如何调参？,"要调整的参数主要是 n_estimators和max_features 
- n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好
- max_features是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多
    - 回归：max_features = n_features
    - 分类：max_features = sqrt(n_features)
其他参数中
- class_weight也可以调整正负样本的权重
-  max_depth = None 和 min_samples_split = 2 结合，为不限制生成一个不修剪的完全树
",提问->答案
随机森林,RF的优缺点 ？,知识点->提问
RF的优缺点 ？,"- 优点:
    - 不同决策树可以由不同主机并行训练生成，效率很高
    - 随机森林算法继承了CART的优点
    - 将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题
- 缺点：
    - 没有严格数学理论支持
",提问->答案
机器学习,集成学习,主科目到一级科目
集成学习,GBDT,一级科目到二级科目
GBDT,介绍一下Boosting的思想？,知识点->提问
介绍一下Boosting的思想？,"- 初始化训练一个弱学习器，初始化下的各条样本的权重一致
- 根据上一个弱学习器的结果，调整权重，使得错分的样本的权重变得更高
- 基于调整后的样本及样本权重训练下一个弱学习器
- 预测时直接串联综合各学习器的加权结果
",提问->答案
GBDT,最小二乘回归树的切分过程是怎么样的？,知识点->提问
最小二乘回归树的切分过程是怎么样的？,"- 回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值
- 分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性
- 属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值
- 递归重复以上步骤，直到满足叶子结点上值的要求
",提问->答案
GBDT,有哪些直接利用了Boosting思想的树模型？,知识点->提问
有哪些直接利用了Boosting思想的树模型？,"adaboost，gbdt等等
",提问->答案
GBDT,gbdt和boostingtree的boosting分别体现在哪里？,知识点->提问
gbdt和boostingtree的boosting分别体现在哪里？,"- boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）
- gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度
",提问->答案
GBDT,gbdt的中的tree是什么tree？有什么特征？,知识点->提问
gbdt的中的tree是什么tree？有什么特征？,"Cart tree，但是都是回归树
",提问->答案
GBDT,常用回归问题的损失函数？,知识点->提问
常用回归问题的损失函数？,"- mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg)
    - 负梯度：y-h(x)
    - 初始模型F0由目标变量的平均值给出
- 绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg)
    - 负梯度：sign(y-h(x))
    - 初始模型F0由目标变量的中值给出
- Huber损失：mse和绝对损失的结合
    - 负梯度：y-h(x)和sign(y-h(x))分段函数
    - 它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量
",提问->答案
GBDT,常用分类问题的损失函数？,知识点->提问
常用分类问题的损失函数？,"- 对数似然损失函数
    - 二元且标签y属于{-1,+1}：𝐿(𝑦,𝑓(𝑥))=𝑙𝑜𝑔(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))
        - 负梯度：y/(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))
    - 多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg)
- 指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))
    - 负梯度：y·𝑒𝑥𝑝(−𝑦𝑓(𝑥))
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同
",提问->答案
GBDT,什么是gbdt中的损失函数的负梯度？,知识点->提问
什么是gbdt中的损失函数的负梯度？,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果y-H(x)正好与boostingtree的拟合残差一致
",提问->答案
GBDT,如何用损失函数的负梯度实现gbdt？,知识点->提问
如何用损失函数的负梯度实现gbdt？,"- 利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置
- 构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg)
- 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)
    - 首先，根据feature切分后的损失均方差大小，选取最优的特征切分
    - 其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值
    - 这样就完整的构造出一棵树：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bfr5cn5j303d01kjr6.jpg)
- 本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg)
",提问->答案
GBDT,拟合损失函数的负梯度为什么是可行的？,知识点->提问
拟合损失函数的负梯度为什么是可行的？,"- 泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg)
- m轮树模型可以写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h8zovulj305v00iglf.jpg)
    - 对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m-1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)
- 我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进
",提问->答案
GBDT,即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,知识点->提问
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,"- 前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是 局部最优方向（负梯度）*步长（𝛽）
- 依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此 当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，Boosting Tree也很难处理回归之外问题。 而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理
",提问->答案
GBDT,Shrinkage收缩的作用？,知识点->提问
Shrinkage收缩的作用？,"每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。 这个技巧类似于梯度下降里的学习率
- 原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94i9bokjzj306g00iglf.jpg)
- Shrinkage：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94iawlfq3j307600it8j.jpg)
",提问->答案
GBDT,feature属性会被重复多次使用么？,知识点->提问
feature属性会被重复多次使用么？,"会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大
",提问->答案
GBDT,gbdt如何进行正则化的？,知识点->提问
gbdt如何进行正则化的？,"- 子采样
    - 每一棵树基于原始原本的一个子集进行训练
    - rf是有放回采样，gbdt是无放回采样
    - 特征子采样可以来控制模型整体的方差
- 利用Shrinkage收缩，控制每一棵子树的贡献度
- 每棵Cart树的枝剪
",提问->答案
GBDT,为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,知识点->提问
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,"- 对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受
- bagging，关注于提升分类器的泛化能力
- boosting，关注于提升分类器的精度
",提问->答案
GBDT,gbdt的优缺点？,知识点->提问
gbdt的优缺点？,"优点：
- 数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强
- 调参相对较简单
",提问->答案
GBDT,gbdt和randomforest区别？,知识点->提问
gbdt和randomforest区别？,"- 相同：
    - 都是多棵树的组合
- 不同：
    - RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
    - gbdt对异常值比rf更加敏感
    - gbdt是串行，rf是并行
    - gbdt是cart回归树，rf是cart分类回归树都可以
    - gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能
    - gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均
    
",提问->答案
GBDT,GBDT和LR的差异？,知识点->提问
GBDT和LR的差异？,"- 从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线
- 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合",提问->答案
集成学习,LightGBM,一级科目到二级科目
LightGBM,XGboost缺点,知识点->提问
XGboost缺点,"- 每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间
- 预排序方法需要保存特征值，及特征排序后的索引结果，占用空间
- level-wise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数
",提问->答案
LightGBM,LightGBM对Xgboost的优化,知识点->提问
LightGBM对Xgboost的优化,"- 将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点
    - 优点：时间开销由O(features)降低到O(bins)
    - 缺点：很多数据精度被丢失，相当于用了正则
- 利用leaf-wise代替level-wise
    - 每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环
- 直方图做差加速
",提问->答案
LightGBM,LightGBM亮点,知识点->提问
LightGBM亮点,"- 单边梯度采样 Gradient-based One-Side Sampling (GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益
- 互斥稀疏特征绑定Exclusive Feature Bundling (EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]
",提问->答案
集成学习,Xgboost,一级科目到二级科目
Xgboost,xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,知识点->提问
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,"- 显示的把树模型复杂度作为正则项加到优化目标中
- 优化目标计算中用到二阶泰勒展开代替一阶，更加准确
- 实现了分裂点寻找近似算法
    - 暴力枚举
    - 近似算法（分桶）
- 更加高效和快速
    - 数据事先排序并且以block形式存储，有利于并行计算
    - 基于分布式通信框架rabit，可以运行在MPI和yarn上
    - 实现做了面向体系结构的优化，针对cache和内存做了性能优化
",提问->答案
Xgboost,xgboost和gbdt的区别？,知识点->提问
xgboost和gbdt的区别？,"- 模型优化上：
    - 基模型的优化：
        - gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归)
    - 损失函数上的优化：
        - gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开
        - gbdt没有在loss中带入结点个数和预测值的正则项
    - 特征选择上的优化：
        - 实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索
        - 节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权
    - 正则化的优化：
        - 特征采样
        - 样本采样
- 工程优化上：
    - xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行
    - cache-aware, out-of-core computation
    - 支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit
    
",提问->答案
Xgboost,xgboost优化目标/损失函数改变成什么样？,知识点->提问
xgboost优化目标/损失函数改变成什么样？,"- 原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mjezeisj307401fmx0.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg)
- 改变：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mldvhz5j30ay01k3yf.jpg)
    - J为叶子结点的个数，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mo56g1vj300o00e0s1.jpg)为第j个叶子结点中的最优值    
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒二阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mtjds7qj309r0193yg.jpg)
",提问->答案
Xgboost,xgboost如何使用MAE或MAPE作为目标函数？,知识点->提问
xgboost如何使用MAE或MAPE作为目标函数？,"MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg)
MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg)
- 利用可导的函数逼近MAE或MAPE
    - mse
    - Huber loss
    - Pseudo-Huber loss
",提问->答案
Xgboost,xgboost如何寻找分裂节点的候选集？,知识点->提问
xgboost如何寻找分裂节点的候选集？,"- 暴力枚举
    - 法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大
- 近似算法（approx）
    - 近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升
        - 离散值直接分桶
        - 连续值分位数分桶
",提问->答案
Xgboost,xgboost如何处理缺失值？,知识点->提问
xgboost如何处理缺失值？,"- 训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个
- 预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树
",提问->答案
Xgboost,xgboost在计算速度上有了哪些点上提升？,知识点->提问
xgboost在计算速度上有了哪些点上提升？,"- 特征预排序
    - 按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可
    - block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间
",提问->答案
Xgboost,xgboost特征重要性是如何得到的？,知识点->提问
xgboost特征重要性是如何得到的？,"- ’weight‘：代表着某个特征被选作分裂结点的次数；
- ’gain‘：使用该特征作为分类结点的信息增益；
- ’cover‘：某特征作为划分结点，覆盖样本总数的平均值；
",提问->答案
Xgboost,XGBoost中如何对树进行剪枝？,知识点->提问
XGBoost中如何对树进行剪枝？,"- 在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方
- 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂
- XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝
",提问->答案
Xgboost,XGBoost模型如果过拟合了怎么解决？,知识点->提问
XGBoost模型如果过拟合了怎么解决？,"- 直接修改模型：
    - 降低树的深度
    - 增大叶子结点的权重
    - 增大惩罚系数
- subsample的力度变大，降低异常点的影响
- 减小learning rate，提高estimator
",提问->答案
Xgboost,xgboost如何调参数？,知识点->提问
xgboost如何调参数？,"- 先确定learningrate和estimator
- 再确定每棵树的基本信息，max_depth和 min_child_weight
- 再确定全局信息：比如最小分裂增益，子采样参数，正则参数
- 重新降低learningrate，得到最优解",提问->答案
深度学习,Attention,主科目到一级科目
Attention,Attention对比RNN和CNN，分别有哪点你觉得的优势？,知识点->提问
Attention对比RNN和CNN，分别有哪点你觉得的优势？,"- 对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向
- 对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序
",提问->答案
Attention,写出Attention的公式？,知识点->提问
写出Attention的公式？,"![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986pbyaj0j308t019t8l.jpg)
",提问->答案
Attention,解释你怎么理解Attention的公式的？,知识点->提问
解释你怎么理解Attention的公式的？,"- Q:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rjgs7qj301400g741.jpg),K:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rz9d1ej301a00g741.jpg),V:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986shjz4tj301900g741.jpg)
    - 首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列
- 除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小
    - qk除了点击还可以直接拼接再内接一个参数变量等等
- Multi-Attention只是重复了h次的Attention，最后把结果进行拼接
",提问->答案
Attention,Attention模型怎么避免词袋模型的顺序问题的困境的？,知识点->提问
Attention模型怎么避免词袋模型的顺序问题的困境的？,"增加了position Embedding
- 可以直接随机初始化
- 也可以参考Google的sin/cos位置初始化方法
    - 如此选取的原因之一是sin(a+b)=sin(a)cos(b)+cos(a)sin(b)。这很好的保证了位置p+k可以表示成p的线性变换，相对位置可解释
",提问->答案
Attention,"Attention机制，里面的q,k,v分别代表什么？",知识点->提问
"Attention机制，里面的q,k,v分别代表什么？","- Q：指的是query，相当于decoder的内容
- K：指的是key，相当于encoder的内容
- V：指的是value，相当于encoder的内容
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码
",提问->答案
Attention,为什么self-attention可以替代seq2seq？,知识点->提问
为什么self-attention可以替代seq2seq？,"- seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息
- self-attention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型
",提问->答案
Attention,维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,知识点->提问
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,"- 假设向量 q 和 k 的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积 qk 的均值是0，方差是 dk
- 针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)
    - E(XY) = E(X)E(Y)=0
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9enzh4vzvj30gh017t8t.jpg)
    - 所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化",提问->答案
深度学习,batch_normalization,主科目到一级科目
batch_normalization,你觉得bn过程是什么样的？,知识点->提问
你觉得bn过程是什么样的？,"- 按batch进行期望和标准差计算
- 对整体数据进行标准化
- 对标准化的数据进行线性变换
    - 变换系数需要学习
",提问->答案
batch_normalization,手写一下bn过程？,知识点->提问
手写一下bn过程？,"- mu = 1.0*np.sum(X,axis = 0)/X.shape\[0]
- Xmu = X - mu
- sq = Xmu**2
- var = 1.0*np.sum(sq,axis=0)/X.shape\[0]
- out = alhpa*(X-Xmu)/np.sqrt(var+eps) + beta
",提问->答案
batch_normalization,知道LN么？讲讲原理,知识点->提问
知道LN么？讲讲原理,"- 和bn过程近似，只是作用的方向是在维度上，而不是batch上
- 这样做的好处就是不会受到batch大小不一致的影响",提问->答案
深度学习,残差网络,主科目到一级科目
残差网络,介绍残差网络,知识点->提问
介绍残差网络,"- 常见结构，CV里面用的比较多
    - y=F(x)+x
    - y=F(x)+indentity `*` x
",提问->答案
残差网络,残差网络为什么能解决梯度消失的问题,知识点->提问
残差网络为什么能解决梯度消失的问题,"- ![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is4x344xj304j01jglf.jpg)
- ![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8cigikj305s0180sl.jpg)
    - 虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题
",提问->答案
残差网络,残差网络残差作用,知识点->提问
残差网络残差作用,"- 防止梯度消失
- 恒等映射使得网络突破层数限制，避免网络退化
- 对输出的变化更敏感
    - X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1
    - X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2
    - H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化
",提问->答案
残差网络,你平时有用过么？或者你在哪些地方遇到了,知识点->提问
你平时有用过么？或者你在哪些地方遇到了,"- 我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络
- Bert和Transform中attention部分残差网络用的比较频繁
",提问->答案
自然语言处理,Bert,主科目到一级科目
Bert,Bert的双向体现在什么地方？,知识点->提问
Bert的双向体现在什么地方？,"mask+attention，mask的word结合全部其他encoder word的信息
",提问->答案
Bert,Bert的是怎样实现mask构造的？,知识点->提问
Bert的是怎样实现mask构造的？,"- MLM：将完整句子中的部分字mask，预测该mask词
- NSP：为每个训练前的例子选择句子 A 和 B 时，50% 的情况下 B 是真的在 A 后面的下一个句子， 50% 的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句
",提问->答案
Bert,在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,知识点->提问
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,"- mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了
- 随机替换也帮助训练修正了\[unused]和\[UNK]
- 强迫文本记忆上下文信息
",提问->答案
Bert,为什么BERT有3个嵌入层，它们都是如何实现的？,知识点->提问
为什么BERT有3个嵌入层，它们都是如何实现的？,"- input_id是语义表达，和传统的w2v一样，方法也一样的lookup
- segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup
- position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup
",提问->答案
Bert,bert的损失函数？,知识点->提问
bert的损失函数？,"- MLM:在 encoder 的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用 softmax 计算mask中每个单词的概率
- NSP:用一个简单的分类层将 \[CLS] 标记的输出变换为 2×1 形状的向量,用 softmax 计算 IsNextSequence 的概率
- MLM+NSP即为最后的损失
",提问->答案
Bert,手写一个multi-head attention？,知识点->提问
手写一个multi-head attention？,"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)
",提问->答案
Bert,长文本预测如何构造Tokens？,知识点->提问
长文本预测如何构造Tokens？,"- head-only：保存前 510 个 token （留两个位置给 \[CLS] 和 \[SEP] ）
- tail-only：保存最后 510 个token
- head + tail ：选择前128个 token 和最后382个 token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）
",提问->答案
Bert,你用过什么模块？bert流程是怎么样的？,知识点->提问
你用过什么模块？bert流程是怎么样的？,"- modeling.py
- 首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可
- 在通过embedding_lookup把input_id向量化，如果存在句子之间的位置差异则需要对segment_id进行处理，否则无操作；再进行position_embedding操作
- 进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤
- 输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[-1]
",提问->答案
Bert,知道分词模块：FullTokenizer做了哪些事情么？,知识点->提问
知道分词模块：FullTokenizer做了哪些事情么？,"- BasicTokenizer：根据空格等进行普通的分词
    - 包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等
- WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece
    - 中文不处理，因为有词缀一说：解决OOV
",提问->答案
Bert,Bert中如何获得词意和句意？,知识点->提问
Bert中如何获得词意和句意？,"- get_pooled_out代表了涵盖了整条语句的信息
- get_sentence_out代表了这个获取每个token的output 输出，用的是cls向量
    
",提问->答案
Bert,源码中Attention后实际的流程是如何的？,知识点->提问
源码中Attention后实际的流程是如何的？,"- Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output
- 所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output
",提问->答案
Bert,为什么要在Attention后使用残差结构？,知识点->提问
为什么要在Attention后使用残差结构？,"残差结构能够很好的消除层数加深所带来的信息损失问题
",提问->答案
Bert,平时用官方Bert包么？耗时怎么样？,知识点->提问
平时用官方Bert包么？耗时怎么样？,"- 第三方：bert_serving
- 官方：bert_base
- 耗时：64GTesla，64max_seq_length，80-90doc/s
    - 在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高
    
",提问->答案
Bert,你觉得BERT比普通LM的新颖点？,知识点->提问
你觉得BERT比普通LM的新颖点？,"- mask机制
- next_sentence_predict机制
",提问->答案
Bert,elmo、GPT、bert三者之间有什么区别？,知识点->提问
elmo、GPT、bert三者之间有什么区别？,"- 特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。
- 单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。
- GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子",提问->答案
自然语言处理,CRF,主科目到一级科目
CRF,阐述CRF原理？,知识点->提问
阐述CRF原理？,"- 首先X,Y是随机变量，P(Y/X)是给定X条件下Y的条件概率分布
- 如果Y满足马尔可夫满足马尔科夫性，及不相邻则条件独立
- 则条件概率分布P(Y|X)为条件随机场CRF
",提问->答案
CRF,线性链条件随机场的公式是？,知识点->提问
线性链条件随机场的公式是？,"- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ah66y1nxj30cr015747.jpg)
",提问->答案
CRF,CRF与HMM区别?,知识点->提问
CRF与HMM区别?,"- CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)
- CRF是无向图，HMM是有向图
- CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率
",提问->答案
CRF,Bert+crf中的各部分作用详解？,知识点->提问
Bert+crf中的各部分作用详解？,"- Bert把中文文本进行了embedding，得到每个字的表征向量
- dense操作得到了每个文本文本对应的未归一化的tag概率
- CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程
    - CRF层能从训练数据中获得约束性的规则
        - 比如开始都是以xxx-B，中间都是以xxx-I，结尾都是以xxx-E
        - 比如在只有label1-I，label2-I..的情况下，不会出现label1-B",提问->答案
自然语言处理,GloVe,主科目到一级科目
GloVe,GolVe的损失函数？,知识点->提问
GolVe的损失函数？,"- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9achsr4agj3094016a9x.jpg)
",提问->答案
GloVe,解释GolVe的损失函数？,知识点->提问
解释GolVe的损失函数？,"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。
",提问->答案
GloVe,为什么GolVe会用的相对比W2V少？,知识点->提问
为什么GolVe会用的相对比W2V少？,"- GloVe算法本身使用了全局信息，自然内存费的也就多一些
    - 公现矩阵，NXN的，N为词袋量
- W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能
",提问->答案
GloVe,如何处理未出现词？,知识点->提问
如何处理未出现词？,"按照词性进行已知词替换，\[unknow-n],\[unknow-a],\[unknow-v]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown-?向量替代
",提问->答案
自然语言处理,LDA,主科目到一级科目
LDA,详述LDA原理？,知识点->提问
详述LDA原理？,"- 从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)
    - 多项式分布的共轭分布是狄利克雷分布
    - 二项式分布的共轭分布是Beta分布
- 从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)
- 从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)
- 从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg)
- 文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg)
- 采用EM方法修正词-主题矩阵+主题-文档矩阵直至收敛
",提问->答案
LDA,LDA中的主题矩阵如何计算?词分布矩阵如何计算？,知识点->提问
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点
- 吉布斯采样
	- 先随机给每个词附上主题
	- 因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布
	- 有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布
	- 根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题
	- 收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布
- 通常会引申出如下几个问题：
	- 吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）
	- MCMC中什么叫做蒙特卡洛方法？
		- 通常用于求概率密度的积分
		- 用已知分布去评估未知分布
		- reject-acpect过程
	- 马尔科夫链收敛性质？
		- 非周期性，不能出现死循环
		- 连通性，不能有断点
	- MCMC中什么叫做马尔科夫链采样过程？
		- 先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵
		- 再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)
	- 给定平稳矩阵如何得到概率分布样本集？
		- M-C采样
			- 给定任意的转移矩阵Q，已知π(i)p(i,j) = π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j) = π(j)Q(j,i)a(j,i)
			- 根据Q的条件概率Q(x/xt)得到xt+1
			- u~uniform
			- u<π(xt+1)Q(xt+1,xt) 则accept，就和蒙特模拟一样否则xt+1 = xt
			- (xt,xt+1...)代表着我们的分布样本集
		- M-H采样
			- 左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度
		- Gibbs采样
			- 同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样
	- 什么叫做坐标转换采样？
		- 平面上任意两点满足细致平稳条件π(A)P(A->B) = π(B)P(B->A)
		- 从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2
		- 从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1
		- 其为一对样本，有点像Lasso回归中的固定n-1维特征求一维特征求极值的思路
- 变分推断EM算法
	- 整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta
	- 变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**
		- 实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵
	- EM过程
		- E：最小化相对熵，偏导为0得到变分参数
		- M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值
",提问->答案
LDA,LDA的共轭分布解释下?,知识点->提问
LDA的共轭分布解释下?,"以多项式分布-狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布
",提问->答案
LDA,PLSA和LDA的区别?,知识点->提问
PLSA和LDA的区别?,"- LDA是加了狄利克雷先验的PLSA
- PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的
- LDA是贝叶斯思想，PLSA是MLE
",提问->答案
LDA,怎么确定LDA的topic个数,知识点->提问
怎么确定LDA的topic个数,"- 对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity
- 多次尝试，调优perplexity-topic number曲线
    - 困惑度越小，越容易过拟合
    - 某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg)
",提问->答案
LDA,LDA和Word2Vec区别？LDA和Doc2Vec区别？,知识点->提问
LDA和Word2Vec区别？LDA和Doc2Vec区别？,"- LDA比较是doc，word2vec是词
- LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示
    - LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息
- LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果
",提问->答案
LDA,LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,知识点->提问
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,"- 通常alpha为1/k，k为类别数，beta一般为0.01
- alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确
- beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度
- chucksize大一些更新的过程比较平稳，收敛更加平稳
- 迭代次数一般不超过2000次，200万doc大约在2300次收敛",提问->答案
自然语言处理,Word2Vec,主科目到一级科目
Word2Vec,从隐藏层到输出的Softmax层的计算有哪些方法？,知识点->提问
从隐藏层到输出的Softmax层的计算有哪些方法？,"- 层次softmax
- 负采样
",提问->答案
Word2Vec,层次softmax流程？,知识点->提问
层次softmax流程？,"- 构造Huffman Tree
- 最大化对数似然函数
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9adl5ex45j305g00qmwz.jpg)
        - 输入层：是上下文的词语的词向量
        - 投影层：对其求和，所谓求和，就是简单的向量加法
        - 输出层：输出最可能的word
    - 沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数
- 对每层每个变量求偏导，参考sgd
",提问->答案
Word2Vec,负采样流程？,知识点->提问
负采样流程？,"- 统计每个词出现对概率，丢弃词频过低对词
- 每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）
- 负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布
",提问->答案
Word2Vec,word2vec两种方法各自的优势?,知识点->提问
word2vec两种方法各自的优势?,"- **Mikolov 的原论文，Skip-gram 在处理少量数据时效果很好，可以很好地表示低频单词。而 CBOW 的学习速度更快，对高频单词有更好的表示**
- Skip-gram的时间复杂度是o(kv),CBOW的时间复杂度o(v)
",提问->答案
Word2Vec,怎么衡量学到的embedding的好坏?,知识点->提问
怎么衡量学到的embedding的好坏?,"- 从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关
- 对item2vec得到的词向量进行聚类或者可视化
",提问->答案
Word2Vec,word2vec和glove区别？,知识点->提问
word2vec和glove区别？,"- word2vec是基于邻近词共现，glove是基于全文共现
    - word2vec利用了负采样或者层次softmax加速，相对更快
    - glove用了全局共现矩阵，更占内存资源
- word2vec是“predictive”的模型，而GloVe是“count-based”的模型
",提问->答案
Word2Vec,你觉得word2vec有哪些问题？,知识点->提问
你觉得word2vec有哪些问题？,"- 没考虑词序
- 对于中文依赖分词结果的好坏
- 新生词无法友好处理
- 无正则化处理",提问->答案
