{"cells":[{"cell_type":"markdown","source":["# 注意：\n","该文件中的模型已成功在colab训练，生成了parameter.kpl参数文件，和bert.h5及bert_crf.h5模型文件。"],"metadata":{"id":"7rwVDc02bo5Y"}},{"cell_type":"markdown","source":["# 先布置好colab环境"],"metadata":{"id":"mZgMhfjbYbL7"}},{"cell_type":"code","source":["#colab中运行jupyter文件的步骤：\n","# 1.挂载云盘\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# 2.安装需要的软件\n","!pip3 install transformers\n","!pip3 install pytorch-crf\n","\n","import os\n","def get_root_dir():\n","    if os.path.exists('/content/gdrive/MyDrive/第二次进行实体识别-面向课程_toColab/'):\n","        return '/content/gdrive/MyDrive/第二次进行实体识别-面向课程_toColab/' #在Colab里\n","    else:\n","        return './' #在本地\n","\n","# 3.调用系统命令，切换到对应工程路径，相当于cd，但是直接!cd是不行的\n","print(\"path:\",get_root_dir())\n","os.chdir(get_root_dir())\n","\n","# 4.再次确认路径\n","!pwd\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzLSJhHxS0aU","executionInfo":{"status":"ok","timestamp":1640433878188,"user_tz":-480,"elapsed":7930,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}},"outputId":"6ad9faf8-ca84-42a9-b05a-8f02116b1564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.7/dist-packages (0.7.2)\n","path: /content/gdrive/MyDrive/第二次进行实体识别-面向课程_toColab/\n","/content/gdrive/MyDrive/第二次进行实体识别-面向课程_toColab\n","bert+bert_crf.ipynb\t data\t\t     test.ipynb\n","bert_crf.h5\t\t model_zoo.py\t     安装crf.txt\n","bert.h5\t\t\t parameter.pkl\t     测试模型.ipynb\n","bilstm+bilstm_crf.ipynb  prev_trained_model\n"]}]},{"cell_type":"markdown","source":["# 数据预处理"],"metadata":{"id":"l_uLKeUtS6ka"}},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T08:47:44.294032Z","start_time":"2021-08-28T08:47:43.639093Z"},"id":"RYBHsMYbSJUS","outputId":"4c70dafa-2b2e-4010-dd99-2f6b03aadbd2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640434182387,"user_tz":-480,"elapsed":2506,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["parameter does exist!\n","d_model : 768\n","hid_dim : 300\n","epoch : 2\n","batch_size : 50\n","n_layers : 2\n","dropout : 0.1\n","device : cuda:0\n","lr : 0.001\n","momentum : 0.99\n"]}],"source":["from collections import defaultdict\n","from operator import itemgetter\n","from tqdm import tqdm\n","import numpy as np\n","import random\n","import torch \n","import jieba\n","import json\n","import os\n","\n","import pickle as pk\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","    torch.cuda.set_device(0)\n","else:\n","    device = torch.device('cpu')\n","# 确定模型训练方式，GPU训练或CPU训练\n","\n","#参数\n","parameter_copy = {\n","    # 此处embedding维度为768\n","    'd_model':768, \n","    # rnn的隐层维度为300\n","    'hid_dim':300,\n","    # 训练的批次为100轮\n","    'epoch':2,\n","    # 单次训练的batch_size为100条数据\n","    'batch_size':50,\n","    # 设置两个lstm，原文应该是一个\n","    'n_layers':2,\n","    # 设置dropout，为防止过拟合\n","    'dropout':0.1,\n","    # 配置cpu、gpu\n","    'device':device,\n","    # 设置训练学习率\n","    'lr':0.001,\n","    # 优化器的参数，动量主要用于随机梯度下降\n","    'momentum':0.99,\n","}\n","\n","def build_dataSet(parameter):\n","    data_name = ['train','dev']\n","    # 准备相应的字典\n","    data_set = {}\n","    key_table = defaultdict(int)\n","    vocab_table = defaultdict(int)\n","    # 预先准备相应的标志位\n","    vocab_table['<PAD>'] = 0\n","    vocab_table['<UNK>'] = 0\n","    # 数据内容可以参考data文件夹下的README，基于CLUENER 数据进行处理\n","    # 因为有两份数据，dev和train，因为构建时候同时进行构建\n","    for i in data_name:\n","        data_set[i] = []\n","        data_src = open('data/'+i+'.json','r',encoding = 'utf-8').readlines()\n","        for data in data_src:\n","            # 加载相应的数据\n","            data = json.loads(data)\n","            # 获取对应的文本和标签\n","            text = list(data['text'])\n","            label = data['label']\n","            # 初始化标准ner标签\n","            label_new = ['O']*len(text)\n","            key_table['O']\n","            # 根据其所带有的标签，如game、address进行数据提取\n","            for keys in label:\n","                inds = label[keys].values()\n","                # 因为其标签下的数据是一个数组，代表这类型标签的数据有多个\n","                # 因此循环处理，且护士其keys（文本内容），因为可以通过id索引到\n","                for id_list in inds:\n","                    for ind in id_list:\n","                        if ind[1] - ind[0] == 0:\n","                            # 当id号相同，表明这个实体只有一个字，\n","                            # 那么他的标签为'S-'+对应的字段\n","                            keys_list = ['S-'+keys]\n","                            label_new[ind[0]] = keys_list[0]\n","                        if ind[1] - ind[0] == 1:\n","                            # 如果id号相差，仅为1，表明这个实体有两个字\n","                            # 那么他的标签为 B-*，E-*，表明开始和结束的位置\n","                            keys_list = ['B-'+keys,'E-'+keys]\n","                            label_new[ind[0]] = keys_list[0]\n","                            label_new[ind[1]] = keys_list[1]\n","                        if ind[1] - ind[0] > 1:\n","                            # 如果id号相差，大于1，表明这个实体有多个字\n","                            # 那么他的标签除了 B-*，E-*，表明开始和结束的位置\n","                            # 还应该有I-*，来表明中间的位置\n","                            keys_list = ['B-'+keys,'I-'+keys,'E-'+keys]\n","                            label_new[ind[0]] = keys_list[0]\n","                            label_new[ind[0]+1:ind[1]] = [keys_list[1]]*(ind[1]-1-ind[0])\n","                            label_new[ind[1]] = keys_list[2]\n","                        for key in keys_list:\n","                            # 为了后面标签转id，提前准好相应的字典\n","                            key_table[key] += 1\n","            # 此处用于构建文本的字典\n","            for j in text:\n","                vocab_table[j] += 1\n","            # 保存原始的文本和处理好的标签\n","            data_set[i].append([text,label_new])\n","    # 保存标签转id，id转标签的字典\n","    key2ind = dict(zip(key_table.keys(),range(len(key_table))))\n","    ind2key = dict(zip(range(len(key_table)),key_table.keys()))\n","    # 保存字转id，id转字的字典\n","    word2ind = dict(zip(vocab_table.keys(),range(len(vocab_table))))\n","    ind2word = dict(zip(range(len(vocab_table)),vocab_table.keys()))\n","    parameter['key2ind'] = key2ind\n","    parameter['ind2key'] = ind2key\n","    parameter['word2ind'] = word2ind\n","    parameter['ind2word'] = ind2word\n","    parameter['data_set'] = data_set\n","    parameter['output_size'] = len(key2ind)\n","    parameter['word_size'] = len(word2ind)\n","    return parameter\n","\n","\n","def batch_yield_bert(parameter,shuffle = True,isTrain = True):\n","    # 构建数据迭代器\n","    # 根据训练状态或非训练状态获取相应数据\n","    data_set = parameter['data_set']['train'] if isTrain else parameter['data_set']['dev']\n","    Epoch = parameter['epoch'] if isTrain else 1\n","    for epoch in range(Epoch):\n","        # 每轮对原始数据进行随机化\n","        if shuffle:\n","            random.shuffle(data_set)\n","        inputs,targets = [],[]\n","        max_len = 0\n","        for items in tqdm(data_set):\n","            # 基于所构建的字典，将原始文本转成id，进行多分类\n","            # 此处和bilstm处不一致，使用bert自带字典\n","            input = tokenizer.convert_tokens_to_ids(items[0])\n","            target = itemgetter(*items[1])(parameter['key2ind'])\n","            target = target if type(target) == type(()) else (target,0)\n","            if len(input) > max_len:\n","                max_len = len(input)\n","            inputs.append(list(input))\n","            targets.append(list(target))\n","            if len(inputs) >= parameter['batch_size']:\n","                # 填空补齐\n","                inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n","                targets = [i+[0]*(max_len-len(i)) for i in targets]\n","                yield list2torch(inputs),list2torch(targets),None,False\n","                inputs,targets = [],[]\n","                max_len = 0\n","        inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n","        targets = [i+[0]*(max_len-len(i)) for i in targets]\n","        yield list2torch(inputs),list2torch(targets),epoch,False\n","        inputs,targets = [],[]\n","        max_len = 0\n","    yield None,None,None,True\n","            \n","\n","def list2torch(ins):\n","    return torch.from_numpy(np.array(ins)).long().to(parameter['device'])\n","\n","# 因此这边提前配置好用于训练的相关参数\n","# 不要每次重新生成\n","if not os.path.exists('parameter.pkl'):\n","    print(\"parameter not exist!\")\n","    parameter = parameter_copy\n","    # 构建相关字典和对应的数据集\n","    parameter = build_dataSet(parameter)\n","    pk.dump(parameter,open('parameter.pkl','wb'))\n","else:\n","    print(\"parameter does exist!\")\n","    # 读取已经处理好的parameter，但是考虑到模型训练的参数会发生变化，\n","    # 因此此处对于parameter中模型训练参数进行替换\n","    parameter = pk.load(open('parameter.pkl','rb'))\n","    for i in parameter_copy.keys():\n","        if i not in parameter:\n","            parameter[i] = parameter_copy[i]\n","            continue\n","        if parameter_copy[i] != parameter[i]:\n","            parameter[i] = parameter_copy[i]\n","    for i in parameter_copy.keys():\n","        print(i,':',parameter[i])\n","    pk.dump(parameter,open('parameter.pkl','wb'))\n","    del parameter_copy,i"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"oRRBOY9QYaDc"}},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T07:52:15.394821Z","start_time":"2021-08-28T07:52:15.377868Z"},"id":"ZrlQlHtXSJUZ"},"outputs":[],"source":["tokenizer.convert_tokens_to_ids(['你','好']) #有时候可以有时候不行"]},{"cell_type":"markdown","metadata":{"id":"GlpFlGy6SJUZ"},"source":["# 基于bert预训练模型"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T08:47:48.896814Z","start_time":"2021-08-28T08:47:48.646661Z"},"id":"2ZwfdcB_SJUa"},"outputs":[],"source":["# 加载transformers相关用于模型训练的包\n","from transformers import WEIGHTS_NAME, BertConfig,get_linear_schedule_with_warmup,AdamW, BertTokenizer\n","from transformers import BertModel,BertPreTrainedModel\n","from torch.nn import CrossEntropyLoss\n","import torch.nn as nn\n","import torch\n","\n","import torch.nn.functional as F # pytorch 激活函数的类\n","from torch import nn,optim # 构建模型和优化器\n","\n","class bert(BertPreTrainedModel):\n","    def __init__(self, config,parameter):\n","        super(bert, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        # 写法就是torch的写法，区别在于BertModel的网络结构已经不需要自己完成\n","        # 上游特征提取\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        # 下游对应任务处理，直接使用fc，进行线性变换即可\n","        embedding_dim = parameter['d_model']\n","        output_size = parameter['output_size']\n","        self.fc = nn.Linear(embedding_dim, output_size)\n","        self.init_weights()\n","        \n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None,labels=None):\n","        # 基于bert进行特征提取（上游）\n","        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        # 提取对应的encoder输出\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        # 下游直接进行全连接\n","        logits = self.fc(sequence_output)\n","        # 完成模型输出\n","        return logits.view(-1,logits.size(-1))\n","    \n","# 加载bert自带的config文件，初始化bert需要\n","# 加载预训练模型bert的字典，数据处理时需要使用\n","config_class, bert, tokenizer_class = BertConfig, bert, BertTokenizer\n","config = config_class.from_pretrained(\"prev_trained_model\")\n","tokenizer = tokenizer_class.from_pretrained(\"prev_trained_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T08:48:05.480250Z","start_time":"2021-08-28T08:48:03.484433Z"},"id":"TxjpKmtnSJUa"},"outputs":[],"source":["model = bert(config,parameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T08:52:03.187576Z","start_time":"2021-08-28T08:52:03.167630Z"},"id":"lSkzNacDSJUb","outputId":"061a89bb-d5c2-4980-b1cd-74f355ddcb78","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640434211826,"user_tz":-480,"elapsed":518,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.pooler.dense.weight', 'fc.weight']\n","****************************************************************************************************\n","['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.bias', 'fc.bias']\n","****************************************************************************************************\n"]},{"output_type":"execute_result","data":{"text/plain":["['bert.embeddings.word_embeddings.weight',\n"," 'bert.embeddings.position_embeddings.weight',\n"," 'bert.embeddings.token_type_embeddings.weight',\n"," 'bert.embeddings.LayerNorm.weight',\n"," 'bert.embeddings.LayerNorm.bias',\n"," 'bert.encoder.layer.0.attention.self.query.weight',\n"," 'bert.encoder.layer.0.attention.self.query.bias',\n"," 'bert.encoder.layer.0.attention.self.key.weight',\n"," 'bert.encoder.layer.0.attention.self.key.bias',\n"," 'bert.encoder.layer.0.attention.self.value.weight',\n"," 'bert.encoder.layer.0.attention.self.value.bias',\n"," 'bert.encoder.layer.0.attention.output.dense.weight',\n"," 'bert.encoder.layer.0.attention.output.dense.bias',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.0.intermediate.dense.weight',\n"," 'bert.encoder.layer.0.intermediate.dense.bias',\n"," 'bert.encoder.layer.0.output.dense.weight',\n"," 'bert.encoder.layer.0.output.dense.bias',\n"," 'bert.encoder.layer.0.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.attention.self.query.weight',\n"," 'bert.encoder.layer.1.attention.self.query.bias',\n"," 'bert.encoder.layer.1.attention.self.key.weight',\n"," 'bert.encoder.layer.1.attention.self.key.bias',\n"," 'bert.encoder.layer.1.attention.self.value.weight',\n"," 'bert.encoder.layer.1.attention.self.value.bias',\n"," 'bert.encoder.layer.1.attention.output.dense.weight',\n"," 'bert.encoder.layer.1.attention.output.dense.bias',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.intermediate.dense.weight',\n"," 'bert.encoder.layer.1.intermediate.dense.bias',\n"," 'bert.encoder.layer.1.output.dense.weight',\n"," 'bert.encoder.layer.1.output.dense.bias',\n"," 'bert.encoder.layer.1.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.attention.self.query.weight',\n"," 'bert.encoder.layer.2.attention.self.query.bias',\n"," 'bert.encoder.layer.2.attention.self.key.weight',\n"," 'bert.encoder.layer.2.attention.self.key.bias',\n"," 'bert.encoder.layer.2.attention.self.value.weight',\n"," 'bert.encoder.layer.2.attention.self.value.bias',\n"," 'bert.encoder.layer.2.attention.output.dense.weight',\n"," 'bert.encoder.layer.2.attention.output.dense.bias',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.intermediate.dense.weight',\n"," 'bert.encoder.layer.2.intermediate.dense.bias',\n"," 'bert.encoder.layer.2.output.dense.weight',\n"," 'bert.encoder.layer.2.output.dense.bias',\n"," 'bert.encoder.layer.2.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.attention.self.query.weight',\n"," 'bert.encoder.layer.3.attention.self.query.bias',\n"," 'bert.encoder.layer.3.attention.self.key.weight',\n"," 'bert.encoder.layer.3.attention.self.key.bias',\n"," 'bert.encoder.layer.3.attention.self.value.weight',\n"," 'bert.encoder.layer.3.attention.self.value.bias',\n"," 'bert.encoder.layer.3.attention.output.dense.weight',\n"," 'bert.encoder.layer.3.attention.output.dense.bias',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.intermediate.dense.weight',\n"," 'bert.encoder.layer.3.intermediate.dense.bias',\n"," 'bert.encoder.layer.3.output.dense.weight',\n"," 'bert.encoder.layer.3.output.dense.bias',\n"," 'bert.encoder.layer.3.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.attention.self.query.weight',\n"," 'bert.encoder.layer.4.attention.self.query.bias',\n"," 'bert.encoder.layer.4.attention.self.key.weight',\n"," 'bert.encoder.layer.4.attention.self.key.bias',\n"," 'bert.encoder.layer.4.attention.self.value.weight',\n"," 'bert.encoder.layer.4.attention.self.value.bias',\n"," 'bert.encoder.layer.4.attention.output.dense.weight',\n"," 'bert.encoder.layer.4.attention.output.dense.bias',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.intermediate.dense.weight',\n"," 'bert.encoder.layer.4.intermediate.dense.bias',\n"," 'bert.encoder.layer.4.output.dense.weight',\n"," 'bert.encoder.layer.4.output.dense.bias',\n"," 'bert.encoder.layer.4.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.attention.self.query.weight',\n"," 'bert.encoder.layer.5.attention.self.query.bias',\n"," 'bert.encoder.layer.5.attention.self.key.weight',\n"," 'bert.encoder.layer.5.attention.self.key.bias',\n"," 'bert.encoder.layer.5.attention.self.value.weight',\n"," 'bert.encoder.layer.5.attention.self.value.bias',\n"," 'bert.encoder.layer.5.attention.output.dense.weight',\n"," 'bert.encoder.layer.5.attention.output.dense.bias',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.intermediate.dense.weight',\n"," 'bert.encoder.layer.5.intermediate.dense.bias',\n"," 'bert.encoder.layer.5.output.dense.weight',\n"," 'bert.encoder.layer.5.output.dense.bias',\n"," 'bert.encoder.layer.5.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.attention.self.query.weight',\n"," 'bert.encoder.layer.6.attention.self.query.bias',\n"," 'bert.encoder.layer.6.attention.self.key.weight',\n"," 'bert.encoder.layer.6.attention.self.key.bias',\n"," 'bert.encoder.layer.6.attention.self.value.weight',\n"," 'bert.encoder.layer.6.attention.self.value.bias',\n"," 'bert.encoder.layer.6.attention.output.dense.weight',\n"," 'bert.encoder.layer.6.attention.output.dense.bias',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.intermediate.dense.weight',\n"," 'bert.encoder.layer.6.intermediate.dense.bias',\n"," 'bert.encoder.layer.6.output.dense.weight',\n"," 'bert.encoder.layer.6.output.dense.bias',\n"," 'bert.encoder.layer.6.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.attention.self.query.weight',\n"," 'bert.encoder.layer.7.attention.self.query.bias',\n"," 'bert.encoder.layer.7.attention.self.key.weight',\n"," 'bert.encoder.layer.7.attention.self.key.bias',\n"," 'bert.encoder.layer.7.attention.self.value.weight',\n"," 'bert.encoder.layer.7.attention.self.value.bias',\n"," 'bert.encoder.layer.7.attention.output.dense.weight',\n"," 'bert.encoder.layer.7.attention.output.dense.bias',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.intermediate.dense.weight',\n"," 'bert.encoder.layer.7.intermediate.dense.bias',\n"," 'bert.encoder.layer.7.output.dense.weight',\n"," 'bert.encoder.layer.7.output.dense.bias',\n"," 'bert.encoder.layer.7.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.attention.self.query.weight',\n"," 'bert.encoder.layer.8.attention.self.query.bias',\n"," 'bert.encoder.layer.8.attention.self.key.weight',\n"," 'bert.encoder.layer.8.attention.self.key.bias',\n"," 'bert.encoder.layer.8.attention.self.value.weight',\n"," 'bert.encoder.layer.8.attention.self.value.bias',\n"," 'bert.encoder.layer.8.attention.output.dense.weight',\n"," 'bert.encoder.layer.8.attention.output.dense.bias',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.intermediate.dense.weight',\n"," 'bert.encoder.layer.8.intermediate.dense.bias',\n"," 'bert.encoder.layer.8.output.dense.weight',\n"," 'bert.encoder.layer.8.output.dense.bias',\n"," 'bert.encoder.layer.8.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.attention.self.query.weight',\n"," 'bert.encoder.layer.9.attention.self.query.bias',\n"," 'bert.encoder.layer.9.attention.self.key.weight',\n"," 'bert.encoder.layer.9.attention.self.key.bias',\n"," 'bert.encoder.layer.9.attention.self.value.weight',\n"," 'bert.encoder.layer.9.attention.self.value.bias',\n"," 'bert.encoder.layer.9.attention.output.dense.weight',\n"," 'bert.encoder.layer.9.attention.output.dense.bias',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.intermediate.dense.weight',\n"," 'bert.encoder.layer.9.intermediate.dense.bias',\n"," 'bert.encoder.layer.9.output.dense.weight',\n"," 'bert.encoder.layer.9.output.dense.bias',\n"," 'bert.encoder.layer.9.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.attention.self.query.weight',\n"," 'bert.encoder.layer.10.attention.self.query.bias',\n"," 'bert.encoder.layer.10.attention.self.key.weight',\n"," 'bert.encoder.layer.10.attention.self.key.bias',\n"," 'bert.encoder.layer.10.attention.self.value.weight',\n"," 'bert.encoder.layer.10.attention.self.value.bias',\n"," 'bert.encoder.layer.10.attention.output.dense.weight',\n"," 'bert.encoder.layer.10.attention.output.dense.bias',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.intermediate.dense.weight',\n"," 'bert.encoder.layer.10.intermediate.dense.bias',\n"," 'bert.encoder.layer.10.output.dense.weight',\n"," 'bert.encoder.layer.10.output.dense.bias',\n"," 'bert.encoder.layer.10.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.attention.self.query.weight',\n"," 'bert.encoder.layer.11.attention.self.query.bias',\n"," 'bert.encoder.layer.11.attention.self.key.weight',\n"," 'bert.encoder.layer.11.attention.self.key.bias',\n"," 'bert.encoder.layer.11.attention.self.value.weight',\n"," 'bert.encoder.layer.11.attention.self.value.bias',\n"," 'bert.encoder.layer.11.attention.output.dense.weight',\n"," 'bert.encoder.layer.11.attention.output.dense.bias',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.intermediate.dense.weight',\n"," 'bert.encoder.layer.11.intermediate.dense.bias',\n"," 'bert.encoder.layer.11.output.dense.weight',\n"," 'bert.encoder.layer.11.output.dense.bias',\n"," 'bert.encoder.layer.11.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.output.LayerNorm.bias',\n"," 'bert.pooler.dense.weight',\n"," 'bert.pooler.dense.bias',\n"," 'fc.weight',\n"," 'fc.bias']"]},"metadata":{},"execution_count":15}],"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","print([n for n, p in param_optimizer if not any(nd in n for nd in no_decay)])\n","print('*'*100)\n","print([n for n, p in param_optimizer if any(nd in n for nd in no_decay)])\n","print('*'*100)\n","[i[0] for i in param_optimizer]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-28T08:07:58.511405Z","start_time":"2021-08-28T08:03:33.527351Z"},"id":"q0VF5dQ0SJUb","outputId":"79225f07-7429-4633-9fd8-0354468fba3b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640434295920,"user_tz":-480,"elapsed":16497,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at prev_trained_model were not used when initializing bert: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of bert were not initialized from the model checkpoint at prev_trained_model and are newly initialized: ['fc.bias', 'fc.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["bert.h5 exist!\n"]}],"source":["import os\n","import shutil\n","import pickle as pk\n","from torch.utils.tensorboard import SummaryWriter\n","import random\n","random.seed(2019)\n","\n","# 构建模型，并加载预训练权重\n","model = bert.from_pretrained(\"prev_trained_model\",config=config,parameter = parameter).to(parameter['device'])\n","\n","# 确定训练权重，可以只训练fc层，但是还是推荐全部训练\n","full_finetuning = True\n","if full_finetuning:\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n","             'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n","             'weight_decay': 0.0}\n","        ]\n","else: \n","        param_optimizer = list(model.fc.named_parameters()) \n","        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n","\n","# 确定训练的优化器和学习策略，这个是bert常用的优化方法和学习策略，可以还是基于以前的方式，只是效果会有所下降\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, correct_bias=False)\n","train_steps_per_epoch = 10748 // parameter['batch_size']\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=train_steps_per_epoch, num_training_steps=parameter['epoch'] * train_steps_per_epoch)\n","\n","\n","# 确定训练模式\n","model.train()\n","\n","# 确定损失\n","criterion = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","\n","# 准备迭代器\n","train_yield = batch_yield_bert(parameter)\n","\n","# 开始训练\n","loss_cal = []\n","min_loss = float('inf')\n","logging_steps = 0\n","if os.path.exists('bert.h5'):\n","    print(\"bert.h5 exist!\")\n","else:\n","  print(\"bert.h5 not exist, now tranning!\")\n","\n","  #训练模型\n","  while 1:\n","          inputs,targets,epoch,keys = next(train_yield)\n","          if keys:\n","              break\n","          out = model(inputs)\n","          loss = criterion(out, targets.view(-1))\n","          optimizer.zero_grad()\n","          loss.backward()\n","          # 适当梯度修饰\n","          nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5)\n","          # 优化器和学习策略更新\n","          optimizer.step()\n","          scheduler.step()\n","          \n","          loss_cal.append(loss.item())\n","          logging_steps += 1\n","          if logging_steps%100 == 0:\n","              print(sum(loss_cal)/len(loss_cal))\n","          if epoch is not None:\n","              if (epoch+1)%1 == 0:\n","                  loss_cal = sum(loss_cal)/len(loss_cal)\n","                  if loss_cal < min_loss:\n","                      min_loss = loss_cal\n","                      torch.save(model.state_dict(), 'bert.h5')\n","                  print('epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \\\n","                                                        parameter['epoch'],loss_cal))\n","              loss_cal = [loss.item()]\n","              scheduler.step()\n"]},{"cell_type":"markdown","metadata":{"id":"cdR-dBRxSJUc"},"source":["# 基于bert预训练模型+CRF"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-26T11:23:13.985018Z","start_time":"2021-08-26T11:23:13.777011Z"},"id":"tCz2EgMVSJUd"},"outputs":[],"source":["from transformers import WEIGHTS_NAME, BertConfig,get_linear_schedule_with_warmup,AdamW, BertTokenizer\n","from transformers import BertModel,BertPreTrainedModel\n","from torch.nn import CrossEntropyLoss\n","import torch.nn as nn\n","import torch\n","from torchcrf import CRF\n","\n","import torch.nn.functional as F # pytorch 激活函数的类\n","from torch import nn,optim # 构建模型和优化器\n","\n","# 方法与bert没有什么区别，只是加上了CRF进行处理\n","# 构建基于bert+crf实现ner\n","class bert_crf(BertPreTrainedModel):\n","    def __init__(self, config,parameter):\n","        super(bert_crf, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        embedding_dim = parameter['d_model']\n","        output_size = parameter['output_size']\n","        self.fc = nn.Linear(embedding_dim, output_size)\n","        self.init_weights()\n","        \n","        self.crf = CRF(output_size,batch_first=True)\n","        \n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None,labels=None):\n","        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.fc(sequence_output)\n","        return logits\n","    \n","config_class, bert_crf, tokenizer_class = BertConfig, bert_crf, BertTokenizer\n","config = config_class.from_pretrained(\"prev_trained_model\")\n","tokenizer = tokenizer_class.from_pretrained(\"prev_trained_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-08-26T11:32:23.389347Z","start_time":"2021-08-26T11:23:14.885242Z"},"id":"INhawFtbSJUd","outputId":"cfc393d4-03c8-4b55-8f30-ba7b8b591ff4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640432989445,"user_tz":-480,"elapsed":138745,"user":{"displayName":"aiden zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00038965460335847869"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at prev_trained_model were not used when initializing bert_crf: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing bert_crf from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing bert_crf from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of bert_crf were not initialized from the model checkpoint at prev_trained_model and are newly initialized: ['crf.end_transitions', 'fc.bias', 'crf.start_transitions', 'fc.weight', 'crf.transitions']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/10748 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n","  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"," 47%|████▋     | 5000/10748 [00:30<00:35, 162.16it/s]"]},{"output_type":"stream","name":"stdout","text":["2414.820708618164\n"]},{"output_type":"stream","name":"stderr","text":[" 93%|█████████▎| 10000/10748 [01:01<00:04, 160.24it/s]"]},{"output_type":"stream","name":"stdout","text":["1518.5285807800292\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10748/10748 [01:06<00:00, 162.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch [1/2], Loss: 1452.2503\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|███▉      | 4250/10748 [00:26<00:40, 159.84it/s]"]},{"output_type":"stream","name":"stdout","text":["431.40916017044424\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 9250/10748 [00:57<00:09, 160.07it/s]"]},{"output_type":"stream","name":"stdout","text":["399.96661770728326\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10748/10748 [01:07<00:00, 160.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch [2/2], Loss: 391.0218\n"]}],"source":["import os\n","import shutil\n","import pickle as pk\n","from torch.utils.tensorboard import SummaryWriter\n","\n","random.seed(2019)\n","\n","# 构建模型\n","model = bert_crf.from_pretrained(\"prev_trained_model\",config=config,parameter = parameter).to(parameter['device'])\n","\n","# 确定训练权重\n","full_finetuning = True\n","if full_finetuning:\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n","             'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n","             'weight_decay': 0.0}\n","        ]\n","else: \n","        param_optimizer = list(model.fc.named_parameters()) \n","        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n","\n","# 确定训练的优化器和学习策略\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, correct_bias=False)\n","train_steps_per_epoch = 10748 // parameter['batch_size']\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=train_steps_per_epoch, num_training_steps=parameter['epoch'] * train_steps_per_epoch)\n","\n","\n","# 确定训练模式\n","model.train()\n","\n","# 确定损失\n","criterion = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","\n","# 准备迭代器\n","train_yield = batch_yield_bert(parameter)\n","\n","# 开始训练\n","loss_cal = []\n","min_loss = float('inf')\n","logging_steps = 0\n","\n","if os.path.exists('bert.h5'):\n","    print(\"bert_crf.h5 exist!\")\n","else:\n","  print(\"bert_crf.h5 not exist, now tranning!\")\n","  while 1:\n","          inputs,targets,epoch,keys = next(train_yield)\n","          if keys:\n","              break\n","          out = model(inputs)\n","          # 同样crf被用于损失\n","          loss = -model.crf(out, targets)\n","          optimizer.zero_grad()\n","          loss.backward()\n","          # 适当梯度修饰\n","          nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5)\n","          # 优化器和学习策略更新\n","          optimizer.step()\n","          scheduler.step()\n","          \n","          loss_cal.append(loss.item())\n","          logging_steps += 1\n","          if logging_steps%100 == 0:\n","              print(sum(loss_cal)/len(loss_cal))\n","          if epoch is not None:\n","              if (epoch+1)%1 == 0:\n","                  loss_cal = sum(loss_cal)/len(loss_cal)\n","                  if loss_cal < min_loss:\n","                      min_loss = loss_cal\n","                      torch.save(model.state_dict(), 'bert_crf.h5')\n","                  print('epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \\\n","                                                        parameter['epoch'],loss_cal))\n","              loss_cal = [loss.item()]\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"165px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"bert+bert_crf.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}