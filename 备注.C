
##监督学习：感知机 逻辑回归 k近邻 决策树  朴素贝叶斯 提升学习 SVM 隐马尔可夫 
	生成式：
	判别式：


#2感知机
	目标：若数据集T线性可分，则感知机学习的目标就是找到使得训练集正实例和负实例被完全切分开的超平面，。
	学习策略：通过误分类点距离之和定义损失函数，并用梯度下降法求解优化问题，得出损失最小的w 和 b
	备注：当数据集线性可分时，感知机算法收敛且有无穷多解。注意联系SVM
	
#SVM(有别于感知机？)当训练集可分时，感知机只求可分类的超平面，有无穷多个，svm求最优的一个。
	目标：寻找间隔最大超平面(软硬间隔最大化)和分类决策函数
	策略：求解凸二次规格问题，等价于正则化的合页损失函数最小化问题  -->怎么解该凸优化问题？
	
	备注：是一种二分类模型；数据集三种情况：线性可分 接近线性可分 线性不可分 分别对应：线性可分支持向量机(硬间隔支持向量机) 线性支持向量机(软间隔支持向量机) 非线性支持向量机
	
		线性可分支持向量机 -->硬间隔最大化 -->直接求
		线性支持向量机     -->软间隔最大化 -->约束条件添加松弛变量 , 目标函数添加惩罚项
		非线性支持向量机   -->找到核函数   -->修改软间隔最大化的目标函数和分类决策函数并求解
		
#逻辑回归与最大熵模型
	#逻辑回归模型
		分二项逻辑回归和多项逻辑回归->用以解决二分类和多分类问题
		
		对于二项逻辑回归
			目标：求出逻辑回归模型-->估计y=1和y=0对应的概率，判断哪个大，从而判断y=1 or 0；典型二分类模型
			学习策略：最大似然估计
		对于多项逻辑回归
	#最大熵模型
	
#朴素贝叶斯


#HMM
	目标：求出观测序列和状态序列的联合概率分布函数模型(生成式模型)
	策略：
#CRF
	目标：直接求出状态序列相对于观测序列的条件概率分布模型(判别是模型)；该模型可在不知道观测序列的情况下用来求出某一状态序列出现的概率
	策略：
#决策树

#提升学习
	目标：训练出一系列弱分类器并的线性组合
	策略：前向学习M轮，每一轮虽然训练集数据不变，但每个数据的权重迭代的进行调整，每一轮训练出一个弱分类器后，确定其在训练集上的误差，根据误差确定该分类器的权重，同时更新下一个若分类器序列时各个数据的权重。
	
	ada boost：
	
	boosting tree：
		以决策树为基函数