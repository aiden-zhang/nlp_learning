{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ner模型功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_file= model/ner/-9\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000017307B79D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000017307B79D68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000017307B79D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000017307B79D68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000172928474E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000172928474E0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000172928474E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000172928474E0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "INFO:tensorflow:Restoring parameters from model/ner/-9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'I-km2',\n",
       " 2: 'S-author',\n",
       " 3: 'B-class',\n",
       " 4: 'E-km2',\n",
       " 5: 'I-title',\n",
       " 6: 'B-tag',\n",
       " 7: 'E-km1',\n",
       " 8: 'S-kg',\n",
       " 9: 'E-title',\n",
       " 10: 'E-author',\n",
       " 11: 'B-kg',\n",
       " 12: 'E-kg',\n",
       " 13: 'B-km1',\n",
       " 14: 'I-km1',\n",
       " 15: 'I-kg',\n",
       " 16: 'I-class',\n",
       " 17: 'S-title',\n",
       " 18: 'I-tag',\n",
       " 19: 'B-km2',\n",
       " 20: 'I-author',\n",
       " 21: 'B-title',\n",
       " 22: 'E-class',\n",
       " 23: 'S-class',\n",
       " 24: 'E-tag',\n",
       " 25: 'B-author'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "import pickle as pk\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def seq2id(seq, vocab):\n",
    "    sentence_id = []\n",
    "    for word in seq:\n",
    "        if word not in vocab:\n",
    "            word = '<UNK>'\n",
    "        sentence_id.append(vocab[word])\n",
    "    return sentence_id\n",
    "\n",
    "\n",
    "class BiLSTM_ADD_CRF():\n",
    "    def __init__(self,parameter):\n",
    "        self.batch_size = parameter['batch_size']\n",
    "        self.epoch_num = parameter['epoch']\n",
    "        self.hidden_dim = parameter['hidden_dim']\n",
    "        self.embeddings = parameter['embeddings']\n",
    "        self.update_embedding = parameter['update_embedding']\n",
    "        self.dropout_keep_prob = parameter['dropout']\n",
    "        self.optimizer = parameter['optimizer']\n",
    "        self.lr = parameter['lr']\n",
    "        self.clip_grad = parameter['clip']\n",
    "        self.tag2label = parameter['tag2label']\n",
    "        self.num_tags = parameter['num_tags']\n",
    "        self.vocab = parameter['vocab']\n",
    "        self.shuffle = parameter['shuffle']\n",
    "        self.model_path = parameter['model_path']\n",
    "        self.config = tf.ConfigProto()\n",
    "        \n",
    "    def batch_yield(self,data):\n",
    "        # 构建一个迭代器，获取相应的seqs（index型）和label，按照batch_size提取\n",
    "        if self.shuffle:\n",
    "            random.shuffle(data)\n",
    "        seqs,labels = [],[]\n",
    "        for (seq,label) in data:\n",
    "            seq = seq2id(seq,self.vocab)\n",
    "            label = [self.tag2label[label_] for label_ in label]\n",
    "            if len(seqs) == self.batch_size:\n",
    "                seq_len_list = [len(i) for i in seqs]\n",
    "                max_len = max(seq_len_list)\n",
    "                # tensorflow 1 对于不定长处理不好，placeholder输入需要等长，故进行补0\n",
    "                seqs = [i+[0]*(max_len-len(i)) for i in seqs]\n",
    "                labels = [i+[0]*(max_len-len(i)) for i in labels]\n",
    "                yield seqs,labels,seq_len_list\n",
    "                seqs,labels = [],[]\n",
    "            seqs.append(seq)\n",
    "            labels.append(label)\n",
    "        if len(seqs) != 0:\n",
    "            seq_len_list = [len(i) for i in seqs]\n",
    "            max_len = max(seq_len_list)\n",
    "            # tensorflow 1 对于不定长处理不好，placeholder输入需要等长，故进行补0\n",
    "            seqs = [i+[0]*(max_len-len(i)) for i in seqs]\n",
    "            labels = [i+[0]*(max_len-len(i)) for i in labels]\n",
    "            yield seqs, labels, seq_len_list\n",
    "            \n",
    "    def pre_placeholders(self):\n",
    "        # 提前设置预留的seqs和labels，和各batch的序列长度，以便后续喂参数\n",
    "        self.seqs = tf.placeholder(tf.int32, shape=[None, None], name=\"seqs\")\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\")\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        self.dropout_pl = tf.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")\n",
    "        self.lr_pl = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "        \n",
    "    def lookup_layer(self):\n",
    "        # 从self.embeddings,提取对应index的embedding\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            _word_embeddings = tf.Variable(self.embeddings,\n",
    "                                           dtype=tf.float32,\n",
    "                                           trainable=self.update_embedding,\n",
    "                                           name=\"_word_embeddings\")\n",
    "            # 寻找_word_embeddings矩阵中分别为seqs中元素作为下标的值，提取并组合成一个新的向量\n",
    "            word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,\n",
    "                                                     ids=self.seqs,\n",
    "                                                     name=\"word_embeddings\")\n",
    "        # 模型实现过程设置test3、test4进行过程中变量的输出，查看结构和预期是否一致\n",
    "        self.test3 = word_embeddings\n",
    "        self.test4 = self.seqs\n",
    "        # dropout函数是为了防止在训练中过拟合的操作\n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout_pl)\n",
    "        \n",
    "        \n",
    "    def biLSTM_layer(self):\n",
    "        # 双向lstm就是两个lstm组成\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            # 准备前向cell\n",
    "            cell_fw = LSTMCell(self.hidden_dim)\n",
    "            # 准备反向cell\n",
    "            cell_bw = LSTMCell(self.hidden_dim)\n",
    "            # 此处的输出，包含一个前向输出的结果和后项输出的结果\n",
    "            (output_fw_seq, output_bw_seq), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=cell_fw,\n",
    "                cell_bw=cell_bw,\n",
    "                inputs=self.word_embeddings,\n",
    "                sequence_length=self.sequence_lengths,\n",
    "                dtype=tf.float32)\n",
    "            # 简单查看下前向和后项输出结果的形状\n",
    "            self.test1 = output_fw_seq\n",
    "            self.test2 = output_bw_seq\n",
    "            # 输出结果为隐层的输出，将两者拼接在一起作为最终，双向lstm的输出\n",
    "            output = tf.concat([output_fw_seq, output_bw_seq], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout_pl)\n",
    "\n",
    "        with tf.variable_scope(\"predict\"):\n",
    "            # 初始化相应的权重和偏置\n",
    "            W = tf.get_variable(name=\"W\",\n",
    "                                shape=[2 * self.hidden_dim, self.num_tags],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                dtype=tf.float32)\n",
    "            b = tf.get_variable(name=\"b\",\n",
    "                                shape=[self.num_tags],\n",
    "                                initializer=tf.zeros_initializer(),\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "            s = tf.shape(output)\n",
    "            output = tf.reshape(output, [-1, 2*self.hidden_dim])\n",
    "            # 通过矩阵乘法操作，将结果投影到num_tags维的空间上\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, s[1], self.num_tags])\n",
    "            \n",
    "    def crf_pred(self):\n",
    "        # 大家可以查下tensorflow里面关于crf_log_likelihood这个api\n",
    "        # crf_log_likelihood 在这里是一个损失函数\n",
    "        # 输入：\n",
    "        # inputs，就是每个标签的预测概率值，就是经过矩阵乘法变换得到的self.logits\n",
    "        # tag_indices，是期望输出，target\n",
    "        # sequence_lengths，和上面一样，是样本真实的序列长度\n",
    "        # transition_params，就是李老师讲的转移概率，可以没有，这个函数可以自己计算\n",
    "        # 输出：\n",
    "        # log_likelihood，是一个标量，在这里有点类似于-损失\n",
    "        # transition_params，函数自己计算的转移概率，在我们推理阶段就是用这个\n",
    "        log_likelihood, self.transition_params = crf_log_likelihood(inputs=self.logits,\n",
    "                                                                   tag_indices=self.labels,\n",
    "                                                                   sequence_lengths=self.sequence_lengths)\n",
    "        self.loss = -tf.reduce_mean(log_likelihood)\n",
    "\n",
    "        \n",
    "    def trainstep(self):\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            # 此处按照所需选择不同的优化器，如所提供的论文中所用Adam、RMSprop等\n",
    "            if self.optimizer in ['Adam','Adadelta','Adagrad','RMSProp']:\n",
    "                optim = eval('tf.train.'+self.optimizer+'Optimizer(learning_rate=self.lr_pl)')\n",
    "            elif self.optimizer == 'Momentum':\n",
    "                optim = tf.train.MomentumOptimizer(learning_rate=self.lr_pl, momentum=0.9)\n",
    "            else:\n",
    "                optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n",
    "            grads_and_vars = optim.compute_gradients(self.loss)\n",
    "            # 此处和之前在RNN、LSTM处一致，对梯度进行修饰\n",
    "            # tf.clip_by_value(A, min, max)指将列表A中元素压缩在min和max之间，大于max或小于min的值改成max和min\n",
    "            grads_and_vars_clip = [[tf.clip_by_value(g, -self.clip_grad, self.clip_grad), v] for g, v in grads_and_vars]\n",
    "            # 梯度更新\n",
    "            self.train = optim.apply_gradients(grads_and_vars_clip, global_step=self.global_step)\n",
    "        \n",
    "    def init(self):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "    def have_a_test(self,data):\n",
    "        self.pre_placeholders()\n",
    "        self.lookup_layer()\n",
    "#         self.biLSTM_layer()\n",
    "#         self.crf_pred()\n",
    "#         self.trainstep()\n",
    "        self.init()\n",
    "        #test data\n",
    "        data = self.batch_yield(data)\n",
    "        data_seqs,data_labels,seq_len_list = next(data)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(self.init)\n",
    "            feed_dict = {self.seqs: data_seqs,\n",
    "                         self.sequence_lengths: seq_len_list,\n",
    "                         self.labels: data_labels,\n",
    "                         self.dropout_pl:self.dropout_keep_prob,\n",
    "                         self.lr_pl:self.lr\n",
    "                        }\n",
    "            return sess.run([self.test3,self.test4],feed_dict=feed_dict),seq_len_list\n",
    "        \n",
    "    def train_one_epoch(self,sess,epoch,data):\n",
    "        # 训练批次数\n",
    "        num_batches = (len(data) + self.batch_size - 1) // self.batch_size\n",
    "        # 记录时间\n",
    "        sta_time = time.time()\n",
    "        # 数据迭代器\n",
    "        batches = self.batch_yield(data)\n",
    "        cal_loss = 0\n",
    "        for step,(seqs,labels,seq_len_list) in enumerate(batches):\n",
    "            feed_dict = {self.seqs: seqs,\n",
    "                         self.sequence_lengths: seq_len_list,\n",
    "                         self.labels: labels,\n",
    "                         self.dropout_pl:self.dropout_keep_prob,\n",
    "                         self.lr_pl:self.lr\n",
    "                        }\n",
    "            _, loss_train,  step_num_ = sess.run([self.train, self.loss, self.global_step],feed_dict=feed_dict)\n",
    "            cal_loss += loss_train\n",
    "            sys.stdout.write(' processing: {} epoch / {} batch / {} batches / {} loss / {} time'.format(epoch, step + 1, num_batches ,cal_loss/(step+1) ,time.time()-sta_time) + '\\r')\n",
    "        print('\\n')\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.pre_placeholders()\n",
    "        self.lookup_layer()\n",
    "        self.biLSTM_layer()\n",
    "        self.crf_pred()\n",
    "        self.trainstep()\n",
    "        self.init()\n",
    "        \n",
    "    def train(self,train_data):\n",
    "        self.build_graph()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(self.init)\n",
    "            # 循环训练epoch_num次\n",
    "            for epoch in range(self.epoch_num):\n",
    "                self.train_one_epoch(sess, epoch, train_data)\n",
    "                saver.save(sess, self.model_path, global_step=epoch)\n",
    "                \n",
    "    #预测一批数据集\n",
    "    def predict_one_batch(self, sess, seqs, seq_len_list):\n",
    "        #将样本进行整理（填充0方式使得每句话长度一样，并返回每句话实际长度）\n",
    "        feed_dict = {self.seqs: seqs,\n",
    "                         self.sequence_lengths: seq_len_list,\n",
    "                         self.dropout_pl:1.0,\n",
    "                        }\n",
    "        logits, transition_params = sess.run([self.logits, self.transition_params],\n",
    "                                                 feed_dict=feed_dict)\n",
    "        label_list = []\n",
    "        for logit, seq_len in zip(logits, seq_len_list):\n",
    "            # 维特比算法（解码），通过转移概率递推，计算最优路径，具体可参考李老师的ppt或后续有关于crf的实现\n",
    "            viterbi_seq, _ = viterbi_decode(logit[:seq_len], transition_params)\n",
    "            label_list.append(viterbi_seq)\n",
    "        return label_list\n",
    "\n",
    "def load_model():\n",
    "    parameter = pk.load(open('model/ner/parameter.pkl','rb'))\n",
    "    tf.reset_default_graph()\n",
    "    tag2label = parameter['tag2label']\n",
    "    model_path = parameter['model_path']\n",
    "    ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "    print('ckpt_file=',ckpt_file)\n",
    "    #加载图\n",
    "    model = BiLSTM_ADD_CRF(parameter)\n",
    "    model.build_graph()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session(config=tf.ConfigProto())\n",
    "    saver.restore(sess, ckpt_file)\n",
    "    label2tag = dict(zip(tag2label.values(),tag2label.keys()))\n",
    "    return sess,model,label2tag\n",
    "\n",
    "def for_pred(ins,model,sess,label2tag):\n",
    "    for_pred = list(ins.strip())#例：['在', '弄', '恩', '哦', '呜']\n",
    "    for_pred = [(for_pred, ['O'] * len(for_pred))]\n",
    "    for_pred_batch,_,for_pred_len = next(model.batch_yield(for_pred))\n",
    "    res = model.predict_one_batch(sess,for_pred_batch,for_pred_len)[0]\n",
    "    entity_list = []\n",
    "    pad = -1\n",
    "    tmp = None\n",
    "    for ind,i in enumerate(res):\n",
    "        if ind <= pad:\n",
    "            continue\n",
    "        i = label2tag[i] \n",
    "        if i[0] in ['S','B']:\n",
    "            if i[0] == 'S':\n",
    "                entity_list.append([for_pred[0][0][ind],i.split('-')[1],[ind]])\n",
    "            else:\n",
    "                tmp = [for_pred[0][0][ind],i.split('-')[1],[ind]]\n",
    "                for j in range(ind+1,len(res)):\n",
    "                    j_tag = label2tag[res[j]]\n",
    "                    if j_tag == 'O':\n",
    "                        pad = ind\n",
    "                        tmp = None\n",
    "                        break\n",
    "                    if j_tag.split('-')[1] == tmp[1] and j_tag[0] != 'B':\n",
    "                        tmp[0] += for_pred[0][0][j]\n",
    "                        tmp[-1].append(j)\n",
    "                        if j_tag[0] == 'E':\n",
    "                            pad = j\n",
    "                            entity_list.append(tmp)\n",
    "                            tmp = None\n",
    "                            break\n",
    "                        if j_tag[0] == 'I':\n",
    "                            pad = j\n",
    "                    else:\n",
    "                        pad = ind\n",
    "                        tmp = None\n",
    "                        break\n",
    "    return entity_list,res,for_pred[0][0]\n",
    "\n",
    "\n",
    "ner_sess,ner_model,ner_label2tag = load_model()\n",
    "ner_label2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 10, 0, 0]\n",
      "['李', '白', '是', '谁']\n",
      "[['李白', 'author', [0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "ins = '李白是谁'\n",
    "entity_list,pred,inputs = for_pred(ins,ner_model,ner_sess,ner_label2tag)\n",
    "print(pred)\n",
    "print(inputs)\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ner-20211121模型功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:18:32.409385Z",
     "start_time": "2021-11-21T03:18:32.298681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2272, 689, 1427, 389]\n",
      "('B-author', 'E-author', 'O', 'O')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torchcrf\\__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorCompare.cpp:255.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['李白', 'author', [0, 1]]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F # pytorch 激活函数的类\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "# 构建基于bilstm+crf实现ner\n",
    "class bilstm_crf(nn.Module):\n",
    "    def __init__(self, parameter):\n",
    "        super(bilstm_crf, self).__init__()\n",
    "        vocab_size = parameter['vocab_size']\n",
    "        embedding_dim = parameter['d_model']\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        hidden_size = parameter['hid_dim']\n",
    "        num_layers = parameter['n_layers']\n",
    "        dropout = parameter['dropout']\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "        output_size = parameter['num_tags']\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "        self.crf = CRF(output_size,batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out,(h, c)= self.lstm(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 此处是加载对应的模型和配置文件\n",
    "def load_model(mode_path):\n",
    "    parameter = pk.load(open(mode_path+'parameter.pkl','rb'))\n",
    "    #     parameter['device'] = torch.device('cpu')\n",
    "    # 因为bert模型需要加载他对应的config文件，因此此处进行了一定的区分\n",
    "    model = bilstm_crf(parameter).to(parameter['device'])\n",
    "    model.load_state_dict(torch.load(model_path+'bilstm_crf.h5'))\n",
    "    model.eval() \n",
    "    return model,parameter\n",
    "\n",
    "def keyword_predict(input):\n",
    "    def list2torch(ins):\n",
    "        return torch.from_numpy(np.array(ins))\n",
    "    def seq2id(seq, vocab):\n",
    "        sentence_id = []\n",
    "        for word in seq:\n",
    "            if word not in vocab:\n",
    "                word = '<UNK>'\n",
    "            sentence_id.append(vocab[word])\n",
    "        return sentence_id\n",
    "    input = list(input)\n",
    "    ind2key = dict(zip(parameter['tag2label'].values(),parameter['tag2label'].keys()))\n",
    "    input_id = seq2id(input,parameter['vocab'])#itemgetter(*input)(parameter['word2ind'])\n",
    "    print(input_id)\n",
    "    predict = model.crf.decode(model(list2torch([input_id]).long().to(parameter['device'])))[0]\n",
    "    predict = itemgetter(*predict)(ind2key)\n",
    "    print(predict)\n",
    "    keys_list = []\n",
    "    for ind,i in enumerate(predict):\n",
    "        if i == 'O':\n",
    "            continue\n",
    "        if i[0] == 'S':\n",
    "            if not(len(keys_list) == 0 or keys_list[-1][-1]):\n",
    "                del keys_list[-1]\n",
    "            keys_list.append([input[ind],[i],[ind],True])\n",
    "            continue\n",
    "        if i[0] == 'B':\n",
    "            if not(len(keys_list) == 0 or keys_list[-1][-1]):\n",
    "                del keys_list[-1]\n",
    "            keys_list.append([input[ind],[i],[ind],False])\n",
    "            continue\n",
    "        if i[0] == 'I':\n",
    "            if len(keys_list) > 0 and not keys_list[-1][-1] and \\\n",
    "            keys_list[-1][1][0].split('-')[1] == i.split('-')[1]:\n",
    "                keys_list[-1][0] += input[ind]\n",
    "                keys_list[-1][1] += [i]\n",
    "                keys_list[-1][2] += [ind]\n",
    "            else:\n",
    "                if len(keys_list) > 0:\n",
    "                    del keys_list[-1]\n",
    "            continue\n",
    "        if i[0] == 'E':\n",
    "            if len(keys_list) > 0 and not keys_list[-1][-1] and \\\n",
    "            keys_list[-1][1][0].split('-')[1] == i.split('-')[1]:\n",
    "                keys_list[-1][0] += input[ind]\n",
    "                keys_list[-1][1] += [i]\n",
    "                keys_list[-1][2] += [ind]\n",
    "                keys_list[-1][3] = True\n",
    "            else:\n",
    "                if len(keys_list) > 0:\n",
    "                    del keys_list[-1]\n",
    "            continue\n",
    "    keys_list = [[i[0],i[1][0].split('-')[1],i[2]] for i in keys_list]\n",
    "    return keys_list\n",
    "\n",
    "model_path = 'model/ner/'\n",
    "model,parameter = load_model(model_path)\n",
    "\n",
    "keyword_predict('李白是谁')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:47:37.160232Z",
     "start_time": "2021-11-21T03:47:37.150256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('author', '是', '谁')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于ner重建后的提问\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def takelong(ins):\n",
    "    return len(ins[0])\n",
    "\n",
    "def rebuildiins(ins,entity_list):\n",
    "    new_ins = {}\n",
    "    left_ind = set(range(len(ins)))\n",
    "    for i in entity_list:\n",
    "        left_ind -= set(range(i[-1][0],i[-1][-1]+1))\n",
    "        new_ins[i[-1][0]] = i[1]\n",
    "    for i in left_ind:\n",
    "        new_ins[i] = ins[i]\n",
    "    new_id = list(new_ins.keys())\n",
    "    new_id.sort()\n",
    "    return itemgetter(*new_id)(new_ins)\n",
    "\n",
    "\n",
    "\n",
    "question = '李白是谁'\n",
    "entity_list = [['李白', 'author', [0, 1]]]\n",
    "entity_list.sort(key = takelong)\n",
    "entity_list = entity_list[::-1]\n",
    "new_question = rebuildiins(question,entity_list)\n",
    "new_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 意图识别功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:20:14.140097Z",
     "start_time": "2021-11-21T03:20:14.085235Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F # pytorch 激活函数的类\n",
    "from torch import nn,optim # 构建模型和优化器\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "# 构建分类模型\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, parameter):\n",
    "        super(TextRNN, self).__init__()\n",
    "        embedding_dim = parameter['embedding_dim']\n",
    "        hidden_size = parameter['hidden_size']\n",
    "        output_size = parameter['output_size']\n",
    "        num_layers = parameter['num_layers']\n",
    "        dropout = parameter['dropout']\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out,(h, c)= self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "def load_model(path):\n",
    "    parameter = pk.load(open(path,'rb'))\n",
    "    parameter['dropout'] = 0\n",
    "    model = TextRNN(parameter).to(parameter['cuda'])\n",
    "    model.load_state_dict(torch.load(parameter['model_path']+'model-rnn.h5'))\n",
    "    return parameter,model\n",
    "\n",
    "def batch_predict(chars,parameter):\n",
    "        max_len = 0\n",
    "        batch_x = []\n",
    "        for iters in range(len(chars)):\n",
    "            for i in range(len(chars[iters])):\n",
    "                if chars[iters][i] not in parameter['char2ind']:\n",
    "                    chars[iters][i] = '<unk>'\n",
    "            batch_ids = itemgetter(*chars[iters])(parameter['char2ind'])\n",
    "            try:\n",
    "                batch_ids = list(batch_ids)\n",
    "            except:\n",
    "                batch_ids = [batch_ids,0]\n",
    "            if len(batch_ids) > max_len:\n",
    "                max_len = len(batch_ids)\n",
    "            batch_x.append(batch_ids)\n",
    "        batch_x = [np.array(list(itemgetter(*x_ids)(parameter['ind2embeding']))+[parameter['ind2embeding'][0]]*(max_len-len(x_ids))) for x_ids in batch_x]\n",
    "        device = parameter['cuda']\n",
    "        return torch.from_numpy(np.array(batch_x)).to(device)\n",
    "    \n",
    "def predict(ins,model,parameter):\n",
    "    seqs = batch_predict(ins,parameter)\n",
    "    res = model(seqs)\n",
    "    predicted_prob,predicted_index = torch.max(F.softmax(res, 1), 1)\n",
    "    res = predicted_index.cpu().numpy()\n",
    "    return res\n",
    "\n",
    "\n",
    "intent0_parameter,intent0_model = load_model('model/intent0/parameter.pkl')\n",
    "intent1_parameter,intent1_model = load_model('model/intent1/parameter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:20:16.393917Z",
     "start_time": "2021-11-21T03:20:16.376933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '课'],\n",
       " ['说', '你', '的', '工', '作'],\n",
       " ['唱', '歌', '吧'],\n",
       " 0,\n",
       " 1,\n",
       " 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "x,y = pk.load(open('data/data-intent0.pkl','rb'))\n",
    "x[1],x[300],x[500],y[1],y[300],y[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:20:17.945822Z",
     "start_time": "2021-11-21T03:20:17.922920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '课'],\n",
    "        ['说', '你', '的', '工', '作'],\n",
    "         ['唱', '歌', '吧'],\n",
    "        ],intent0_model,intent0_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:20:27.852053Z",
     "start_time": "2021-11-21T03:20:27.673248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '课'],\n",
       " ['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '知', '识', '点'],\n",
       " ['老', '师', 'km2', '有', '哪', '些', '重', '要', '的', '例', '题', '需', '要', '掌', '握'],\n",
       " 0,\n",
       " 1,\n",
       " 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "x,y,_,_ = pk.load(open('data/data-intent1-ner.pkl','rb'))\n",
    "x[1],x[20],x[100],y[1],y[20],y[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T03:20:29.622695Z",
     "start_time": "2021-11-21T03:20:29.602749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '课'],\n",
    " ['老', '师', 'km1', '有', '哪', '些', '重', '要', '的', '知', '识', '点'],\n",
    " ['老', '师', 'km2', '有', '哪', '些', '重', '要', '的', '例', '题', '需', '要', '掌', '握'],\n",
    "        ],intent1_model,intent1_parameter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
