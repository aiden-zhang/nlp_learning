《深度学习入门》
1.激活函数，作用增加非线性，现在一般用reLu相比sigmoid优势是？

2.分类问题的推理一般会省去softmax，减少计算资源的消耗。

3.数据预处理
    手写数字识别的例子是吧每幅图图的每个像素点从0-255转化成0-1。利用数据整体的均值或标准差，移动数据，使数据整体以0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。BN？LN？

4.批处理
    大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快

5.一个batch的交叉熵损失要会写
6 batchsize iteration 和epoch的定义，
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递

假设数据一共1000条，每个iteration取bachsize = 10条数据训练，那运行100个iteration，所有数据都被"看过"一次，这算一个epoch，每个epoch计算一次损失。

7.过拟合是泛化能力不够，过度学习了训练数据的特征，使得训练时精度很高，推理时用新数据时精度变得很低。

8.最优化方法
为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法，简称SGD，缺点：更新路径呈"z"形，效率低。Momentum相比SGD更优，能降低寻优过程中的"晃动"次数。AdaGrad进一步优化寻优过程，给每个参数调整学习率，且学习率随着训练逐步衰减。Adam综合了AdaGrad和Momentum，引入了3个超参数，但效果没有AdaGrad好。按理来说AdaGrad最好，应该都用这个，但没有一个优化算法在所有问题上表现都好，且对于AdaGrad随着超参数设定的不同，结果也会变化，所以面对实际问题具体选哪个可以都试试看。

9.权重初始化，不能设置为0，会导致梯度消失，激活函数是sogmoid和tanx时用“Xavier 初始值 ”,激活函数为ReLu用“He初始值”，好的初始值应以增加激活值(激活层的输出)分布的广度为目标；

10.Batch Normalization
为了使各层拥有适当的广度，“强制性”地调整激活值的分布，为此在激活层前加入BN层，研究表明有三个优点：①加快学习速度，模型不那么依赖参数的初始化方法(对初始值健壮)，抑制过拟合(降低Dropout等的必要性)。方法就是按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1 的正规化，接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换

11.抑制过拟合的其他方法
过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。我们在制作复杂的、表现力强的模型同时也要注意避免过拟合。方法处理上面提到的还有：
1）权值衰减
通过在学习的过程中对大的权重进行惩罚，来抑制过拟合->L1和L2正则，惩罚项关键权重W的表达式不同。
2）Dropout
是一种在学习的过程中按比例随机删除神经元的方法，Dropout对于表现力强的网络也可以抑制过拟合即减小训练和测试精度的差值，但可能导致测试和训练精度同时都有所降低。Dropout类似于集成学习，因为每次训练时随机删除不同的神经元后可以看成每次用不同的模型，推理时求模型输出的平均值。
12.超参数的验证
除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。这里所说的超参数是指，比如各层的神经元数量、batch 大小、参数更新时的学习率或权值衰减等。为此需要验证集来评估超参数的好坏，注意为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。超参数最优化方法略。

13 CNN
CNN 中新增了Convolution 层和Pooling 层。CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling 层有时会被省
略）。这可以理解为之前的“Affine - ReLU”连接被替换成了“Convolution -ReLU -（Pooling）”连接。注意靠近输出层还会用“Affi ne - ReLU”组合，且输出层还会用“Affi ne -Softmax”组合。

14.卷积核数量和通道数概念
在卷积层的计算中，假设输入是H x W x C, C是输入的深度(即通道数)，那么卷积核(滤波器)的通道数需要和输入的通道数相同，所以也为C，假设卷积核的大小为K x K，一个卷积核就为K x K x C，计算时卷积核的对应通道应用于输入的对应通道，这样一个卷积核应用于输入就得到输出的一个通道。假设有P个K x K x C的卷积核，这样每个卷积核应用于输入都会得到一个通道，所以输出有P个通道。

15.pooling的作用
做窗口滑动卷积的时候，卷积值就代表了整个窗口的特征。因为滑动的窗口间有大量重叠区域，出来的卷积值有冗余，进行最大pooling或者平均pooling就是减少冗余。换句话说pooling技术将卷积层后得到的小邻域内的特征点整合得到新的特征。一方面防止无用参数增加时间复杂度，一方面增加了特征的整合度。


《深度学习进阶》

1.Word2vect改进
多分类改为二分类，负采样，负采样让预测时负例概率更接近0

2.语言模型用困惑度评价，即预测下一个字出现为实际的字的概率的导数

3.RNN存在梯度消失和梯度爆炸问题
原因：以tanh作为激活函数为例，导数远离下x远离0时迅速衰减到0，这样反向传播导数相乘是的梯度快速降到0，出现消失。改为ReLu可以有效减少梯度消失的情况。

以MatMul矩阵相乘为例(dh x W)，反向传播求梯度时多次相乘很容易溢出。
解决办法：梯度剪枝可以防止梯度爆炸，采用LSTM可以防止梯度消失

4.LSTM在RNN基础上增加记忆单元c，引入遗忘门决定对c的以往，输入门决定对新内容xt添加到上一个ct-1中的程度，输出门管理由ct到ht的输出

5.分别有LSTM构成编解码器，就有了seq2seq，需要优化于是引入attention进行改进

6.transformer主要利用attention完成特征提取及解码

7.使用transformer编码器端作为特征提取器得到bert

8.BN和LN的区别，BN训练和推理时的差别

卷积层BN：
卷积层BN通常会在卷积之后，激活之前进行。多个卷积核卷积生成多通道的特征图，每个通道都要单独处理，各通道有自己的均值方差，γ，β，每个通道内部（一张特征图上）所有神经元共享均值方差（mini-batch数据卷积后(m×c ×w×h)，对(m×w×h)个神经元求均值方差）。
 
和LN的区别：
对于[B,C,W,H]这样的训练数据而言，BN是在B,W,H维度求均值方差进行规范化(每次对B个同一通道图像数据求一次，一个batch求一次每次求C个)，而LN是对C,W,H(C一般是序列长度)维度求均值方差进行规范化（当前层一共会求batchsize个均值和方差，每个batchsize分别规范化），这样LN就与batchsize无关了，小的batchsize也可以进行归一化训练，LN也可以很轻松地用到RNN中。

BN在batchsize范围内求，LN在序列长度范围内求

LN与batchsize无关，在小batchsize上效果可能会比BN好，但是大batchsize的话还是BN效果好。

训练与测试时BN差别：
训练时每次iteration对batchsize个训练两边计算一次mean和var，


书之外：
准确率、精准(确)率，召回率，F1值，困惑度
准确率 正负例中预测正确的比例 TP+TN/(TP+TN+FP+FN)
精准率 针对预测结果，表示预测为正的有多少比例是真的->TP/(TP+FP)
召回率 针对原来样本，表示样本中正例有多少比例被预测正确了->TP/(TP+FN) 真正和假负原本都是正
F1值是精确率和召回率的调和均值