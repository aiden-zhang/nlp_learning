《深度学习入门》
1.激活函数，作用增加非线性，现在一般用reLu相比sigmoid优势是？
sigmod先行段导数小，且双侧导数都趋于0，链式求导公式容易导致梯度消失。ReLu导数为0或1不会导致梯度消失，但ReLu会带来神经元死亡问题，进一步优化出现了LeakyReLU，解决了神经元死亡问题，更好的还有ELU可以单侧饱和，更利于收敛。

实际使用是先用ReLu试，效果好就用，不好再试LeakyReLu等更复杂的激活函数。

2.分类问题的推理一般会省去softmax，减少计算资源的消耗。

3.数据预处理
    手写数字识别的例子是吧每幅图图的每个像素点从0-255转化成0-1。利用数据整体的均值或标准差，移动数据，使数据整体以0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。BN？LN？

4.批处理
    大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快

5.一个batch的交叉熵损失要会写
6 batchsize iteration 和epoch的定义，
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递

假设数据一共1000条，每个iteration取bachsize = 10条数据训练，那运行100个iteration，所有数据都被"看过"一次，这算一个epoch，每个epoch计算一次损失。

7.过拟合是泛化能力不够，过度学习了训练数据的特征，使得训练时精度很高，推理时用新数据时精度变得很低。

8.最优化方法
为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法，简称SGD，缺点：更新路径呈"z"形，效率低。Momentum相比SGD更优，能降低寻优过程中的"晃动"次数。AdaGrad进一步优化寻优过程，给每个参数调整学习率，且学习率随着训练逐步衰减。Adam综合了AdaGrad和Momentum，引入了3个超参数，但效果没有AdaGrad好。按理来说AdaGrad最好，应该都用这个，但没有一个优化算法在所有问题上表现都好，且对于AdaGrad随着超参数设定的不同，结果也会变化，所以面对实际问题具体选哪个可以都试试看。

9.权重初始化，不能设置为0，会导致梯度消失，激活函数是sogmoid和tanx时用“Xavier 初始值 ”,激活函数为ReLu用“He初始值”，好的初始值应以增加激活值(激活层的输出)分布的广度为目标；

10.Batch Normalization
为了使各层拥有适当的广度，“强制性”地调整激活值的分布，为此在激活层前加入BN层，研究表明有三个优点：①加快学习速度，模型不那么依赖参数的初始化方法(对初始值健壮)，抑制过拟合(降低Dropout等的必要性)。方法就是按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1 的正规化，接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换

11.抑制过拟合的其他方法
过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。我们在制作复杂的、表现力强的模型同时也要注意避免过拟合。方法处理上面提到的还有：
1）权值衰减
通过在学习的过程中对大的权重进行惩罚，来抑制过拟合->L1和L2正则，惩罚项关键权重W的表达式不同。
2）Dropout
是一种在学习的过程中按比例随机删除神经元的方法，Dropout对于表现力强的网络也可以抑制过拟合即减小训练和测试精度的差值，但可能导致测试和训练精度同时都有所降低。Dropout类似于集成学习，因为每次训练时随机删除不同的神经元后可以看成每次用不同的模型，推理时求模型输出的平均值。
12.超参数的验证
除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。这里所说的超参数是指，比如各层的神经元数量、batch 大小、参数更新时的学习率或权值衰减等。为此需要验证集来评估超参数的好坏，注意为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。超参数最优化方法略。

13 CNN
CNN 中新增了Convolution 层和Pooling 层。CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling 层有时会被省
略）。这可以理解为之前的“Affine - ReLU”连接被替换成了“Convolution -ReLU -（Pooling）”连接。注意靠近输出层还会用“Affi ne - ReLU”组合，且输出层还会用“Affi ne -Softmax”组合。

14.卷积核数量和通道数概念
在卷积层的计算中，假设输入是H x W x C, C是输入的深度(即通道数)，那么卷积核(滤波器)的通道数需要和输入的通道数相同，所以也为C，假设卷积核的大小为K x K，一个卷积核就为K x K x C，计算时卷积核的对应通道应用于输入的对应通道，这样一个卷积核应用于输入就得到输出的一个通道。假设有P个K x K x C的卷积核，这样每个卷积核应用于输入都会得到一个通道，所以输出有P个通道。

15.pooling的作用
做窗口滑动卷积的时候，卷积值就代表了整个窗口的特征。因为滑动的窗口间有大量重叠区域，出来的卷积值有冗余，进行最大pooling或者平均pooling就是减少冗余。换句话说pooling技术将卷积层后得到的小邻域内的特征点整合得到新的特征。一方面防止无用参数增加时间复杂度，一方面增加了特征的整合度。


《深度学习进阶》

1.Word2vect改进
多分类改为二分类，负采样，负采样让预测时负例概率更接近0

2.语言模型用困惑度评价，即预测下一个字出现为实际的字的概率的导数

3.RNN存在梯度消失和梯度爆炸问题
原因：以tanh作为激活函数为例，导数远离下x远离0时迅速衰减到0，这样反向传播导数相乘是的梯度快速降到0，出现消失。改为ReLu可以有效减少梯度消失的情况。

以MatMul矩阵相乘为例(dh x W)，反向传播求梯度时多次相乘很容易溢出。
解决办法：梯度剪枝可以防止梯度爆炸，采用LSTM可以防止梯度消失

4.LSTM在RNN基础上增加记忆单元c，引入遗忘门决定对c的以往，输入门决定对新内容xt添加到上一个ct-1中的程度，输出门管理由ct到ht的输出

5.分别有LSTM构成编解码器，就有了seq2seq，需要优化于是引入attention进行改进

6.transformer主要利用attention完成特征提取及解码

7.使用transformer编码器端作为特征提取器得到bert

8.BN和LN的区别，BN训练和推理时的差别

卷积层BN：
卷积层BN通常会在卷积之后，激活之前进行。多个卷积核卷积生成多通道的特征图，每个通道都要单独处理，各通道有自己的均值方差，γ，β，每个通道内部（一张特征图上）所有神经元共享均值方差（mini-batch数据卷积后(m×c ×w×h)，对(m×w×h)个神经元求均值方差）。
 
和LN的区别：
对于[B,C,W,H]这样的训练数据而言，BN是在B,W,H维度求均值方差进行规范化(每次对B个同一通道图像数据求一次，一个batch求一次每次求C个)，而LN是对C,W,H(C一般是序列长度)维度求均值方差进行规范化（当前层一共会求batchsize个均值和方差，每个batchsize分别规范化），这样LN就与batchsize无关了，小的batchsize也可以进行归一化训练，LN也可以很轻松地用到RNN中。

BN在batchsize范围内求，LN在序列长度范围内求

LN与batchsize无关，在小batchsize上效果可能会比BN好，但是大batchsize的话还是BN效果好。

训练与测试时BN差别：
训练时每次iteration对batchsize个训练两边计算一次mean和var，


书之外：
准确率、精准(确)率，召回率，F1值，困惑度
准确率 正负例中预测正确的比例 TP+TN/(TP+TN+FP+FN)
精准率 针对预测结果，表示预测为正的有多少比例是真的->TP/(TP+FP)
召回率 针对原来样本，表示样本中正例有多少比例被预测正确了->TP/(TP+FN) 真正和假负原本都是正
F1值是精确率和召回率的调和均值

对话机器人总结：
第一次 教育机器人
制作教育对话机器人，可满足用户3个方向的提问，包括针对机器人资深属性的，基于闲聊对话，基于高中地理语文等教学内容的提问。围绕这个需求，需要训练三个NLP模型，两个用于意图识别，一个用于命名实体识别，为此首先利用已有的结构化数据构训练数据集，同时对于复杂的高中学科知识整理成实体数据和关系数据两个csv文件，放入Neo4j中构建知识图谱。最后训练好的模型将封装好的各模型的预测功能放到flask服务服务器上。完成后的对话机器人，当用户输入一句话后，第一个意图模型将提问进行分类，若是对于机器人自身属性的提问则从提前存储了机器人自身属性的xml文件中搜索最接近的问题对应的答案，若是闲聊的在闲聊数据集中搜索最接近的问题的答案，针对高中学科知识的，则会再次送入另一个意图识别模型，识别出是哪个学科的问题，确定出类别后，进行命名实体识别，提取问题中与专业知识相关的关键词，利用这些关键词从Neo4j知识图谱中检索出答案返回给用户。

意图识别用的LSTM，实体识别用的LSTM+CRF。CRF层的作用是为结果序列增加标签先后顺序的约束。crf条件随机场是一种给定随机变量预x求解条件概率p(y|x)的模型，训练时，给定训练集条件下的最大释然估计对应的参数作为crf模型的最终参数。推理时给定x的情况下各种可能状态序列y的概率最大的序列状态作为推理结果。


可以优化的地方，可以优化模型采用效果更好的模型，数据方面可以扩充更多数据，为了提高回答的精准度，可以对数据进行更详细的分类。


第二次 面试机器人
项目的目标是制作一个面试机器人，能够通过web自动根据面试者的提交的简历进行提问，并根据面试者的回答返回最终面试者面试是否通过的结果。

针对这些需求首先需要完成pdf转txt，然后对文本数据进行命名实体识别，确定面试者简历中涉及到的机器学习的知识点有哪些，有了知识点就可以是实现构建好的知识图谱中搜索知识点对应的问题，当面试者在网页上输入了他的答案后，后端会对答案进行评分，具体评分的策略是分别对面试者的回答和问题提取特征，然后求余弦相似度，根据相似度确定分数，总分>60面试通过。

关键字识别模型bert+crf 打分模型 两个LSTM组成。

可优化的方向，首先是数据，数据量不大，为了提升泛化能力，可以考虑增加一些数据增强的策略，另外打分的模型目前是用的LSTM提取特征，可以采用bert，提升性能，另外实测最后区分度不够高，用标准答案稍加修改测试确实能达到很高的分数，但面试者随便写的答案也有三四十分，直接把问题填到答案框里也能得出比较不错的分值，可以想办法让错误答案的分数降下来增加区分度。训练打分模型时可以让他学习更多负样本数据一起训练

意图识别：cnn rnn rnn+attention transformerEncoder 0.82 0.83 0.84 0.81
实体识别：bilstm bilstm_crf  bert bert_crf 0.72 0.74 0.834 0.837

bert总结：
是一种预训练模型，是transformer的编码侧模型，采用大量文本数据进行训练，最适用于文本生成。如何预训练，两个任务，第一个masked LM: mask 15%(80%替换成[mask],10%替换成其他token，10%不变)，最后算loss只计算mask掉的。第二个Next Sentence Prediction，两句话，第一句前+[cls]，两句中间+[sep],用一半上下文句子，一半非上下文句子训练，最后编码器对应[cls]的编码器输出就是整个句子编码后的向量。

由12层encoder layer构成，每个layer主要由多头只注意力层 feed forward层和LN层组成，先embeding(3种：字，句，位置)，在multi_head_attention，输出转置后与embeding后的向量形状一致，所以可以直接做残差连接，紧接着LN处理按每一行句子分别处理

bert使用根据不同的任务，分类任务可以在其后面接ffn+softmax，ner就加crf，训练时小数据量就微调后面层的参数，数据量大可以微调bert的参数。


深度学习的瓶颈：
1.对标注数据的依赖性大
2.模型具有领域依赖性，难以直接迁移，目前NLP模型+微调缓解了这一问题，但任有待提升
3.模型越来越大
4.缺乏高效的超参数自动搜索方案
5.可解释性不强


介绍下自己：
1.学校，专业，研究生课题，可能主要完成的工作，
毕业后来到海康负责门禁产品嵌入式软件开发，主要工作是完成音视频业务逻辑，以及新算法的集成等工作。因为对AI算法比较感兴趣，因此从毕业工作就有自学人工智能方面的理论知识，因为没有这个领域的人指导，担心直接学习的今年5月就想更系统的学习一下，











